{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:1\n",
      "Using seed 28.\n",
      "Dataset: PTC\n",
      "loading data\n",
      "# classes: 2\n",
      "# maximum node tag: 19\n",
      "# data: 344\n",
      "Num classes: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/lige/HKN/utils/data_utils.py:613: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adjs.append(sp.csr_matrix(nx.adjacency_matrix(g.g)))\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import sys\n",
    "sys.path.append('/data/lige/HKN')# Please change accordingly!\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "from optim import RiemannianAdam, RiemannianSGD\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from config import parser\n",
    "from models.base_models import NCModel, LPModel, GCModel\n",
    "from utils.data_utils import load_data, get_nei, GCDataset, split_batch\n",
    "from utils.train_utils import get_dir_name, format_metrics\n",
    "from utils.eval_utils import acc_f1\n",
    "\n",
    "from geoopt import ManifoldParameter as geoopt_ManifoldParameter\n",
    "from manifolds.base import ManifoldParameter as base_ManifoldParameter\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "config_args = {\n",
    "    'training_config': {\n",
    "        'lr': (1e-3, 'learning rate'),\n",
    "        'dropout': (0.3, 'dropout probability'),\n",
    "        'cuda': (1, 'which cuda device to use (-1 for cpu training)'),\n",
    "        'epochs': (1000, 'maximum number of epochs to train for'),\n",
    "        'weight_decay': (1e-3, 'l2 regularization strength'),\n",
    "        'optimizer': ('radam', 'which optimizer to use, can be any of [rsgd, radam]'),\n",
    "        'momentum': (0.999, 'momentum in optimizer'),\n",
    "        'patience': (15, 'patience for early stopping'),\n",
    "        'seed': (28, 'seed for training'),\n",
    "        'log_freq': (1, 'how often to compute print train/val metrics (in epochs)'),\n",
    "        'eval_freq': (1, 'how often to compute val metrics (in epochs)'),\n",
    "        'save': (0, '1 to save model and logs and 0 otherwise'),\n",
    "        'save_dir': (None, 'path to save training logs and model weights (defaults to logs/task/date/run/)'),\n",
    "        'sweep_c': (0, ''),\n",
    "        'lr_reduce_freq': (None, 'reduce lr every lr-reduce-freq or None to keep lr constant'),\n",
    "        'gamma': (0.5, 'gamma for lr scheduler'),\n",
    "        'print_epoch': (True, ''),\n",
    "        'grad_clip': (None, 'max norm for gradient clipping, or None for no gradient clipping'),\n",
    "        'min_epochs': (300, 'do not early stop before min-epochs')\n",
    "    },\n",
    "    'model_config': {\n",
    "        'use_geoopt': (False, \"which manifold class to use, if false then use basd.manifold\"),\n",
    "        'AggKlein':(True, \"if false, then use hyperboloid centorid for aggregation\"),\n",
    "        'corr': (1,'0: d(x_i ominus x, x_k), 1: d(x_ik,x_k)'),\n",
    "        'task': ('gc', 'which tasks to train on, can be any of [lp, nc]'),\n",
    "        'model': ('BKNet', 'which encoder to use, can be any of [Shallow, MLP, HNN, GCN, GAT, HyperGCN, HyboNet,BKNet,BMLP]'),\n",
    "        'dim': (32, 'embedding dimension'), #The final dimension as the embedded vector\n",
    "        #'dim': (64, 'embedding dimension'),\n",
    "        'manifold': ('PoincareBall', 'which manifold to use, can be any of [Euclidean, Hyperboloid, PoincareBall, Lorentz]'),\n",
    "        'c': (1.0, 'hyperbolic radius, set to None for trainable curvature'),\n",
    "        'r': (2., 'fermi-dirac decoder parameter for lp'),\n",
    "        't': (1., 'fermi-dirac decoder parameter for lp'),\n",
    "        'margin': (2., 'margin of MarginLoss'),\n",
    "        'pretrained_embeddings': (None, 'path to pretrained embeddings (.npy file) for Shallow node classification'),\n",
    "        'pos_weight': (0, 'whether to upweight positive class in node classification tasks'),\n",
    "        'num_layers': (2, 'number of hidden layers in encoder'),\n",
    "        'bias': (1, 'whether to use bias (1) or not (0)'),\n",
    "        'act': ('relu', 'which activation function to use (or None for no activation)'),\n",
    "        'n_heads': (4, 'number of attention heads for graph attention networks, must be a divisor dim'),\n",
    "        'alpha': (0.2, 'alpha for leakyrelu in graph attention networks'),\n",
    "        'double_precision': ('1', 'whether to use double precision'),\n",
    "        'use_att': (0, 'whether to use hyperbolic attention or not'),\n",
    "        'local_agg': (0, 'whether to local tangent space aggregation or not'),\n",
    "        'kernel_size': (6, 'number of kernels'),\n",
    "        'KP_extent': (0.66, 'influence radius of each kernel point'),\n",
    "        'radius': (1, 'radius used for kernel point init'),\n",
    "        'deformable': (False, 'deformable kernel'),\n",
    "        #'linear_before': (64, 'dim of linear before gcn')#The dimension after linear_before(dimensionality reduction if you would)\n",
    "        'linear_before': (32, 'dim of linear before gcn')#64\n",
    "    },\n",
    "    'data_config': {\n",
    "        'dataset': ('PTC', 'which dataset to use(cornell,wisconsin,texas,squirrel,cora)'),\n",
    "        #'dataset': ('film', 'which dataset to use(cornell,wisconsin,texas,squirrel,cora)'),\n",
    "        'batch_size': (32, 'batch size for gc'),\n",
    "        'val_prop': (0.05, 'proportion of validation edges for link prediction'),\n",
    "        'test_prop': (0.1, 'proportion of test edges for link prediction'),\n",
    "        'use_feats': (1, 'whether to use node features or not'),\n",
    "        'normalize_feats': (1, 'whether to normalize input node features'),\n",
    "        'normalize_adj': (1, 'whether to row-normalize the adjacency matrix'),\n",
    "        'split_seed': (28, 'seed for data splits (train/test/val)'),\n",
    "        'split_graph': (False, 'whether to split the graph')\n",
    "    }\n",
    "}\n",
    "\n",
    "# 将所有参数转换为 SimpleNamespace\n",
    "args = SimpleNamespace(\n",
    "    **{k: v[0] for config in config_args.values() for k, v in config.items()}\n",
    ")\n",
    "\n",
    "#choose which manifold class to follow \n",
    "if args.use_geoopt == False:\n",
    "    ManifoldParameter = base_ManifoldParameter\n",
    "else:\n",
    "    ManifoldParameter = geoopt_ManifoldParameter\n",
    "np.random.seed(args.seed)#args.seed\n",
    "torch.manual_seed(args.seed)#args.seed\n",
    "if int(args.cuda):#args.double_precision\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "if int(args.cuda) >= 0:#args.cuda\n",
    "    torch.cuda.manual_seed(args.seed)#args.seed\n",
    "args.device = 'cuda:' + str(args.cuda) if int(args.cuda) >= 0 else 'cpu' #args.device actually,<-args.cuda\n",
    "args.patience = args.epochs if not args.patience else args.patience #args.patience<-args.epochs|args.patience\n",
    "\n",
    "print(f'Using: {args.device}')\n",
    "print(\"Using seed {}.\".format(args.seed))\n",
    "print(f\"Dataset: {args.dataset}\")\n",
    "\n",
    "# Load data\n",
    "data = load_data(args, os.path.join('data', args.dataset))\n",
    "if args.task == 'gc':\n",
    "    args.n_nodes, args.feat_dim = data['features'][0].shape\n",
    "else:\n",
    "    args.n_nodes, args.feat_dim = data['features'].shape\n",
    "if args.task == 'nc':\n",
    "    Model = NCModel\n",
    "    args.n_classes = int(data['labels'].max() + 1)\n",
    "    args.data = data\n",
    "    print(f'Num classes: {args.n_classes}')\n",
    "elif args.task == 'gc':\n",
    "    Model = GCModel\n",
    "    args.n_classes = int(data['labels'].max() + 1)\n",
    "    print(f'Num classes: {args.n_classes}')\n",
    "else:\n",
    "    args.nb_false_edges = len(data['train_edges_false'])\n",
    "    args.nb_edges = len(data['train_edges'])\n",
    "    if args.task == 'lp':\n",
    "        Model = LPModel\n",
    "        args.n_classes = 2\n",
    "\n",
    "if not args.lr_reduce_freq:\n",
    "    args.lr_reduce_freq = args.epochs\n",
    "\n",
    "model = Model(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['adj_train', 'features', 'labels', 'idx_train', 'idx_val', 'idx_test'])\n",
      "344 Correct 344 adj, 344 graphs in total\n",
      "torch.Size([22, 19])\n",
      "343 315 338\n",
      "So the idx should probably be the index for graphs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.keys())\n",
    "print(len(data['adj_train']),'Correct 344 adj, 344 graphs in total')\n",
    "print(data['features'][2].shape)#(n,d), but for every graph num of nodes are different\n",
    "print(max(data['idx_train']),max(data['idx_val']),max(data['idx_test']))\n",
    "print(\"So the idx should probably be the index for graphs\")\n",
    "data['labels']#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 4]) torch.Size([65, 4]) torch.Size([65, 19]) torch.Size([2])\n",
      "[265, 300, 222]\n",
      "(39, 39) (26, 26)\n"
     ]
    }
   ],
   "source": [
    "dataset = GCDataset((data['adj_train'], data['features'], data['labels']), KP=(args.model == 'HKPNet' or args.model == 'BKNet'),\n",
    "                             normlize=1, device = 'cpu')\n",
    "\n",
    "nei, nei_mask, features, labels, ed_idx = dataset[data['idx_train'][0:2]]\n",
    "print(nei.shape,nei_mask.shape,features.shape,ed_idx.shape)\n",
    "\n",
    "print(data['idx_train'][:3])\n",
    "print(data['adj_train'][265].todense().shape,data['adj_train'][300].todense().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([39, 65])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The shapes of x and y must match",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m     einsum_expr \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdims\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mk,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdims\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mk->\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdims\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39meinsum(einsum_expr, x, y)\n\u001b[0;32m---> 16\u001b[0m \u001b[43meinsum_last_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[46], line 7\u001b[0m, in \u001b[0;36meinsum_last_dim\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m      5\u001b[0m x_shape \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      6\u001b[0m y_shape \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m x_shape \u001b[38;5;241m==\u001b[39m y_shape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe shapes of x and y must match\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m num_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x_shape)\n\u001b[1;32m     11\u001b[0m dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m97\u001b[39m \u001b[38;5;241m+\u001b[39m i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_dims \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# a, b, c, ...\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The shapes of x and y must match"
     ]
    }
   ],
   "source": [
    "#a =  torch.randn([39])\n",
    "b = 0.05*torch.randn([39,8,5])\n",
    "a = 1/(torch.sqrt(1.0-torch.norm(b,dim=-1)).abs().clamp_min(1e-5))\n",
    "def einsum_last_dim(x, y):\n",
    "    x_shape = x.shape\n",
    "    y_shape = y.shape\n",
    "    assert x_shape == y_shape, \"The shapes of x and y must match\"\n",
    "\n",
    "    num_dims = len(x_shape)\n",
    "\n",
    "    dims = ''.join(chr(97 + i) for i in range(num_dims - 1))  # a, b, c, ...\n",
    "    einsum_expr = f'{dims}k,{dims}k->{dims}'\n",
    "\n",
    "    return torch.einsum(einsum_expr, x, y)\n",
    "\n",
    "einsum_last_dim(a,b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0061, 0.0058, 0.0056, 0.0044, 0.0042, 0.0044, 0.0051, 0.0049])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.randn([30,8])\n",
    "b = torch.ones(list(a.shape))\n",
    "c=0.01*torch.rand([30,8])\n",
    "model.manifold.klein_to_poincare(model.manifold.klein_midpoint(model.manifold.poincare_to_klein(c,1)),1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HKNet",
   "language": "python",
   "name": "hknet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
