{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(use_geoopt=False,\n",
       "          lr=0.001,\n",
       "          dropout=0.2,\n",
       "          cuda=0,\n",
       "          epochs=100,\n",
       "          weight_decay=0.0,\n",
       "          optimizer='radam',\n",
       "          momentum=0.999,\n",
       "          patience=150,\n",
       "          seed=1234,\n",
       "          log_freq=1,\n",
       "          eval_freq=1,\n",
       "          save=0,\n",
       "          save_dir=None,\n",
       "          sweep_c=0,\n",
       "          lr_reduce_freq=None,\n",
       "          gamma=0.5,\n",
       "          print-epoch=True,\n",
       "          grad-clip=None,\n",
       "          min-epochs=100,\n",
       "          task='nc',\n",
       "          model='BKNet',\n",
       "          dim=32,\n",
       "          manifold='PoincareBall',\n",
       "          c=1.0,\n",
       "          r=2.0,\n",
       "          t=1.0,\n",
       "          margin=2.0,\n",
       "          pretrained_embeddings=None,\n",
       "          pos_weight=0,\n",
       "          num_layers=2,\n",
       "          bias=1,\n",
       "          act='relu',\n",
       "          n_heads=4,\n",
       "          alpha=0.2,\n",
       "          double_precision='1',\n",
       "          use_att=0,\n",
       "          local_agg=0,\n",
       "          kernel_size=8,\n",
       "          KP_extent=0.66,\n",
       "          radius=1,\n",
       "          deformable=False,\n",
       "          linear_before=64,\n",
       "          dataset='cornell',\n",
       "          batch_size=32,\n",
       "          val_prop=0.05,\n",
       "          test_prop=0.1,\n",
       "          use_feats=1,\n",
       "          normalize_feats=1,\n",
       "          normalize_adj=1,\n",
       "          split_seed=1234,\n",
       "          split_graph=False)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "from types import SimpleNamespace\n",
    "import sys\n",
    "sys.path.append('/data/lige/HKN')# Please change accordingly!\n",
    "config_args = {\n",
    "    'training_config': {\n",
    "        'use_geoopt': (False, \"which manifold class to use, if false then use basd.manifold\"),\n",
    "        'lr': (0.001, 'learning rate'),\n",
    "        'dropout': (0.2, 'dropout probability'),\n",
    "        'cuda': (0, 'which cuda device to use (-1 for cpu training)'),\n",
    "        'epochs': (100, 'maximum number of epochs to train for'),\n",
    "        'weight_decay': (0., 'l2 regularization strength'),\n",
    "        'optimizer': ('radam', 'which optimizer to use, can be any of [rsgd, radam]'),\n",
    "        'momentum': (0.999, 'momentum in optimizer'),\n",
    "        'patience': (150, 'patience for early stopping'),\n",
    "        'seed': (1234, 'seed for training'),\n",
    "        'log_freq': (1, 'how often to compute print train/val metrics (in epochs)'),\n",
    "        'eval_freq': (1, 'how often to compute val metrics (in epochs)'),\n",
    "        'save': (0, '1 to save model and logs and 0 otherwise'),\n",
    "        'save_dir': (None, 'path to save training logs and model weights (defaults to logs/task/date/run/)'),\n",
    "        'sweep_c': (0, ''),\n",
    "        'lr_reduce_freq': (None, 'reduce lr every lr-reduce-freq or None to keep lr constant'),\n",
    "        'gamma': (0.5, 'gamma for lr scheduler'),\n",
    "        'print-epoch': (True, ''),\n",
    "        'grad-clip': (None, 'max norm for gradient clipping, or None for no gradient clipping'),\n",
    "        'min-epochs': (100, 'do not early stop before min-epochs')\n",
    "    },\n",
    "    'model_config': {\n",
    "        'task': ('nc', 'which tasks to train on, can be any of [lp, nc]'),\n",
    "        'model': ('BKNet', 'which encoder to use, can be any of [Shallow, MLP, HNN, GCN, GAT, HyperGCN, HyboNet,BKN]'),\n",
    "        'dim': (32, 'embedding dimension'),\n",
    "        'manifold': ('PoincareBall', 'which manifold to use, can be any of [Euclidean, Hyperboloid, PoincareBall, Lorentz]'),\n",
    "        'c': (1.0, 'hyperbolic radius, set to None for trainable curvature'),\n",
    "        'r': (2., 'fermi-dirac decoder parameter for lp'),\n",
    "        't': (1., 'fermi-dirac decoder parameter for lp'),\n",
    "        'margin': (2., 'margin of MarginLoss'),\n",
    "        'pretrained_embeddings': (None, 'path to pretrained embeddings (.npy file) for Shallow node classification'),\n",
    "        'pos_weight': (0, 'whether to upweight positive class in node classification tasks'),\n",
    "        'num_layers': (2, 'number of hidden layers in encoder'),\n",
    "        'bias': (1, 'whether to use bias (1) or not (0)'),\n",
    "        'act': ('relu', 'which activation function to use (or None for no activation)'),\n",
    "        'n_heads': (4, 'number of attention heads for graph attention networks, must be a divisor dim'),\n",
    "        'alpha': (0.2, 'alpha for leakyrelu in graph attention networks'),\n",
    "        'double_precision': ('1', 'whether to use double precision'),\n",
    "        'use_att': (0, 'whether to use hyperbolic attention or not'),\n",
    "        'local_agg': (0, 'whether to local tangent space aggregation or not'),\n",
    "        'kernel_size': (8, 'number of kernels'),\n",
    "        'KP_extent': (0.66, 'influence radius of each kernel point'),\n",
    "        'radius': (1, 'radius used for kernel point init'),\n",
    "        'deformable': (False, 'deformable kernel'),\n",
    "        'linear_before': (64, 'dim of linear before gcn')\n",
    "    },\n",
    "    'data_config': {\n",
    "        'dataset': ('cornell', 'which dataset to use'),\n",
    "        'batch_size': (32, 'batch size for gc'),\n",
    "        'val_prop': (0.05, 'proportion of validation edges for link prediction'),\n",
    "        'test_prop': (0.1, 'proportion of test edges for link prediction'),\n",
    "        'use_feats': (1, 'whether to use node features or not'),\n",
    "        'normalize_feats': (1, 'whether to normalize input node features'),\n",
    "        'normalize_adj': (1, 'whether to row-normalize the adjacency matrix'),\n",
    "        'split_seed': (1234, 'seed for data splits (train/test/val)'),\n",
    "        'split_graph': (False, 'whether to split the graph')\n",
    "    }\n",
    "}\n",
    "\n",
    "# 将所有参数转换为 SimpleNamespace\n",
    "args = SimpleNamespace(\n",
    "    **{k: v[0] for config in config_args.values() for k, v in config.items()}\n",
    ")\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/lige/miniconda3/envs/HKNet/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from geoopt import ManifoldParameter as geoopt_ManifoldParameter\n",
    "from manifolds.base import ManifoldParameter as base_ManifoldParameter\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "from optim import RiemannianAdam, RiemannianSGD\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from config import parser\n",
    "from models.base_models import NCModel, LPModel, GCModel\n",
    "from utils.data_utils import load_data, get_nei, GCDataset, split_batch\n",
    "from utils.train_utils import get_dir_name, format_metrics\n",
    "from utils.eval_utils import acc_f1\n",
    "\n",
    "from geoopt import ManifoldParameter as geoopt_ManifoldParameter\n",
    "from manifolds.base import ManifoldParameter as base_ManifoldParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n",
      "Using seed 1234.\n",
      "Dataset: cornell\n",
      "Num classes: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/lige/HKN/utils/data_utils.py:300: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G, sorted(G.nodes()))\n"
     ]
    }
   ],
   "source": [
    "#choose which manifold class to follow \n",
    "if args.use_geoopt == False:\n",
    "    ManifoldParameter = base_ManifoldParameter\n",
    "else:\n",
    "    ManifoldParameter = geoopt_ManifoldParameter\n",
    "np.random.seed(args.seed)#args.seed\n",
    "torch.manual_seed(args.seed)#args.seed\n",
    "if int(args.cuda):#args.double_precision\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "if int(args.cuda) >= 0:#args.cuda\n",
    "    torch.cuda.manual_seed(args.seed)#args.seed\n",
    "args.device = 'cuda:' + str(args.cuda) if int(args.cuda) >= 0 else 'cpu' #args.device actually,<-args.cuda\n",
    "# args.device = 'cpu'\n",
    "args.patience = args.epochs if not args.patience else args.patience #args.patience<-args.epochs|args.patience\n",
    "\"\"\"\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "if args.save:\n",
    "    if not args.save_dir:\n",
    "        dt = datetime.datetime.now()\n",
    "        date = f\"{dt.year}_{dt.month}_{dt.day}\"\n",
    "        models_dir = os.path.join('logs', args.task, date)\n",
    "        save_dir = get_dir_name(models_dir)\n",
    "    else:\n",
    "        save_dir = args.save_dir\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        handlers=[\n",
    "                            logging.FileHandler(\n",
    "                                os.path.join(save_dir, 'log.txt')),\n",
    "                            logging.StreamHandler()\n",
    "                        ])\n",
    "\"\"\"\n",
    "\n",
    "print(f'Using: {args.device}')\n",
    "print(\"Using seed {}.\".format(args.seed))\n",
    "print(f\"Dataset: {args.dataset}\")\n",
    "\n",
    "# Load data\n",
    "data = load_data(args, os.path.join('data', args.dataset))\n",
    "if args.task == 'gc':\n",
    "    args.n_nodes, args.feat_dim = data['features'][0].shape\n",
    "else:\n",
    "    args.n_nodes, args.feat_dim = data['features'].shape\n",
    "if args.task == 'nc':\n",
    "    Model = NCModel\n",
    "    args.n_classes = int(data['labels'].max() + 1)\n",
    "    args.data = data\n",
    "    print(f'Num classes: {args.n_classes}')\n",
    "elif args.task == 'gc':\n",
    "    Model = GCModel\n",
    "    args.n_classes = int(data['labels'].max() + 1)\n",
    "    print(f'Num classes: {args.n_classes}')\n",
    "else:\n",
    "    args.nb_false_edges = len(data['train_edges_false'])\n",
    "    args.nb_edges = len(data['train_edges'])\n",
    "    if args.task == 'lp':\n",
    "        Model = LPModel\n",
    "        args.n_classes = 2\n",
    "\n",
    "if not args.lr_reduce_freq:\n",
    "    args.lr_reduce_freq = args.epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['adj_train', 'features', 'labels', 'idx_train', 'idx_val', 'idx_test', 'adj_train_norm'])\n",
      "(183, 183)\n",
      "torch.Size([183, 1703])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################Checks####################################\n",
    "print(data.keys())\n",
    "#Note it's always stored in sparse matrix\n",
    "print(data['adj_train'].todense().shape)\n",
    "print(data['features'].shape)\n",
    "data['features'][]\n",
    "####################################Checks####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCModel(\n",
      "  (encoder): BKNet(\n",
      "    (linear_before): BLinear(in_features=1703, out_features=64, c=tensor([1.], device='cuda:0'))\n",
      "    (layers): Sequential(\n",
      "      (0): KPGraphConvolution(\n",
      "        (net): KernelPointAggregation(\n",
      "          (linears): ModuleList(\n",
      "            (0): BLayer(\n",
      "              (linear): BLinear(in_features=64, out_features=32, c=1)\n",
      "              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))\n",
      "            )\n",
      "            (1): BLayer(\n",
      "              (linear): BLinear(in_features=64, out_features=32, c=1)\n",
      "              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))\n",
      "            )\n",
      "            (2): BLayer(\n",
      "              (linear): BLinear(in_features=64, out_features=32, c=1)\n",
      "              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))\n",
      "            )\n",
      "            (3): BLayer(\n",
      "              (linear): BLinear(in_features=64, out_features=32, c=1)\n",
      "              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))\n",
      "            )\n",
      "            (4): BLayer(\n",
      "              (linear): BLinear(in_features=64, out_features=32, c=1)\n",
      "              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))\n",
      "            )\n",
      "            (5): BLayer(\n",
      "              (linear): BLinear(in_features=64, out_features=32, c=1)\n",
      "              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))\n",
      "            )\n",
      "            (6): BLayer(\n",
      "              (linear): BLinear(in_features=64, out_features=32, c=1)\n",
      "              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))\n",
      "            )\n",
      "            (7): BLayer(\n",
      "              (linear): BLinear(in_features=64, out_features=32, c=1)\n",
      "              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): KPGraphConvolution(\n",
      "        (net): KernelPointAggregation(\n",
      "          (linears): ModuleList(\n",
      "            (0): BLayer(\n",
      "              (linear): BLinear(in_features=32, out_features=32, c=1)\n",
      "              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))\n",
      "            )\n",
      "            (1): BLayer(\n",
      "              (linear): BLinear(in_features=32, out_features=32, c=1)\n",
      "              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))\n",
      "            )\n",
      "            (2): BLayer(\n",
      "              (linear): BLinear(in_features=32, out_features=32, c=1)\n",
      "              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))\n",
      "            )\n",
      "            (3): BLayer(\n",
      "              (linear): BLinear(in_features=32, out_features=32, c=1)\n",
      "              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))\n",
      "            )\n",
      "            (4): BLayer(\n",
      "              (linear): BLinear(in_features=32, out_features=32, c=1)\n",
      "              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))\n",
      "            )\n",
      "            (5): BLayer(\n",
      "              (linear): BLinear(in_features=32, out_features=32, c=1)\n",
      "              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))\n",
      "            )\n",
      "            (6): BLayer(\n",
      "              (linear): BLinear(in_features=32, out_features=32, c=1)\n",
      "              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))\n",
      "            )\n",
      "            (7): BLayer(\n",
      "              (linear): BLinear(in_features=32, out_features=32, c=1)\n",
      "              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): PoincareDecoder()\n",
      ")\n",
      "Total number of parameters: 135109\n"
     ]
    }
   ],
   "source": [
    "# Model and optimizer\n",
    "model = Model(args)\n",
    "print(str(model))\n",
    "no_decay = ['bias', 'scale']\n",
    "optimizer_grouped_parameters = [{\n",
    "    'params': [\n",
    "        p for n, p in model.named_parameters()\n",
    "        if p.requires_grad and not any(\n",
    "            nd in n\n",
    "            for nd in no_decay) and not isinstance(p, ManifoldParameter)\n",
    "    ],\n",
    "    'weight_decay':\n",
    "    args.weight_decay\n",
    "}, {\n",
    "    'params': [\n",
    "        p for n, p in model.named_parameters() if p.requires_grad and any(\n",
    "            nd in n\n",
    "            for nd in no_decay) or isinstance(p, ManifoldParameter)\n",
    "    ],\n",
    "    'weight_decay':\n",
    "    0.0\n",
    "}]\n",
    "if args.optimizer == 'radam':\n",
    "    optimizer = RiemannianAdam(params=optimizer_grouped_parameters,\n",
    "                                lr=args.lr,\n",
    "                                stabilize=10)\n",
    "elif args.optimizer == 'rsgd':\n",
    "    optimizer = RiemannianSGD(params=optimizer_grouped_parameters,\n",
    "                                lr=args.lr,\n",
    "                                stabilize=10)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=int(\n",
    "                                                args.lr_reduce_freq),\n",
    "                                                gamma=float(args.gamma))\n",
    "tot_params = sum([np.prod(p.size()) for p in model.parameters()])\n",
    "model = model.to(args.device)\n",
    "for x, val in data.items():\n",
    "    if torch.is_tensor(data[x]):\n",
    "        data[x] = data[x].to(args.device)\n",
    "print(f\"Total number of parameters: {tot_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': -1, 'f1': -1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model for nc:\n",
    "t_total = time.time()\n",
    "counter = 0\n",
    "best_val_metrics = model.init_metric_dict()\n",
    "best_test_metrics = None\n",
    "best_emb = None\n",
    "if args.n_classes > 2:\n",
    "    f1_average = 'micro'\n",
    "else:\n",
    "    f1_average = 'binary'\n",
    "\n",
    "best_val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  ...,  0,  0,  0],\n",
       "        [90,  0,  0,  ...,  0,  0,  0],\n",
       "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
       "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
       "        [57,  0,  0,  ...,  0,  0,  0]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we go\n",
    "if args.model == 'HKPNet':\n",
    "    nei, nei_mask = get_nei(data['adj_train'])\n",
    "    nei = nei.to(args.device)\n",
    "    nei_mask = nei_mask.to(args.device)\n",
    "elif args.model == 'BKNet':\n",
    "    nei, nei_mask = get_nei(data['adj_train'])\n",
    "    nei = nei.to(args.device)\n",
    "    nei_mask = nei_mask.to(args.device) #nei/nei_mask on cuda now\n",
    "\n",
    "nei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<manifolds.poincare.PoincareBall object at 0x7f537059f8e0>\n",
      "[1703, 32, 32] \n",
      " [<function relu at 0x7f534890b2e0>, <function relu at 0x7f534890b2e0>] \n",
      " [tensor([1.], device='cuda:0'), tensor([1.], device='cuda:0')]\n",
      "[tensor([1.], device='cuda:0'), tensor([1.], device='cuda:0'), tensor([1.], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "####################################Checks####################################\n",
    "import manifolds\n",
    "import models.encoders as encoders\n",
    "import layers.B_layers as B_layers\n",
    "manifold=getattr(manifolds, 'PoincareBall')()\n",
    "print(manifold)\n",
    "BKNet=getattr(encoders, 'BKNet')(torch.tensor([args.c]).to(args.device),args)\n",
    "#print(BKNet)\n",
    "dims, acts, curvatures = B_layers.get_dim_act_curv(args)\n",
    "print(dims,'\\n',acts,'\\n',BKNet.curvatures)\n",
    "BKNet.curvatures.append(torch.tensor([args.c]).to(args.device))\n",
    "print(BKNet.curvatures)\n",
    "####################################Checks####################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnei\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnei_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/lige/HKN/models/base_models.py:52\u001b[0m, in \u001b[0;36mBaseModel.encode\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanifold\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPoincareBall\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     51\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanifold\u001b[38;5;241m.\u001b[39mexpmap0(x,c\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc)\u001b[38;5;66;03m#Using manifold.base\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#Note: h is the updated feature points matrix of shape (n,d')\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h\n",
      "File \u001b[0;32m/data/lige/HKN/models/encoders.py:369\u001b[0m, in \u001b[0;36mBKNet.encode\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m    367\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_before(x)\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m#print(f\"adj: {adj}\")\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBKNet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/lige/HKN/models/encoders.py:35\u001b[0m, in \u001b[0;36mEncoder.encode\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m     33\u001b[0m     nei, nei_mask \u001b[38;5;241m=\u001b[39m adj\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m (x, nei, nei_mask)\n\u001b[0;32m---> 35\u001b[0m     output, _, __ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m#Actually corresponds to (h, nei, nei_mask), but we only need h as output\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_graph:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#Not sure what this means\u001b[39;00m\n",
      "File \u001b[0;32m/data/lige/miniconda3/envs/HKNet/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/data/lige/miniconda3/envs/HKNet/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/lige/HKN/layers/B_layers.py:473\u001b[0m, in \u001b[0;36mKPGraphConvolution.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    472\u001b[0m     x, nei, nei_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 473\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnei\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnei_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m     output \u001b[38;5;241m=\u001b[39m h, nei, nei_mask\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/data/lige/miniconda3/envs/HKNet/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/lige/HKN/layers/B_layers.py:452\u001b[0m, in \u001b[0;36mKernelPointAggregation.forward\u001b[0;34m(self, x, nei, nei_mask, transp, sample, sample_num)\u001b[0m\n\u001b[1;32m    450\u001b[0m x_nei_kernel_dis \u001b[38;5;241m=\u001b[39m x_nei_kernel_dis \u001b[38;5;241m*\u001b[39m nei_mask  \u001b[38;5;66;03m# (n, k, nei_num)\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m#Wait check this out\u001b[39;00m\n\u001b[0;32m--> 452\u001b[0m x_nei_transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_kernel_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_nei\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m klein_x_nei_transform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanifold\u001b[38;5;241m.\u001b[39mpoincare_to_klein(x_nei_transform,c\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc)\u001b[38;5;66;03m#(n,K,nei_num,d')\u001b[39;00m\n\u001b[1;32m    454\u001b[0m klein_x_nei_transform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavg_kernel(klein_x_nei_transform, x_nei_kernel_dis)\u001b[38;5;66;03m#inner_agg#(n,nei_num,d') in Klein\u001b[39;00m\n",
      "File \u001b[0;32m/data/lige/HKN/layers/B_layers.py:398\u001b[0m, in \u001b[0;36mKernelPointAggregation.apply_kernel_transform\u001b[0;34m(self, x_nei)\u001b[0m\n\u001b[1;32m    396\u001b[0m res \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK):\n\u001b[0;32m--> 398\u001b[0m     res\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinears\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_nei\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mconcat(res, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/data/lige/miniconda3/envs/HKNet/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/lige/HKN/layers/B_layers.py:215\u001b[0m, in \u001b[0;36mBLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    214\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[0;32m--> 215\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m h\n",
      "File \u001b[0;32m/data/lige/HKN/layers/B_layers.py:196\u001b[0m, in \u001b[0;36mBAct.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 196\u001b[0m     xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanifold\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogmap0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_in\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanifold\u001b[38;5;241m.\u001b[39mproj_tan0(xt, c\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_out)\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanifold\u001b[38;5;241m.\u001b[39mproj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanifold\u001b[38;5;241m.\u001b[39mexpmap0(xt, c\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_out), c\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_out)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "model.encode(data['features'], (nei, nei_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Here we go\n",
    "        for epoch in range(args.epochs):\n",
    "            t = time.time()\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            if args.model == 'HKPNet':\n",
    "                embeddings = model.encode(data['features'], (nei, nei_mask))\n",
    "                # print(embeddings.isnan().sum())\n",
    "            elif args.model == 'BKNet':\n",
    "                embeddings = model.encode(data['features'], (nei, nei_mask))#if correctly, embeddings on cuda as well\n",
    "            else:\n",
    "                embeddings = model.encode(data['features'], data['adj_train_norm'])\n",
    "            train_metrics = model.compute_metrics(embeddings, data, 'train')\n",
    "            train_metrics['loss'].backward()\n",
    "            if args.grad_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            if (epoch + 1) % args.log_freq == 0:\n",
    "                logging.info(\" \".join([\n",
    "                    'Epoch: {:04d}'.format(epoch + 1),\n",
    "                    'lr: {}'.format(lr_scheduler.get_last_lr()),\n",
    "                    format_metrics(train_metrics, 'train'),\n",
    "                    'time: {:.4f}s'.format(time.time() - t)\n",
    "                ]))\n",
    "            with torch.no_grad():\n",
    "                if (epoch + 1) % args.eval_freq == 0:\n",
    "                    model.eval()\n",
    "                    if args.model == 'HKPNet':\n",
    "                        embeddings = model.encode(data['features'], (nei, nei_mask))\n",
    "                    elif args.model == 'BKNet':\n",
    "                        embeddings = model.encode(data['features'], (nei, nei_mask))\n",
    "                    else:\n",
    "                        embeddings = model.encode(data['features'],\n",
    "                                                data['adj_train_norm'])\n",
    "                    val_metrics = model.compute_metrics(embeddings, data, 'val')\n",
    "                    if (epoch + 1) % args.log_freq == 0:\n",
    "                        logging.info(\" \".join([\n",
    "                            'Epoch: {:04d}'.format(epoch + 1),\n",
    "                            format_metrics(val_metrics, 'val')\n",
    "                        ]))\n",
    "                    if model.has_improved(best_val_metrics, val_metrics):\n",
    "                        best_test_metrics = model.compute_metrics(\n",
    "                            embeddings, data, 'test')\n",
    "                        best_emb = embeddings.cpu()\n",
    "                        if args.save:\n",
    "                            np.save(os.path.join(save_dir, 'embeddings.npy'),\n",
    "                                    best_emb.detach().numpy())\n",
    "                        best_val_metrics = val_metrics\n",
    "                        counter = 0\n",
    "                    else:\n",
    "                        counter += 1\n",
    "                        if counter == args.patience and epoch > args.min_epochs:\n",
    "                            logging.info(\"Early stopping\")\n",
    "                            break\n",
    "\n",
    "        logging.info(\"Optimization Finished!\")\n",
    "        logging.info(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "        if not best_test_metrics:\n",
    "            model.eval()\n",
    "            best_emb = model.encode(data['features'], data['adj_train_norm'])\n",
    "            best_test_metrics = model.compute_metrics(best_emb, data, 'test')\n",
    "        logging.info(\" \".join(\n",
    "            [\"Val set results:\",\n",
    "            format_metrics(best_val_metrics, 'val')]))\n",
    "        logging.info(\" \".join(\n",
    "            [\"Test set results:\",\n",
    "            format_metrics(best_test_metrics, 'test')]))\n",
    "        if args.save:\n",
    "            np.save(os.path.join(save_dir, 'embeddings.npy'),\n",
    "                    best_emb.cpu().detach().numpy())\n",
    "            if hasattr(model.encoder, 'att_adj'):\n",
    "                filename = os.path.join(save_dir, args.dataset + '_att_adj.p')\n",
    "                pickle.dump(model.encoder.att_adj.cpu().to_dense(),\n",
    "                            open(filename, 'wb'))\n",
    "                print('Dumped attention adj: ' + filename)\n",
    "\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, 'model.pth'))\n",
    "            json.dump(vars(args), open(os.path.join(save_dir, 'config.json'), 'w'))\n",
    "            logging.info(f\"Saved model in {save_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HKNet",
   "language": "python",
   "name": "hknet"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
