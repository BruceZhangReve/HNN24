INFO:root:Using: cuda:7
INFO:root:Using seed 8.
INFO:root:Dataset: texas
INFO:root:Num classes: 5
INFO:root:NCModel(
  (encoder): BKNet(
    (linear_before): BLinear(in_features=1703, out_features=32, c=tensor([1.], device='cuda:7'), use_bias=True, act=None)
    (layers): Sequential(
      (0): KPGraphConvolution(
        (net): KernelPointAggregation(
          (linears): ModuleList(
            (0): BLinear(in_features=32, out_features=32, c=tensor([1.], device='cuda:7'), use_bias=1, act=None)
            (1): BLinear(in_features=32, out_features=32, c=tensor([1.], device='cuda:7'), use_bias=1, act=None)
            (2): BLinear(in_features=32, out_features=32, c=tensor([1.], device='cuda:7'), use_bias=1, act=None)
          )
          (MLP_f): BMLP(
            (linear1): BLinear(in_features=32, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=<function relu at 0x7f1b2d27f6d0>)
            (linear2): BLinear(in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None)
          )
          (MLP_fi): BMLP(
            (linear1): BLinear(in_features=64, out_features=32, c=tensor([1.], device='cuda:7'), use_bias=1, act=<function relu at 0x7f1b2d27f6d0>)
            (linear2): BLinear(in_features=32, out_features=32, c=tensor([1.], device='cuda:7'), use_bias=1, act=None)
          )
        )
      )
    )
  )
  (decoder): PoincareDecoder()
)
INFO:root:Total number of parameters: 67397
INFO:root:Epoch: 0010 lr: [0.001, 0.001] train_loss: 1.404434 train_acc: 0.581967 train_f1: 0.581967 time: 0.0781s
INFO:root:Epoch: 0010 val_loss: 1.293954 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0020 lr: [0.001, 0.001] train_loss: 1.230723 train_acc: 0.581967 train_f1: 0.581967 time: 0.0752s
INFO:root:Epoch: 0020 val_loss: 1.106825 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0030 lr: [0.001, 0.001] train_loss: 1.189932 train_acc: 0.581967 train_f1: 0.581967 time: 0.0763s
INFO:root:Epoch: 0030 val_loss: 1.025232 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0040 lr: [0.001, 0.001] train_loss: 1.147985 train_acc: 0.581967 train_f1: 0.581967 time: 0.0756s
INFO:root:Epoch: 0040 val_loss: 1.003436 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0050 lr: [0.001, 0.001] train_loss: 1.115311 train_acc: 0.581967 train_f1: 0.581967 time: 0.0777s
INFO:root:Epoch: 0050 val_loss: 0.975514 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0060 lr: [0.001, 0.001] train_loss: 1.058132 train_acc: 0.581967 train_f1: 0.581967 time: 0.0750s
INFO:root:Epoch: 0060 val_loss: 0.915295 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0070 lr: [0.001, 0.001] train_loss: 0.970899 train_acc: 0.581967 train_f1: 0.581967 time: 0.0756s
INFO:root:Epoch: 0070 val_loss: 0.815234 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0080 lr: [0.001, 0.001] train_loss: 0.859023 train_acc: 0.581967 train_f1: 0.581967 time: 0.0880s
INFO:root:Epoch: 0080 val_loss: 0.719792 val_acc: 0.772727 val_f1: 0.772727
INFO:root:Epoch: 0090 lr: [0.001, 0.001] train_loss: 0.808791 train_acc: 0.786885 train_f1: 0.786885 time: 0.0921s
INFO:root:Epoch: 0090 val_loss: 0.662801 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0100 lr: [0.0005, 0.0005] train_loss: 0.749039 train_acc: 0.786885 train_f1: 0.786885 time: 0.0754s
INFO:root:Epoch: 0100 val_loss: 0.578515 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0110 lr: [0.0005, 0.0005] train_loss: 0.690424 train_acc: 0.786885 train_f1: 0.786885 time: 0.0765s
INFO:root:Epoch: 0110 val_loss: 0.569866 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0120 lr: [0.0005, 0.0005] train_loss: 0.663112 train_acc: 0.786885 train_f1: 0.786885 time: 0.0760s
INFO:root:Epoch: 0120 val_loss: 0.539260 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0130 lr: [0.0005, 0.0005] train_loss: 0.650010 train_acc: 0.786885 train_f1: 0.786885 time: 0.0753s
INFO:root:Epoch: 0130 val_loss: 0.528598 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0140 lr: [0.0005, 0.0005] train_loss: 0.638968 train_acc: 0.786885 train_f1: 0.786885 time: 0.0755s
INFO:root:Epoch: 0140 val_loss: 0.519122 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0150 lr: [0.0005, 0.0005] train_loss: 0.641206 train_acc: 0.786885 train_f1: 0.786885 time: 0.0847s
INFO:root:Epoch: 0150 val_loss: 0.492562 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0160 lr: [0.0005, 0.0005] train_loss: 0.626430 train_acc: 0.786885 train_f1: 0.786885 time: 0.0759s
INFO:root:Epoch: 0160 val_loss: 0.488282 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0170 lr: [0.0005, 0.0005] train_loss: 0.612845 train_acc: 0.786885 train_f1: 0.786885 time: 0.0795s
INFO:root:Epoch: 0170 val_loss: 0.470356 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0180 lr: [0.0005, 0.0005] train_loss: 0.600684 train_acc: 0.786885 train_f1: 0.786885 time: 0.0756s
INFO:root:Epoch: 0180 val_loss: 0.472125 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0190 lr: [0.0005, 0.0005] train_loss: 0.606822 train_acc: 0.786885 train_f1: 0.786885 time: 0.0952s
INFO:root:Epoch: 0190 val_loss: 0.449034 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0200 lr: [0.00025, 0.00025] train_loss: 0.581520 train_acc: 0.786885 train_f1: 0.786885 time: 0.0989s
INFO:root:Epoch: 0200 val_loss: 0.443196 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0210 lr: [0.00025, 0.00025] train_loss: 0.598592 train_acc: 0.786885 train_f1: 0.786885 time: 0.0955s
INFO:root:Epoch: 0210 val_loss: 0.481446 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0220 lr: [0.00025, 0.00025] train_loss: 0.552383 train_acc: 0.786885 train_f1: 0.786885 time: 0.0950s
INFO:root:Epoch: 0220 val_loss: 0.433385 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0230 lr: [0.00025, 0.00025] train_loss: 0.594385 train_acc: 0.786885 train_f1: 0.786885 time: 0.1010s
INFO:root:Epoch: 0230 val_loss: 0.431931 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0240 lr: [0.00025, 0.00025] train_loss: 0.576119 train_acc: 0.786885 train_f1: 0.786885 time: 0.0960s
INFO:root:Epoch: 0240 val_loss: 0.433480 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0250 lr: [0.00025, 0.00025] train_loss: 0.576204 train_acc: 0.786885 train_f1: 0.786885 time: 0.0946s
INFO:root:Epoch: 0250 val_loss: 0.438874 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0260 lr: [0.00025, 0.00025] train_loss: 0.581377 train_acc: 0.786885 train_f1: 0.786885 time: 0.0947s
INFO:root:Epoch: 0260 val_loss: 0.425314 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0270 lr: [0.00025, 0.00025] train_loss: 0.546254 train_acc: 0.786885 train_f1: 0.786885 time: 0.1023s
INFO:root:Epoch: 0270 val_loss: 0.451151 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0280 lr: [0.00025, 0.00025] train_loss: 0.582314 train_acc: 0.786885 train_f1: 0.786885 time: 0.0955s
INFO:root:Epoch: 0280 val_loss: 0.418398 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0290 lr: [0.00025, 0.00025] train_loss: 0.566227 train_acc: 0.786885 train_f1: 0.786885 time: 0.0963s
INFO:root:Epoch: 0290 val_loss: 0.427410 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0300 lr: [0.000125, 0.000125] train_loss: 0.531561 train_acc: 0.786885 train_f1: 0.786885 time: 0.0986s
INFO:root:Epoch: 0300 val_loss: 0.412632 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0310 lr: [0.000125, 0.000125] train_loss: 0.536244 train_acc: 0.786885 train_f1: 0.786885 time: 0.0972s
INFO:root:Epoch: 0310 val_loss: 0.421717 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0320 lr: [0.000125, 0.000125] train_loss: 0.541848 train_acc: 0.786885 train_f1: 0.786885 time: 0.0955s
INFO:root:Epoch: 0320 val_loss: 0.413008 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0330 lr: [0.000125, 0.000125] train_loss: 0.579128 train_acc: 0.786885 train_f1: 0.786885 time: 0.0961s
INFO:root:Epoch: 0330 val_loss: 0.412946 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0340 lr: [0.000125, 0.000125] train_loss: 0.549310 train_acc: 0.786885 train_f1: 0.786885 time: 0.1015s
INFO:root:Epoch: 0340 val_loss: 0.423722 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0350 lr: [0.000125, 0.000125] train_loss: 0.554336 train_acc: 0.786885 train_f1: 0.786885 time: 0.0959s
INFO:root:Epoch: 0350 val_loss: 0.416766 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0360 lr: [0.000125, 0.000125] train_loss: 0.553991 train_acc: 0.786885 train_f1: 0.786885 time: 0.0995s
INFO:root:Epoch: 0360 val_loss: 0.422160 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0370 lr: [0.000125, 0.000125] train_loss: 0.563682 train_acc: 0.786885 train_f1: 0.786885 time: 0.1001s
INFO:root:Epoch: 0370 val_loss: 0.414575 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0380 lr: [0.000125, 0.000125] train_loss: 0.560013 train_acc: 0.786885 train_f1: 0.786885 time: 0.0954s
INFO:root:Epoch: 0380 val_loss: 0.417234 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0390 lr: [0.000125, 0.000125] train_loss: 0.548278 train_acc: 0.786885 train_f1: 0.786885 time: 0.0952s
INFO:root:Epoch: 0390 val_loss: 0.421394 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0400 lr: [6.25e-05, 6.25e-05] train_loss: 0.551870 train_acc: 0.786885 train_f1: 0.786885 time: 0.0970s
INFO:root:Epoch: 0400 val_loss: 0.413458 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0410 lr: [6.25e-05, 6.25e-05] train_loss: 0.554022 train_acc: 0.786885 train_f1: 0.786885 time: 0.0952s
INFO:root:Epoch: 0410 val_loss: 0.423116 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0420 lr: [6.25e-05, 6.25e-05] train_loss: 0.528186 train_acc: 0.786885 train_f1: 0.786885 time: 0.0959s
INFO:root:Epoch: 0420 val_loss: 0.416426 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0430 lr: [6.25e-05, 6.25e-05] train_loss: 0.555466 train_acc: 0.786885 train_f1: 0.786885 time: 0.0951s
INFO:root:Epoch: 0430 val_loss: 0.425612 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0440 lr: [6.25e-05, 6.25e-05] train_loss: 0.538424 train_acc: 0.786885 train_f1: 0.786885 time: 0.0992s
INFO:root:Epoch: 0440 val_loss: 0.417038 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0450 lr: [6.25e-05, 6.25e-05] train_loss: 0.541371 train_acc: 0.786885 train_f1: 0.786885 time: 0.0961s
INFO:root:Epoch: 0450 val_loss: 0.415333 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0460 lr: [6.25e-05, 6.25e-05] train_loss: 0.520703 train_acc: 0.786885 train_f1: 0.786885 time: 0.0959s
INFO:root:Epoch: 0460 val_loss: 0.429249 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0470 lr: [6.25e-05, 6.25e-05] train_loss: 0.535418 train_acc: 0.786885 train_f1: 0.786885 time: 0.0973s
INFO:root:Epoch: 0470 val_loss: 0.417278 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0480 lr: [6.25e-05, 6.25e-05] train_loss: 0.549936 train_acc: 0.786885 train_f1: 0.786885 time: 0.0974s
INFO:root:Epoch: 0480 val_loss: 0.422947 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0490 lr: [6.25e-05, 6.25e-05] train_loss: 0.532215 train_acc: 0.786885 train_f1: 0.786885 time: 0.0971s
INFO:root:Epoch: 0490 val_loss: 0.425095 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0500 lr: [3.125e-05, 3.125e-05] train_loss: 0.544894 train_acc: 0.786885 train_f1: 0.786885 time: 0.0950s
INFO:root:Epoch: 0500 val_loss: 0.423814 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0510 lr: [3.125e-05, 3.125e-05] train_loss: 0.556404 train_acc: 0.786885 train_f1: 0.786885 time: 0.0911s
INFO:root:Epoch: 0510 val_loss: 0.432156 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0520 lr: [3.125e-05, 3.125e-05] train_loss: 0.546995 train_acc: 0.786885 train_f1: 0.786885 time: 0.0847s
INFO:root:Epoch: 0520 val_loss: 0.417723 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0530 lr: [3.125e-05, 3.125e-05] train_loss: 0.537803 train_acc: 0.786885 train_f1: 0.786885 time: 0.0851s
INFO:root:Epoch: 0530 val_loss: 0.414712 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0540 lr: [3.125e-05, 3.125e-05] train_loss: 0.528118 train_acc: 0.786885 train_f1: 0.786885 time: 0.0863s
INFO:root:Epoch: 0540 val_loss: 0.416586 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0550 lr: [3.125e-05, 3.125e-05] train_loss: 0.516459 train_acc: 0.786885 train_f1: 0.786885 time: 0.0849s
INFO:root:Epoch: 0550 val_loss: 0.414698 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0560 lr: [3.125e-05, 3.125e-05] train_loss: 0.537700 train_acc: 0.786885 train_f1: 0.786885 time: 0.0869s
INFO:root:Epoch: 0560 val_loss: 0.420421 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0570 lr: [3.125e-05, 3.125e-05] train_loss: 0.530760 train_acc: 0.786885 train_f1: 0.786885 time: 0.0881s
INFO:root:Epoch: 0570 val_loss: 0.419526 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0580 lr: [3.125e-05, 3.125e-05] train_loss: 0.543593 train_acc: 0.786885 train_f1: 0.786885 time: 0.0870s
INFO:root:Epoch: 0580 val_loss: 0.418473 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Epoch: 0590 lr: [3.125e-05, 3.125e-05] train_loss: 0.540492 train_acc: 0.786885 train_f1: 0.786885 time: 0.0958s
INFO:root:Epoch: 0590 val_loss: 0.419212 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Early stopping
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 56.0083s
INFO:root:Val set results: val_loss: 0.662801 val_acc: 0.795455 val_f1: 0.795455
INFO:root:Test set results: test_loss: 0.705040 test_acc: 0.795455 test_f1: 0.795455
