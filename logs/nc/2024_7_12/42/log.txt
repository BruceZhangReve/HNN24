INFO:root:Using: cuda:7
INFO:root:Using seed 7.
INFO:root:Dataset: texas
INFO:root:Num classes: 5
INFO:root:NCModel(
  (encoder): BKNet(
    (linear_before): BLinear(in_features=1703, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=True, act=None)
    (layers): Sequential(
      (0): KPGraphConvolution(
        (net): KernelPointAggregation(
          (linears): ModuleList(
            (0): BLinear(in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None)
            (1): BLinear(in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None)
            (2): BLinear(in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None)
            (3): BLinear(in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None)
          )
          (MLP_f): BMLP(
            (linear1): BLinear(in_features=64, out_features=100, c=tensor([1.], device='cuda:7'), use_bias=1, act=None)
            (act): BAct(c=tensor([1.], device='cuda:7'), act=<function relu at 0x7f1baa12b6d0>)
            (linear2): BLinear(in_features=100, out_features=100, c=tensor([1.], device='cuda:7'), use_bias=1, act=None)
          )
          (MLP_fi): BMLP(
            (linear1): BLinear(in_features=100, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None)
            (act): BAct(c=tensor([1.], device='cuda:7'), act=<function relu at 0x7f1baa12b6d0>)
            (linear2): BLinear(in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None)
          )
        )
      )
    )
  )
  (decoder): PoincareDecoder()
)
INFO:root:Total number of parameters: 153565
INFO:root:Epoch: 0010 lr: [0.0005, 0.0005] train_loss: 1.388238 train_acc: 0.606557 train_f1: 0.606557 time: 0.1150s
INFO:root:Epoch: 0010 val_loss: 1.374298 val_acc: 0.590909 val_f1: 0.590909
INFO:root:Epoch: 0020 lr: [0.0005, 0.0005] train_loss: 1.250576 train_acc: 0.606557 train_f1: 0.606557 time: 0.1155s
INFO:root:Epoch: 0020 val_loss: 1.242811 val_acc: 0.590909 val_f1: 0.590909
INFO:root:Epoch: 0030 lr: [0.0005, 0.0005] train_loss: 1.186753 train_acc: 0.606557 train_f1: 0.606557 time: 0.1153s
INFO:root:Epoch: 0030 val_loss: 1.177267 val_acc: 0.590909 val_f1: 0.590909
INFO:root:Epoch: 0040 lr: [0.0005, 0.0005] train_loss: 1.148273 train_acc: 0.606557 train_f1: 0.606557 time: 0.1149s
INFO:root:Epoch: 0040 val_loss: 1.142657 val_acc: 0.590909 val_f1: 0.590909
INFO:root:Epoch: 0050 lr: [0.0005, 0.0005] train_loss: 1.135744 train_acc: 0.606557 train_f1: 0.606557 time: 0.1194s
INFO:root:Epoch: 0050 val_loss: 1.117502 val_acc: 0.590909 val_f1: 0.590909
INFO:root:Epoch: 0060 lr: [0.0005, 0.0005] train_loss: 1.116490 train_acc: 0.606557 train_f1: 0.606557 time: 0.1240s
INFO:root:Epoch: 0060 val_loss: 1.100376 val_acc: 0.590909 val_f1: 0.590909
INFO:root:Epoch: 0070 lr: [0.0005, 0.0005] train_loss: 1.107576 train_acc: 0.606557 train_f1: 0.606557 time: 0.1167s
INFO:root:Epoch: 0070 val_loss: 1.080045 val_acc: 0.590909 val_f1: 0.590909
INFO:root:Epoch: 0080 lr: [0.0005, 0.0005] train_loss: 1.061620 train_acc: 0.606557 train_f1: 0.606557 time: 0.1151s
INFO:root:Epoch: 0080 val_loss: 1.032773 val_acc: 0.590909 val_f1: 0.590909
INFO:root:Epoch: 0090 lr: [0.0005, 0.0005] train_loss: 1.003155 train_acc: 0.606557 train_f1: 0.606557 time: 0.1156s
INFO:root:Epoch: 0090 val_loss: 0.948839 val_acc: 0.590909 val_f1: 0.590909
INFO:root:Epoch: 0100 lr: [0.00025, 0.00025] train_loss: 0.937405 train_acc: 0.606557 train_f1: 0.606557 time: 0.1142s
INFO:root:Epoch: 0100 val_loss: 0.858182 val_acc: 0.590909 val_f1: 0.590909
INFO:root:Epoch: 0110 lr: [0.00025, 0.00025] train_loss: 0.893959 train_acc: 0.606557 train_f1: 0.606557 time: 0.1220s
INFO:root:Epoch: 0110 val_loss: 0.813799 val_acc: 0.590909 val_f1: 0.590909
INFO:root:Epoch: 0120 lr: [0.00025, 0.00025] train_loss: 0.838259 train_acc: 0.750000 train_f1: 0.750000 time: 0.1242s
INFO:root:Epoch: 0120 val_loss: 0.777171 val_acc: 0.772727 val_f1: 0.772727
INFO:root:Epoch: 0130 lr: [0.00025, 0.00025] train_loss: 0.847530 train_acc: 0.606557 train_f1: 0.606557 time: 0.1228s
INFO:root:Epoch: 0130 val_loss: 0.750732 val_acc: 0.772727 val_f1: 0.772727
INFO:root:Epoch: 0140 lr: [0.00025, 0.00025] train_loss: 0.783991 train_acc: 0.778689 train_f1: 0.778689 time: 0.1149s
INFO:root:Epoch: 0140 val_loss: 0.722361 val_acc: 0.772727 val_f1: 0.772727
INFO:root:Epoch: 0150 lr: [0.00025, 0.00025] train_loss: 0.765638 train_acc: 0.778689 train_f1: 0.778689 time: 0.1174s
INFO:root:Epoch: 0150 val_loss: 0.692578 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0160 lr: [0.00025, 0.00025] train_loss: 0.756627 train_acc: 0.778689 train_f1: 0.778689 time: 0.1162s
INFO:root:Epoch: 0160 val_loss: 0.674128 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0170 lr: [0.00025, 0.00025] train_loss: 0.748483 train_acc: 0.778689 train_f1: 0.778689 time: 0.1812s
INFO:root:Epoch: 0170 val_loss: 0.651294 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0180 lr: [0.00025, 0.00025] train_loss: 0.706550 train_acc: 0.778689 train_f1: 0.778689 time: 0.1151s
INFO:root:Epoch: 0180 val_loss: 0.630780 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0190 lr: [0.00025, 0.00025] train_loss: 0.710052 train_acc: 0.778689 train_f1: 0.778689 time: 0.1199s
INFO:root:Epoch: 0190 val_loss: 0.614120 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0200 lr: [0.000125, 0.000125] train_loss: 0.663705 train_acc: 0.778689 train_f1: 0.778689 time: 0.1240s
INFO:root:Epoch: 0200 val_loss: 0.597998 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0210 lr: [0.000125, 0.000125] train_loss: 0.671823 train_acc: 0.778689 train_f1: 0.778689 time: 0.1148s
INFO:root:Epoch: 0210 val_loss: 0.589782 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0220 lr: [0.000125, 0.000125] train_loss: 0.663455 train_acc: 0.778689 train_f1: 0.778689 time: 0.1162s
INFO:root:Epoch: 0220 val_loss: 0.582689 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0230 lr: [0.000125, 0.000125] train_loss: 0.658689 train_acc: 0.778689 train_f1: 0.778689 time: 0.1152s
INFO:root:Epoch: 0230 val_loss: 0.575979 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0240 lr: [0.000125, 0.000125] train_loss: 0.644422 train_acc: 0.778689 train_f1: 0.778689 time: 0.1157s
INFO:root:Epoch: 0240 val_loss: 0.569874 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0250 lr: [0.000125, 0.000125] train_loss: 0.633677 train_acc: 0.778689 train_f1: 0.778689 time: 0.1245s
INFO:root:Epoch: 0250 val_loss: 0.563691 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0260 lr: [0.000125, 0.000125] train_loss: 0.633500 train_acc: 0.778689 train_f1: 0.778689 time: 0.1224s
INFO:root:Epoch: 0260 val_loss: 0.557590 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0270 lr: [0.000125, 0.000125] train_loss: 0.623861 train_acc: 0.778689 train_f1: 0.778689 time: 0.1181s
INFO:root:Epoch: 0270 val_loss: 0.551531 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0280 lr: [0.000125, 0.000125] train_loss: 0.626641 train_acc: 0.778689 train_f1: 0.778689 time: 0.1151s
INFO:root:Epoch: 0280 val_loss: 0.544929 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0290 lr: [0.000125, 0.000125] train_loss: 0.605081 train_acc: 0.778689 train_f1: 0.778689 time: 0.1152s
INFO:root:Epoch: 0290 val_loss: 0.540263 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0300 lr: [6.25e-05, 6.25e-05] train_loss: 0.610533 train_acc: 0.778689 train_f1: 0.778689 time: 0.1153s
INFO:root:Epoch: 0300 val_loss: 0.535913 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0310 lr: [6.25e-05, 6.25e-05] train_loss: 0.613606 train_acc: 0.778689 train_f1: 0.778689 time: 0.1152s
INFO:root:Epoch: 0310 val_loss: 0.533351 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0320 lr: [6.25e-05, 6.25e-05] train_loss: 0.607845 train_acc: 0.778689 train_f1: 0.778689 time: 0.1329s
INFO:root:Epoch: 0320 val_loss: 0.530571 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0330 lr: [6.25e-05, 6.25e-05] train_loss: 0.620624 train_acc: 0.778689 train_f1: 0.778689 time: 0.1287s
INFO:root:Epoch: 0330 val_loss: 0.528128 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0340 lr: [6.25e-05, 6.25e-05] train_loss: 0.609702 train_acc: 0.778689 train_f1: 0.778689 time: 0.1158s
INFO:root:Epoch: 0340 val_loss: 0.526017 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0350 lr: [6.25e-05, 6.25e-05] train_loss: 0.591743 train_acc: 0.778689 train_f1: 0.778689 time: 0.1154s
INFO:root:Epoch: 0350 val_loss: 0.523514 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0360 lr: [6.25e-05, 6.25e-05] train_loss: 0.621751 train_acc: 0.778689 train_f1: 0.778689 time: 0.1198s
INFO:root:Epoch: 0360 val_loss: 0.521446 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0370 lr: [6.25e-05, 6.25e-05] train_loss: 0.592546 train_acc: 0.778689 train_f1: 0.778689 time: 0.1166s
INFO:root:Epoch: 0370 val_loss: 0.519245 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0380 lr: [6.25e-05, 6.25e-05] train_loss: 0.582975 train_acc: 0.778689 train_f1: 0.778689 time: 0.1156s
INFO:root:Epoch: 0380 val_loss: 0.516922 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0390 lr: [6.25e-05, 6.25e-05] train_loss: 0.578613 train_acc: 0.778689 train_f1: 0.778689 time: 0.1227s
INFO:root:Epoch: 0390 val_loss: 0.515090 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0400 lr: [3.125e-05, 3.125e-05] train_loss: 0.572998 train_acc: 0.778689 train_f1: 0.778689 time: 0.1183s
INFO:root:Epoch: 0400 val_loss: 0.513540 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0410 lr: [3.125e-05, 3.125e-05] train_loss: 0.585965 train_acc: 0.778689 train_f1: 0.778689 time: 0.1170s
INFO:root:Epoch: 0410 val_loss: 0.512395 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0420 lr: [3.125e-05, 3.125e-05] train_loss: 0.587595 train_acc: 0.778689 train_f1: 0.778689 time: 0.1154s
INFO:root:Epoch: 0420 val_loss: 0.511282 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0430 lr: [3.125e-05, 3.125e-05] train_loss: 0.572609 train_acc: 0.778689 train_f1: 0.778689 time: 0.1162s
INFO:root:Epoch: 0430 val_loss: 0.510090 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0440 lr: [3.125e-05, 3.125e-05] train_loss: 0.586352 train_acc: 0.778689 train_f1: 0.778689 time: 0.1164s
INFO:root:Epoch: 0440 val_loss: 0.508806 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0450 lr: [3.125e-05, 3.125e-05] train_loss: 0.566392 train_acc: 0.778689 train_f1: 0.778689 time: 0.1216s
INFO:root:Epoch: 0450 val_loss: 0.507481 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0460 lr: [3.125e-05, 3.125e-05] train_loss: 0.577258 train_acc: 0.778689 train_f1: 0.778689 time: 0.1253s
INFO:root:Epoch: 0460 val_loss: 0.506454 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0470 lr: [3.125e-05, 3.125e-05] train_loss: 0.575697 train_acc: 0.778689 train_f1: 0.778689 time: 0.1210s
INFO:root:Epoch: 0470 val_loss: 0.505451 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0480 lr: [3.125e-05, 3.125e-05] train_loss: 0.573305 train_acc: 0.778689 train_f1: 0.778689 time: 0.1173s
INFO:root:Epoch: 0480 val_loss: 0.504553 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0490 lr: [3.125e-05, 3.125e-05] train_loss: 0.597418 train_acc: 0.778689 train_f1: 0.778689 time: 0.1162s
INFO:root:Epoch: 0490 val_loss: 0.504069 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0500 lr: [1.5625e-05, 1.5625e-05] train_loss: 0.578648 train_acc: 0.778689 train_f1: 0.778689 time: 0.1155s
INFO:root:Epoch: 0500 val_loss: 0.503680 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0510 lr: [1.5625e-05, 1.5625e-05] train_loss: 0.572612 train_acc: 0.778689 train_f1: 0.778689 time: 0.1167s
INFO:root:Epoch: 0510 val_loss: 0.503410 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0520 lr: [1.5625e-05, 1.5625e-05] train_loss: 0.571818 train_acc: 0.778689 train_f1: 0.778689 time: 0.1229s
INFO:root:Epoch: 0520 val_loss: 0.503042 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0530 lr: [1.5625e-05, 1.5625e-05] train_loss: 0.569285 train_acc: 0.778689 train_f1: 0.778689 time: 0.1261s
INFO:root:Epoch: 0530 val_loss: 0.502658 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0540 lr: [1.5625e-05, 1.5625e-05] train_loss: 0.573928 train_acc: 0.778689 train_f1: 0.778689 time: 0.1154s
INFO:root:Epoch: 0540 val_loss: 0.502396 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0550 lr: [1.5625e-05, 1.5625e-05] train_loss: 0.585082 train_acc: 0.778689 train_f1: 0.778689 time: 0.1159s
INFO:root:Epoch: 0550 val_loss: 0.502105 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0560 lr: [1.5625e-05, 1.5625e-05] train_loss: 0.575881 train_acc: 0.778689 train_f1: 0.778689 time: 0.1158s
INFO:root:Epoch: 0560 val_loss: 0.501610 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0570 lr: [1.5625e-05, 1.5625e-05] train_loss: 0.579370 train_acc: 0.778689 train_f1: 0.778689 time: 0.1194s
INFO:root:Epoch: 0570 val_loss: 0.500951 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0580 lr: [1.5625e-05, 1.5625e-05] train_loss: 0.578205 train_acc: 0.778689 train_f1: 0.778689 time: 0.1153s
INFO:root:Epoch: 0580 val_loss: 0.500352 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0590 lr: [1.5625e-05, 1.5625e-05] train_loss: 0.576820 train_acc: 0.778689 train_f1: 0.778689 time: 0.1184s
INFO:root:Epoch: 0590 val_loss: 0.499759 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0600 lr: [7.8125e-06, 7.8125e-06] train_loss: 0.575788 train_acc: 0.778689 train_f1: 0.778689 time: 0.1234s
INFO:root:Epoch: 0600 val_loss: 0.499128 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0610 lr: [7.8125e-06, 7.8125e-06] train_loss: 0.569244 train_acc: 0.778689 train_f1: 0.778689 time: 0.1193s
INFO:root:Epoch: 0610 val_loss: 0.498884 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0620 lr: [7.8125e-06, 7.8125e-06] train_loss: 0.558108 train_acc: 0.778689 train_f1: 0.778689 time: 0.1160s
INFO:root:Epoch: 0620 val_loss: 0.498687 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0630 lr: [7.8125e-06, 7.8125e-06] train_loss: 0.557370 train_acc: 0.778689 train_f1: 0.778689 time: 0.1165s
INFO:root:Epoch: 0630 val_loss: 0.498456 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0640 lr: [7.8125e-06, 7.8125e-06] train_loss: 0.582267 train_acc: 0.778689 train_f1: 0.778689 time: 0.1164s
INFO:root:Epoch: 0640 val_loss: 0.498246 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Epoch: 0650 lr: [7.8125e-06, 7.8125e-06] train_loss: 0.570359 train_acc: 0.778689 train_f1: 0.778689 time: 0.1159s
INFO:root:Epoch: 0650 val_loss: 0.498082 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Early stopping
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 79.8025s
INFO:root:Val set results: val_loss: 0.692578 val_acc: 0.818182 val_f1: 0.818182
INFO:root:Test set results: test_loss: 0.673022 test_acc: 0.818182 test_f1: 0.818182
