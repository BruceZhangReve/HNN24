INFO:root:Using: cuda:7
INFO:root:Using seed 1119.
INFO:root:Dataset: texas
INFO:root:Num classes: 5
INFO:root:NCModel(
  (encoder): BKNet(
    (linear_before): BLinear(
      in_features=1703, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=False, act=None, dropout_rate=0.3
      (dropout): Dropout(p=0.3, inplace=False)
      (E_linear): Linear(in_features=1703, out_features=64, bias=False)
    )
    (layers): Sequential(
      (0): KPGraphConvolution(
        (net): KernelPointAggregation(
          (linears): ModuleList(
            (0): BLinear(
              in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.3
              (dropout): Dropout(p=0.3, inplace=False)
              (E_linear): Linear(in_features=64, out_features=64, bias=False)
            )
            (1): BLinear(
              in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.3
              (dropout): Dropout(p=0.3, inplace=False)
              (E_linear): Linear(in_features=64, out_features=64, bias=False)
            )
            (2): BLinear(
              in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.3
              (dropout): Dropout(p=0.3, inplace=False)
              (E_linear): Linear(in_features=64, out_features=64, bias=False)
            )
            (3): BLinear(
              in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.3
              (dropout): Dropout(p=0.3, inplace=False)
              (E_linear): Linear(in_features=64, out_features=64, bias=False)
            )
          )
          (MLP_f): BMLP(
            (linear1): BLinear(
              in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.3
              (dropout): Dropout(p=0.3, inplace=False)
              (E_linear): Linear(in_features=64, out_features=64, bias=False)
            )
            (act): BAct(c=tensor([1.], device='cuda:7'), act=<function relu at 0x7f7c73cc76d0>)
            (linear2): BLinear(
              in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.3
              (dropout): Dropout(p=0.3, inplace=False)
              (E_linear): Linear(in_features=64, out_features=64, bias=False)
            )
          )
          (MLP_fi): BMLP(
            (linear1): BLinear(
              in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.3
              (dropout): Dropout(p=0.3, inplace=False)
              (E_linear): Linear(in_features=64, out_features=64, bias=False)
            )
            (act): BAct(c=tensor([1.], device='cuda:7'), act=<function relu at 0x7f7c73cc76d0>)
            (linear2): BLinear(
              in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.3
              (dropout): Dropout(p=0.3, inplace=False)
              (E_linear): Linear(in_features=64, out_features=64, bias=False)
            )
          )
        )
      )
      (1): KPGraphConvolution(
        (net): KernelPointAggregation(
          (linears): ModuleList(
            (0): BLinear(
              in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.3
              (dropout): Dropout(p=0.3, inplace=False)
              (E_linear): Linear(in_features=64, out_features=64, bias=False)
            )
            (1): BLinear(
              in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.3
              (dropout): Dropout(p=0.3, inplace=False)
              (E_linear): Linear(in_features=64, out_features=64, bias=False)
            )
            (2): BLinear(
              in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.3
              (dropout): Dropout(p=0.3, inplace=False)
              (E_linear): Linear(in_features=64, out_features=64, bias=False)
            )
            (3): BLinear(
              in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.3
              (dropout): Dropout(p=0.3, inplace=False)
              (E_linear): Linear(in_features=64, out_features=64, bias=False)
            )
          )
          (MLP_f): BMLP(
            (linear1): BLinear(
              in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.3
              (dropout): Dropout(p=0.3, inplace=False)
              (E_linear): Linear(in_features=64, out_features=64, bias=False)
            )
            (act): BAct(c=tensor([1.], device='cuda:7'), act=<function relu at 0x7f7c73cc76d0>)
            (linear2): BLinear(
              in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.3
              (dropout): Dropout(p=0.3, inplace=False)
              (E_linear): Linear(in_features=64, out_features=64, bias=False)
            )
          )
          (MLP_fi): BMLP(
            (linear1): BLinear(
              in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.3
              (dropout): Dropout(p=0.3, inplace=False)
              (E_linear): Linear(in_features=64, out_features=64, bias=False)
            )
            (act): BAct(c=tensor([1.], device='cuda:7'), act=<function relu at 0x7f7c73cc76d0>)
            (linear2): BLinear(
              in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.3
              (dropout): Dropout(p=0.3, inplace=False)
              (E_linear): Linear(in_features=64, out_features=64, bias=False)
            )
          )
        )
      )
    )
  )
  (decoder): PoincareDecoder()
)
INFO:root:Total number of parameters: 176453
INFO:root:Epoch: 0005 lr: [0.0002, 0.0002] train_loss: 1.552755 train_acc: 0.622951 train_f1: 0.622951 time: 0.2035s
INFO:root:Epoch: 0005 val_loss: 1.559922 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0010 lr: [0.0002, 0.0002] train_loss: 1.491617 train_acc: 0.622951 train_f1: 0.622951 time: 0.2014s
INFO:root:Epoch: 0010 val_loss: 1.514290 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0015 lr: [0.0002, 0.0002] train_loss: 1.449801 train_acc: 0.622951 train_f1: 0.622951 time: 0.2063s
INFO:root:Epoch: 0015 val_loss: 1.474599 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0020 lr: [0.0002, 0.0002] train_loss: 1.403436 train_acc: 0.622951 train_f1: 0.622951 time: 0.2061s
INFO:root:Epoch: 0020 val_loss: 1.442215 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0025 lr: [0.0002, 0.0002] train_loss: 1.371237 train_acc: 0.622951 train_f1: 0.622951 time: 0.1973s
INFO:root:Epoch: 0025 val_loss: 1.415154 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0030 lr: [0.0002, 0.0002] train_loss: 1.338774 train_acc: 0.622951 train_f1: 0.622951 time: 0.1976s
INFO:root:Epoch: 0030 val_loss: 1.392292 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0035 lr: [0.0002, 0.0002] train_loss: 1.313747 train_acc: 0.622951 train_f1: 0.622951 time: 0.2137s
INFO:root:Epoch: 0035 val_loss: 1.374334 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0040 lr: [0.0002, 0.0002] train_loss: 1.294647 train_acc: 0.622951 train_f1: 0.622951 time: 0.1989s
INFO:root:Epoch: 0040 val_loss: 1.359627 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0045 lr: [0.0002, 0.0002] train_loss: 1.271441 train_acc: 0.622951 train_f1: 0.622951 time: 0.1982s
INFO:root:Epoch: 0045 val_loss: 1.346843 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0050 lr: [0.0002, 0.0002] train_loss: 1.252450 train_acc: 0.622951 train_f1: 0.622951 time: 0.2073s
INFO:root:Epoch: 0050 val_loss: 1.335680 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0055 lr: [0.0002, 0.0002] train_loss: 1.236031 train_acc: 0.622951 train_f1: 0.622951 time: 0.2055s
INFO:root:Epoch: 0055 val_loss: 1.326132 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0060 lr: [0.0002, 0.0002] train_loss: 1.215270 train_acc: 0.622951 train_f1: 0.622951 time: 0.1981s
INFO:root:Epoch: 0060 val_loss: 1.317721 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0065 lr: [0.0002, 0.0002] train_loss: 1.213639 train_acc: 0.622951 train_f1: 0.622951 time: 0.1972s
INFO:root:Epoch: 0065 val_loss: 1.310122 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0070 lr: [0.0002, 0.0002] train_loss: 1.188744 train_acc: 0.622951 train_f1: 0.622951 time: 0.2164s
INFO:root:Epoch: 0070 val_loss: 1.302846 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0075 lr: [0.0002, 0.0002] train_loss: 1.181923 train_acc: 0.622951 train_f1: 0.622951 time: 0.2017s
INFO:root:Epoch: 0075 val_loss: 1.295482 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0080 lr: [0.0002, 0.0002] train_loss: 1.169309 train_acc: 0.622951 train_f1: 0.622951 time: 0.1973s
INFO:root:Epoch: 0080 val_loss: 1.288709 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0085 lr: [0.0002, 0.0002] train_loss: 1.152473 train_acc: 0.622951 train_f1: 0.622951 time: 0.2010s
INFO:root:Epoch: 0085 val_loss: 1.280878 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0090 lr: [0.0002, 0.0002] train_loss: 1.140100 train_acc: 0.622951 train_f1: 0.622951 time: 0.2164s
INFO:root:Epoch: 0090 val_loss: 1.272253 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0095 lr: [0.0002, 0.0002] train_loss: 1.128295 train_acc: 0.622951 train_f1: 0.622951 time: 0.1980s
INFO:root:Epoch: 0095 val_loss: 1.261671 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0100 lr: [0.0002, 0.0002] train_loss: 1.114686 train_acc: 0.622951 train_f1: 0.622951 time: 0.1979s
INFO:root:Epoch: 0100 val_loss: 1.248240 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0105 lr: [0.0002, 0.0002] train_loss: 1.104383 train_acc: 0.622951 train_f1: 0.622951 time: 0.2056s
INFO:root:Epoch: 0105 val_loss: 1.233454 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0110 lr: [0.0002, 0.0002] train_loss: 1.078992 train_acc: 0.622951 train_f1: 0.622951 time: 0.2029s
INFO:root:Epoch: 0110 val_loss: 1.216235 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0115 lr: [0.0002, 0.0002] train_loss: 1.056251 train_acc: 0.622951 train_f1: 0.622951 time: 0.1987s
INFO:root:Epoch: 0115 val_loss: 1.207049 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0120 lr: [0.0002, 0.0002] train_loss: 1.055016 train_acc: 0.622951 train_f1: 0.622951 time: 0.1981s
INFO:root:Epoch: 0120 val_loss: 1.182706 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0125 lr: [0.0002, 0.0002] train_loss: 1.020357 train_acc: 0.622951 train_f1: 0.622951 time: 0.2158s
INFO:root:Epoch: 0125 val_loss: 1.165370 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0130 lr: [0.0002, 0.0002] train_loss: 1.019931 train_acc: 0.622951 train_f1: 0.622951 time: 0.1979s
INFO:root:Epoch: 0130 val_loss: 1.152092 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0135 lr: [0.0002, 0.0002] train_loss: 0.997069 train_acc: 0.622951 train_f1: 0.622951 time: 0.1985s
INFO:root:Epoch: 0135 val_loss: 1.151303 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0140 lr: [0.0002, 0.0002] train_loss: 0.986458 train_acc: 0.622951 train_f1: 0.622951 time: 0.2081s
INFO:root:Epoch: 0140 val_loss: 1.142731 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0145 lr: [0.0002, 0.0002] train_loss: 0.968834 train_acc: 0.622951 train_f1: 0.622951 time: 0.2061s
INFO:root:Epoch: 0145 val_loss: 1.121440 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0150 lr: [0.0002, 0.0002] train_loss: 0.961541 train_acc: 0.622951 train_f1: 0.622951 time: 0.2023s
INFO:root:Epoch: 0150 val_loss: 1.102578 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0155 lr: [0.0002, 0.0002] train_loss: 0.943499 train_acc: 0.622951 train_f1: 0.622951 time: 0.1995s
INFO:root:Epoch: 0155 val_loss: 1.093051 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0160 lr: [0.0002, 0.0002] train_loss: 0.928633 train_acc: 0.622951 train_f1: 0.622951 time: 0.2179s
INFO:root:Epoch: 0160 val_loss: 1.068566 val_acc: 0.522727 val_f1: 0.522727
INFO:root:Epoch: 0165 lr: [0.0002, 0.0002] train_loss: 0.915820 train_acc: 0.663934 train_f1: 0.663934 time: 0.2012s
INFO:root:Epoch: 0165 val_loss: 1.051908 val_acc: 0.613636 val_f1: 0.613636
INFO:root:Epoch: 0170 lr: [0.0002, 0.0002] train_loss: 0.911941 train_acc: 0.663934 train_f1: 0.663934 time: 0.2000s
INFO:root:Epoch: 0170 val_loss: 1.039059 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0175 lr: [0.0002, 0.0002] train_loss: 0.886913 train_acc: 0.717213 train_f1: 0.717213 time: 0.1986s
INFO:root:Epoch: 0175 val_loss: 1.013477 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0180 lr: [0.0002, 0.0002] train_loss: 0.898228 train_acc: 0.688525 train_f1: 0.688525 time: 0.2105s
INFO:root:Epoch: 0180 val_loss: 1.005288 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0185 lr: [0.0002, 0.0002] train_loss: 0.878051 train_acc: 0.717213 train_f1: 0.717213 time: 0.1999s
INFO:root:Epoch: 0185 val_loss: 1.007211 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0190 lr: [0.0002, 0.0002] train_loss: 0.872105 train_acc: 0.721311 train_f1: 0.721311 time: 0.1991s
INFO:root:Epoch: 0190 val_loss: 1.014492 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0195 lr: [0.0002, 0.0002] train_loss: 0.859479 train_acc: 0.725410 train_f1: 0.725410 time: 0.2028s
INFO:root:Epoch: 0195 val_loss: 0.966355 val_acc: 0.681818 val_f1: 0.681818
INFO:root:Epoch: 0200 lr: [0.0002, 0.0002] train_loss: 0.840966 train_acc: 0.758197 train_f1: 0.758197 time: 0.2148s
INFO:root:Epoch: 0200 val_loss: 0.943532 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0205 lr: [0.0002, 0.0002] train_loss: 0.844397 train_acc: 0.758197 train_f1: 0.758197 time: 0.2008s
INFO:root:Epoch: 0205 val_loss: 0.934406 val_acc: 0.681818 val_f1: 0.681818
INFO:root:Epoch: 0210 lr: [0.0002, 0.0002] train_loss: 0.826301 train_acc: 0.754098 train_f1: 0.754098 time: 0.1985s
INFO:root:Epoch: 0210 val_loss: 0.972762 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0215 lr: [0.0002, 0.0002] train_loss: 0.830925 train_acc: 0.745902 train_f1: 0.745902 time: 0.2071s
INFO:root:Epoch: 0215 val_loss: 0.933352 val_acc: 0.681818 val_f1: 0.681818
INFO:root:Epoch: 0220 lr: [0.0002, 0.0002] train_loss: 0.806345 train_acc: 0.774590 train_f1: 0.774590 time: 0.2042s
INFO:root:Epoch: 0220 val_loss: 0.893016 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0225 lr: [0.0002, 0.0002] train_loss: 0.803283 train_acc: 0.778689 train_f1: 0.778689 time: 0.1996s
INFO:root:Epoch: 0225 val_loss: 0.881753 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0230 lr: [0.0002, 0.0002] train_loss: 0.793374 train_acc: 0.778689 train_f1: 0.778689 time: 0.2004s
INFO:root:Epoch: 0230 val_loss: 0.871654 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0235 lr: [0.0002, 0.0002] train_loss: 0.786412 train_acc: 0.774590 train_f1: 0.774590 time: 0.2110s
INFO:root:Epoch: 0235 val_loss: 0.875658 val_acc: 0.681818 val_f1: 0.681818
INFO:root:Epoch: 0240 lr: [0.0002, 0.0002] train_loss: 0.775487 train_acc: 0.786885 train_f1: 0.786885 time: 0.1994s
INFO:root:Epoch: 0240 val_loss: 0.879299 val_acc: 0.681818 val_f1: 0.681818
INFO:root:Epoch: 0245 lr: [0.0002, 0.0002] train_loss: 0.781837 train_acc: 0.782787 train_f1: 0.782787 time: 0.1985s
INFO:root:Epoch: 0245 val_loss: 0.872823 val_acc: 0.681818 val_f1: 0.681818
INFO:root:Epoch: 0250 lr: [0.0002, 0.0002] train_loss: 0.768945 train_acc: 0.774590 train_f1: 0.774590 time: 0.2076s
INFO:root:Epoch: 0250 val_loss: 0.848443 val_acc: 0.681818 val_f1: 0.681818
INFO:root:Epoch: 0255 lr: [0.0002, 0.0002] train_loss: 0.769393 train_acc: 0.770492 train_f1: 0.770492 time: 0.2035s
INFO:root:Epoch: 0255 val_loss: 0.836153 val_acc: 0.681818 val_f1: 0.681818
INFO:root:Epoch: 0260 lr: [0.0002, 0.0002] train_loss: 0.746794 train_acc: 0.786885 train_f1: 0.786885 time: 0.1993s
INFO:root:Epoch: 0260 val_loss: 0.852950 val_acc: 0.681818 val_f1: 0.681818
INFO:root:Epoch: 0265 lr: [0.0002, 0.0002] train_loss: 0.738750 train_acc: 0.786885 train_f1: 0.786885 time: 0.1980s
INFO:root:Epoch: 0265 val_loss: 0.909913 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0270 lr: [0.0002, 0.0002] train_loss: 0.733633 train_acc: 0.786885 train_f1: 0.786885 time: 0.2113s
INFO:root:Epoch: 0270 val_loss: 0.868119 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0275 lr: [0.0002, 0.0002] train_loss: 0.729069 train_acc: 0.786885 train_f1: 0.786885 time: 0.1986s
INFO:root:Epoch: 0275 val_loss: 0.787345 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0280 lr: [0.0002, 0.0002] train_loss: 0.744373 train_acc: 0.778689 train_f1: 0.778689 time: 0.1985s
INFO:root:Epoch: 0280 val_loss: 0.787943 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0285 lr: [0.0002, 0.0002] train_loss: 0.723239 train_acc: 0.786885 train_f1: 0.786885 time: 0.2099s
INFO:root:Epoch: 0285 val_loss: 0.776929 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0290 lr: [0.0002, 0.0002] train_loss: 0.717662 train_acc: 0.786885 train_f1: 0.786885 time: 0.2038s
INFO:root:Epoch: 0290 val_loss: 0.763484 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0295 lr: [0.0002, 0.0002] train_loss: 0.718976 train_acc: 0.774590 train_f1: 0.774590 time: 0.1991s
INFO:root:Epoch: 0295 val_loss: 0.882179 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0300 lr: [0.0002, 0.0002] train_loss: 0.739383 train_acc: 0.782787 train_f1: 0.782787 time: 0.1991s
INFO:root:Epoch: 0300 val_loss: 0.924208 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0305 lr: [0.0002, 0.0002] train_loss: 0.710571 train_acc: 0.778689 train_f1: 0.778689 time: 0.2143s
INFO:root:Epoch: 0305 val_loss: 0.842818 val_acc: 0.681818 val_f1: 0.681818
INFO:root:Epoch: 0310 lr: [0.0002, 0.0002] train_loss: 0.708928 train_acc: 0.786885 train_f1: 0.786885 time: 0.2034s
INFO:root:Epoch: 0310 val_loss: 0.758833 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0315 lr: [0.0002, 0.0002] train_loss: 0.707381 train_acc: 0.786885 train_f1: 0.786885 time: 0.1985s
INFO:root:Epoch: 0315 val_loss: 0.755850 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0320 lr: [0.0002, 0.0002] train_loss: 0.702540 train_acc: 0.778689 train_f1: 0.778689 time: 0.2106s
INFO:root:Epoch: 0320 val_loss: 0.732848 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0325 lr: [0.0002, 0.0002] train_loss: 0.694908 train_acc: 0.786885 train_f1: 0.786885 time: 0.2066s
INFO:root:Epoch: 0325 val_loss: 0.775531 val_acc: 0.681818 val_f1: 0.681818
INFO:root:Epoch: 0330 lr: [0.0002, 0.0002] train_loss: 0.721535 train_acc: 0.774590 train_f1: 0.774590 time: 0.1987s
INFO:root:Epoch: 0330 val_loss: 0.890936 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0335 lr: [0.0002, 0.0002] train_loss: 0.703992 train_acc: 0.778689 train_f1: 0.778689 time: 0.1981s
INFO:root:Epoch: 0335 val_loss: 0.896197 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0340 lr: [0.0002, 0.0002] train_loss: 0.693653 train_acc: 0.782787 train_f1: 0.782787 time: 0.2143s
INFO:root:Epoch: 0340 val_loss: 0.823295 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0345 lr: [0.0002, 0.0002] train_loss: 0.688319 train_acc: 0.786885 train_f1: 0.786885 time: 0.2003s
INFO:root:Epoch: 0345 val_loss: 0.732307 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0350 lr: [0.0002, 0.0002] train_loss: 0.693902 train_acc: 0.778689 train_f1: 0.778689 time: 0.1986s
INFO:root:Epoch: 0350 val_loss: 0.713582 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0355 lr: [0.0002, 0.0002] train_loss: 0.674676 train_acc: 0.786885 train_f1: 0.786885 time: 0.2039s
INFO:root:Epoch: 0355 val_loss: 0.704601 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0360 lr: [0.0002, 0.0002] train_loss: 0.687249 train_acc: 0.786885 train_f1: 0.786885 time: 0.2041s
INFO:root:Epoch: 0360 val_loss: 0.700507 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0365 lr: [0.0002, 0.0002] train_loss: 0.668006 train_acc: 0.786885 train_f1: 0.786885 time: 0.1988s
INFO:root:Epoch: 0365 val_loss: 0.699740 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0370 lr: [0.0002, 0.0002] train_loss: 0.666904 train_acc: 0.786885 train_f1: 0.786885 time: 0.1991s
INFO:root:Epoch: 0370 val_loss: 0.700124 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0375 lr: [0.0002, 0.0002] train_loss: 0.662361 train_acc: 0.786885 train_f1: 0.786885 time: 0.2152s
INFO:root:Epoch: 0375 val_loss: 0.698628 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0380 lr: [0.0002, 0.0002] train_loss: 0.657065 train_acc: 0.786885 train_f1: 0.786885 time: 0.1997s
INFO:root:Epoch: 0380 val_loss: 0.713090 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0385 lr: [0.0002, 0.0002] train_loss: 0.667125 train_acc: 0.786885 train_f1: 0.786885 time: 0.1990s
INFO:root:Epoch: 0385 val_loss: 0.749259 val_acc: 0.681818 val_f1: 0.681818
INFO:root:Epoch: 0390 lr: [0.0002, 0.0002] train_loss: 0.653976 train_acc: 0.786885 train_f1: 0.786885 time: 0.2034s
INFO:root:Epoch: 0390 val_loss: 0.734095 val_acc: 0.681818 val_f1: 0.681818
INFO:root:Epoch: 0395 lr: [0.0002, 0.0002] train_loss: 0.647125 train_acc: 0.786885 train_f1: 0.786885 time: 0.2051s
INFO:root:Epoch: 0395 val_loss: 0.703493 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0400 lr: [0.0002, 0.0002] train_loss: 0.646439 train_acc: 0.786885 train_f1: 0.786885 time: 0.2001s
INFO:root:Epoch: 0400 val_loss: 0.683997 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0405 lr: [0.0002, 0.0002] train_loss: 0.655473 train_acc: 0.786885 train_f1: 0.786885 time: 0.1982s
INFO:root:Epoch: 0405 val_loss: 0.679742 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0410 lr: [0.0002, 0.0002] train_loss: 0.668471 train_acc: 0.778689 train_f1: 0.778689 time: 0.2197s
INFO:root:Epoch: 0410 val_loss: 0.679196 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0415 lr: [0.0002, 0.0002] train_loss: 0.639787 train_acc: 0.786885 train_f1: 0.786885 time: 0.1993s
INFO:root:Epoch: 0415 val_loss: 0.692082 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0420 lr: [0.0002, 0.0002] train_loss: 0.637394 train_acc: 0.786885 train_f1: 0.786885 time: 0.1993s
INFO:root:Epoch: 0420 val_loss: 0.738301 val_acc: 0.681818 val_f1: 0.681818
INFO:root:Epoch: 0425 lr: [0.0002, 0.0002] train_loss: 0.636634 train_acc: 0.786885 train_f1: 0.786885 time: 0.2038s
INFO:root:Epoch: 0425 val_loss: 0.759852 val_acc: 0.681818 val_f1: 0.681818
INFO:root:Epoch: 0430 lr: [0.0002, 0.0002] train_loss: 0.637423 train_acc: 0.786885 train_f1: 0.786885 time: 0.2099s
INFO:root:Epoch: 0430 val_loss: 0.751163 val_acc: 0.681818 val_f1: 0.681818
INFO:root:Epoch: 0435 lr: [0.0002, 0.0002] train_loss: 0.640996 train_acc: 0.786885 train_f1: 0.786885 time: 0.1976s
INFO:root:Epoch: 0435 val_loss: 0.731984 val_acc: 0.681818 val_f1: 0.681818
INFO:root:Epoch: 0440 lr: [0.0002, 0.0002] train_loss: 0.643448 train_acc: 0.786885 train_f1: 0.786885 time: 0.2025s
INFO:root:Epoch: 0440 val_loss: 0.783526 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0445 lr: [0.0002, 0.0002] train_loss: 0.627004 train_acc: 0.786885 train_f1: 0.786885 time: 0.2066s
INFO:root:Epoch: 0445 val_loss: 0.827553 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0450 lr: [0.0002, 0.0002] train_loss: 0.621319 train_acc: 0.786885 train_f1: 0.786885 time: 0.1990s
INFO:root:Epoch: 0450 val_loss: 0.730296 val_acc: 0.681818 val_f1: 0.681818
INFO:root:Epoch: 0455 lr: [0.0002, 0.0002] train_loss: 0.625159 train_acc: 0.786885 train_f1: 0.786885 time: 0.1994s
INFO:root:Epoch: 0455 val_loss: 0.670060 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0460 lr: [0.0002, 0.0002] train_loss: 0.626255 train_acc: 0.786885 train_f1: 0.786885 time: 0.2029s
INFO:root:Epoch: 0460 val_loss: 0.662522 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0465 lr: [0.0002, 0.0002] train_loss: 0.625460 train_acc: 0.782787 train_f1: 0.782787 time: 0.2116s
INFO:root:Epoch: 0465 val_loss: 0.663044 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0470 lr: [0.0002, 0.0002] train_loss: 0.620069 train_acc: 0.786885 train_f1: 0.786885 time: 0.1997s
INFO:root:Epoch: 0470 val_loss: 0.661947 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0475 lr: [0.0002, 0.0002] train_loss: 0.633861 train_acc: 0.778689 train_f1: 0.778689 time: 0.2012s
INFO:root:Epoch: 0475 val_loss: 0.658225 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0480 lr: [0.0002, 0.0002] train_loss: 0.618126 train_acc: 0.786885 train_f1: 0.786885 time: 0.2058s
INFO:root:Epoch: 0480 val_loss: 0.657135 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0485 lr: [0.0002, 0.0002] train_loss: 0.619835 train_acc: 0.786885 train_f1: 0.786885 time: 0.2079s
INFO:root:Epoch: 0485 val_loss: 0.662608 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0490 lr: [0.0002, 0.0002] train_loss: 0.621010 train_acc: 0.782787 train_f1: 0.782787 time: 0.1991s
INFO:root:Epoch: 0490 val_loss: 0.659699 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0495 lr: [0.0002, 0.0002] train_loss: 0.620421 train_acc: 0.786885 train_f1: 0.786885 time: 0.1989s
INFO:root:Epoch: 0495 val_loss: 0.657355 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0500 lr: [0.0002, 0.0002] train_loss: 0.608030 train_acc: 0.786885 train_f1: 0.786885 time: 0.2122s
INFO:root:Epoch: 0500 val_loss: 0.658665 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0505 lr: [0.0002, 0.0002] train_loss: 0.637000 train_acc: 0.770492 train_f1: 0.770492 time: 0.1994s
INFO:root:Epoch: 0505 val_loss: 0.654850 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0510 lr: [0.0002, 0.0002] train_loss: 0.630175 train_acc: 0.782787 train_f1: 0.782787 time: 0.1984s
INFO:root:Epoch: 0510 val_loss: 0.668289 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0515 lr: [0.0002, 0.0002] train_loss: 0.603055 train_acc: 0.786885 train_f1: 0.786885 time: 0.2043s
INFO:root:Epoch: 0515 val_loss: 0.694202 val_acc: 0.681818 val_f1: 0.681818
INFO:root:Epoch: 0520 lr: [0.0002, 0.0002] train_loss: 0.602837 train_acc: 0.786885 train_f1: 0.786885 time: 0.2159s
INFO:root:Epoch: 0520 val_loss: 0.783815 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0525 lr: [0.0002, 0.0002] train_loss: 0.591892 train_acc: 0.786885 train_f1: 0.786885 time: 0.1988s
INFO:root:Epoch: 0525 val_loss: 0.839245 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0530 lr: [0.0002, 0.0002] train_loss: 0.596900 train_acc: 0.786885 train_f1: 0.786885 time: 0.1967s
INFO:root:Epoch: 0530 val_loss: 0.830511 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0535 lr: [0.0002, 0.0002] train_loss: 0.597736 train_acc: 0.786885 train_f1: 0.786885 time: 0.2156s
INFO:root:Epoch: 0535 val_loss: 0.813328 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0540 lr: [0.0002, 0.0002] train_loss: 0.595095 train_acc: 0.786885 train_f1: 0.786885 time: 0.1985s
INFO:root:Epoch: 0540 val_loss: 0.710241 val_acc: 0.681818 val_f1: 0.681818
INFO:root:Epoch: 0545 lr: [0.0002, 0.0002] train_loss: 0.594833 train_acc: 0.786885 train_f1: 0.786885 time: 0.1971s
INFO:root:Epoch: 0545 val_loss: 0.657163 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0550 lr: [0.0002, 0.0002] train_loss: 0.605476 train_acc: 0.782787 train_f1: 0.782787 time: 0.1984s
INFO:root:Epoch: 0550 val_loss: 0.665553 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0555 lr: [0.0002, 0.0002] train_loss: 0.622763 train_acc: 0.770492 train_f1: 0.770492 time: 0.2142s
INFO:root:Epoch: 0555 val_loss: 0.653711 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0560 lr: [0.0002, 0.0002] train_loss: 0.592080 train_acc: 0.786885 train_f1: 0.786885 time: 0.2010s
INFO:root:Epoch: 0560 val_loss: 0.663349 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0565 lr: [0.0002, 0.0002] train_loss: 0.589700 train_acc: 0.786885 train_f1: 0.786885 time: 0.2007s
INFO:root:Epoch: 0565 val_loss: 0.652676 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0570 lr: [0.0002, 0.0002] train_loss: 0.606909 train_acc: 0.782787 train_f1: 0.782787 time: 0.2055s
INFO:root:Epoch: 0570 val_loss: 0.661762 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0575 lr: [0.0002, 0.0002] train_loss: 0.580013 train_acc: 0.786885 train_f1: 0.786885 time: 0.2053s
INFO:root:Epoch: 0575 val_loss: 0.667580 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0580 lr: [0.0002, 0.0002] train_loss: 0.588770 train_acc: 0.786885 train_f1: 0.786885 time: 0.1972s
INFO:root:Epoch: 0580 val_loss: 0.651701 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0585 lr: [0.0002, 0.0002] train_loss: 0.590142 train_acc: 0.786885 train_f1: 0.786885 time: 0.1977s
INFO:root:Epoch: 0585 val_loss: 0.662375 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0590 lr: [0.0002, 0.0002] train_loss: 0.584641 train_acc: 0.786885 train_f1: 0.786885 time: 0.2082s
INFO:root:Epoch: 0590 val_loss: 0.656256 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0595 lr: [0.0002, 0.0002] train_loss: 0.591189 train_acc: 0.786885 train_f1: 0.786885 time: 0.1987s
INFO:root:Epoch: 0595 val_loss: 0.655620 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0600 lr: [0.0002, 0.0002] train_loss: 0.584680 train_acc: 0.786885 train_f1: 0.786885 time: 0.1981s
INFO:root:Epoch: 0600 val_loss: 0.654113 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0605 lr: [0.0002, 0.0002] train_loss: 0.575697 train_acc: 0.786885 train_f1: 0.786885 time: 0.2056s
INFO:root:Epoch: 0605 val_loss: 0.663747 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0610 lr: [0.0002, 0.0002] train_loss: 0.574598 train_acc: 0.786885 train_f1: 0.786885 time: 0.2032s
INFO:root:Epoch: 0610 val_loss: 0.664213 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0615 lr: [0.0002, 0.0002] train_loss: 0.584744 train_acc: 0.786885 train_f1: 0.786885 time: 0.1986s
INFO:root:Epoch: 0615 val_loss: 0.666196 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0620 lr: [0.0002, 0.0002] train_loss: 0.572576 train_acc: 0.786885 train_f1: 0.786885 time: 0.2010s
INFO:root:Epoch: 0620 val_loss: 0.667759 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0625 lr: [0.0002, 0.0002] train_loss: 0.590592 train_acc: 0.778689 train_f1: 0.778689 time: 0.2100s
INFO:root:Epoch: 0625 val_loss: 0.666302 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0630 lr: [0.0002, 0.0002] train_loss: 0.581060 train_acc: 0.786885 train_f1: 0.786885 time: 0.1978s
INFO:root:Epoch: 0630 val_loss: 0.675571 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0635 lr: [0.0002, 0.0002] train_loss: 0.578243 train_acc: 0.786885 train_f1: 0.786885 time: 0.1979s
INFO:root:Epoch: 0635 val_loss: 0.685251 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0640 lr: [0.0002, 0.0002] train_loss: 0.575871 train_acc: 0.786885 train_f1: 0.786885 time: 0.2048s
INFO:root:Epoch: 0640 val_loss: 0.681760 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0645 lr: [0.0002, 0.0002] train_loss: 0.568339 train_acc: 0.786885 train_f1: 0.786885 time: 0.2128s
INFO:root:Epoch: 0645 val_loss: 0.685318 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0650 lr: [0.0002, 0.0002] train_loss: 0.580458 train_acc: 0.786885 train_f1: 0.786885 time: 0.1966s
INFO:root:Epoch: 0650 val_loss: 0.677193 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0655 lr: [0.0002, 0.0002] train_loss: 0.573905 train_acc: 0.786885 train_f1: 0.786885 time: 0.1983s
INFO:root:Epoch: 0655 val_loss: 0.683061 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0660 lr: [0.0002, 0.0002] train_loss: 0.573480 train_acc: 0.786885 train_f1: 0.786885 time: 0.2036s
INFO:root:Epoch: 0660 val_loss: 0.692904 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0665 lr: [0.0002, 0.0002] train_loss: 0.581737 train_acc: 0.782787 train_f1: 0.782787 time: 0.2021s
INFO:root:Epoch: 0665 val_loss: 0.690062 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0670 lr: [0.0002, 0.0002] train_loss: 0.584352 train_acc: 0.786885 train_f1: 0.786885 time: 0.1977s
INFO:root:Epoch: 0670 val_loss: 0.685472 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Epoch: 0675 lr: [0.0002, 0.0002] train_loss: 0.573087 train_acc: 0.786885 train_f1: 0.786885 time: 0.1968s
INFO:root:Epoch: 0675 val_loss: 0.688295 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Early stopping
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 142.2742s
INFO:root:Val set results: val_loss: 1.013477 val_acc: 0.704545 val_f1: 0.704545
INFO:root:Test set results: test_loss: 0.799148 test_acc: 0.840909 test_f1: 0.840909
