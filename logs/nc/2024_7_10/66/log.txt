INFO:root:Using: cuda:7
INFO:root:Using seed 18.
INFO:root:Dataset: texas
INFO:root:Num classes: 5
INFO:root:NCModel(
  (encoder): BKNet(
    (linear_before): BLinear(
      in_features=1703, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=False, act=None, dropout_rate=0.2
      (dropout): Dropout(p=0.2, inplace=False)
      (E_linear): Linear(in_features=1703, out_features=64, bias=False)
    )
    (layers): Sequential(
      (0): KPGraphConvolution(
        (net): KernelPointAggregation(
          (linears): ModuleList(
            (0): BLinear(
              in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.2
              (dropout): Dropout(p=0.2, inplace=False)
              (E_linear): Linear(in_features=64, out_features=64, bias=False)
            )
            (1): BLinear(
              in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.2
              (dropout): Dropout(p=0.2, inplace=False)
              (E_linear): Linear(in_features=64, out_features=64, bias=False)
            )
            (2): BLinear(
              in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.2
              (dropout): Dropout(p=0.2, inplace=False)
              (E_linear): Linear(in_features=64, out_features=64, bias=False)
            )
            (3): BLinear(
              in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.2
              (dropout): Dropout(p=0.2, inplace=False)
              (E_linear): Linear(in_features=64, out_features=64, bias=False)
            )
          )
          (MLP_f): BMLP(
            (linear1): BLinear(
              in_features=64, out_features=128, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.2
              (dropout): Dropout(p=0.2, inplace=False)
              (E_linear): Linear(in_features=64, out_features=128, bias=False)
            )
            (act): BAct(c=tensor([1.], device='cuda:7'), act=<function relu at 0x7f2a436476d0>)
            (linear2): BLinear(
              in_features=128, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.2
              (dropout): Dropout(p=0.2, inplace=False)
              (E_linear): Linear(in_features=128, out_features=64, bias=False)
            )
          )
          (MLP_fi): BMLP(
            (linear1): BLinear(
              in_features=64, out_features=128, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.2
              (dropout): Dropout(p=0.2, inplace=False)
              (E_linear): Linear(in_features=64, out_features=128, bias=False)
            )
            (act): BAct(c=tensor([1.], device='cuda:7'), act=<function relu at 0x7f2a436476d0>)
            (linear2): BLinear(
              in_features=128, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.2
              (dropout): Dropout(p=0.2, inplace=False)
              (E_linear): Linear(in_features=128, out_features=64, bias=False)
            )
          )
        )
      )
      (1): KPGraphConvolution(
        (net): KernelPointAggregation(
          (linears): ModuleList(
            (0): BLinear(
              in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.2
              (dropout): Dropout(p=0.2, inplace=False)
              (E_linear): Linear(in_features=64, out_features=64, bias=False)
            )
            (1): BLinear(
              in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.2
              (dropout): Dropout(p=0.2, inplace=False)
              (E_linear): Linear(in_features=64, out_features=64, bias=False)
            )
            (2): BLinear(
              in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.2
              (dropout): Dropout(p=0.2, inplace=False)
              (E_linear): Linear(in_features=64, out_features=64, bias=False)
            )
            (3): BLinear(
              in_features=64, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.2
              (dropout): Dropout(p=0.2, inplace=False)
              (E_linear): Linear(in_features=64, out_features=64, bias=False)
            )
          )
          (MLP_f): BMLP(
            (linear1): BLinear(
              in_features=64, out_features=128, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.2
              (dropout): Dropout(p=0.2, inplace=False)
              (E_linear): Linear(in_features=64, out_features=128, bias=False)
            )
            (act): BAct(c=tensor([1.], device='cuda:7'), act=<function relu at 0x7f2a436476d0>)
            (linear2): BLinear(
              in_features=128, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.2
              (dropout): Dropout(p=0.2, inplace=False)
              (E_linear): Linear(in_features=128, out_features=64, bias=False)
            )
          )
          (MLP_fi): BMLP(
            (linear1): BLinear(
              in_features=64, out_features=128, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.2
              (dropout): Dropout(p=0.2, inplace=False)
              (E_linear): Linear(in_features=64, out_features=128, bias=False)
            )
            (act): BAct(c=tensor([1.], device='cuda:7'), act=<function relu at 0x7f2a436476d0>)
            (linear2): BLinear(
              in_features=128, out_features=64, c=tensor([1.], device='cuda:7'), use_bias=1, act=None, dropout_rate=0.2
              (dropout): Dropout(p=0.2, inplace=False)
              (E_linear): Linear(in_features=128, out_features=64, bias=False)
            )
          )
        )
      )
    )
  )
  (decoder): PoincareDecoder()
)
INFO:root:Total number of parameters: 209477
INFO:root:Epoch: 0005 lr: [0.0002, 0.0002] train_loss: 1.530427 train_acc: 0.606557 train_f1: 0.606557 time: 0.2273s
INFO:root:Epoch: 0010 lr: [0.0002, 0.0002] train_loss: 1.442165 train_acc: 0.606557 train_f1: 0.606557 time: 0.2256s
INFO:root:Epoch: 0010 val_loss: 1.431951 val_acc: 0.613636 val_f1: 0.613636
INFO:root:Epoch: 0015 lr: [0.0002, 0.0002] train_loss: 1.379364 train_acc: 0.606557 train_f1: 0.606557 time: 0.2231s
INFO:root:Epoch: 0020 lr: [0.0002, 0.0002] train_loss: 1.329574 train_acc: 0.606557 train_f1: 0.606557 time: 0.2266s
INFO:root:Epoch: 0020 val_loss: 1.309704 val_acc: 0.613636 val_f1: 0.613636
INFO:root:Epoch: 0025 lr: [0.0002, 0.0002] train_loss: 1.295039 train_acc: 0.606557 train_f1: 0.606557 time: 0.2233s
INFO:root:Epoch: 0030 lr: [0.0002, 0.0002] train_loss: 1.264028 train_acc: 0.606557 train_f1: 0.606557 time: 0.2388s
INFO:root:Epoch: 0030 val_loss: 1.253567 val_acc: 0.613636 val_f1: 0.613636
INFO:root:Epoch: 0035 lr: [0.0002, 0.0002] train_loss: 1.238792 train_acc: 0.606557 train_f1: 0.606557 time: 0.2360s
INFO:root:Epoch: 0040 lr: [0.0002, 0.0002] train_loss: 1.225999 train_acc: 0.606557 train_f1: 0.606557 time: 0.2321s
INFO:root:Epoch: 0040 val_loss: 1.219366 val_acc: 0.613636 val_f1: 0.613636
INFO:root:Epoch: 0045 lr: [0.0002, 0.0002] train_loss: 1.207897 train_acc: 0.606557 train_f1: 0.606557 time: 0.2246s
INFO:root:Epoch: 0050 lr: [0.0002, 0.0002] train_loss: 1.197519 train_acc: 0.606557 train_f1: 0.606557 time: 0.2296s
INFO:root:Epoch: 0050 val_loss: 1.195664 val_acc: 0.613636 val_f1: 0.613636
INFO:root:Epoch: 0055 lr: [0.0002, 0.0002] train_loss: 1.184104 train_acc: 0.606557 train_f1: 0.606557 time: 0.2214s
INFO:root:Epoch: 0060 lr: [0.0002, 0.0002] train_loss: 1.173319 train_acc: 0.606557 train_f1: 0.606557 time: 0.2288s
INFO:root:Epoch: 0060 val_loss: 1.177600 val_acc: 0.613636 val_f1: 0.613636
INFO:root:Epoch: 0065 lr: [0.0002, 0.0002] train_loss: 1.168458 train_acc: 0.606557 train_f1: 0.606557 time: 0.2307s
INFO:root:Epoch: 0070 lr: [0.0002, 0.0002] train_loss: 1.155853 train_acc: 0.606557 train_f1: 0.606557 time: 0.2344s
INFO:root:Epoch: 0070 val_loss: 1.163122 val_acc: 0.613636 val_f1: 0.613636
INFO:root:Epoch: 0075 lr: [0.0002, 0.0002] train_loss: 1.156724 train_acc: 0.606557 train_f1: 0.606557 time: 0.2251s
INFO:root:Epoch: 0080 lr: [0.0002, 0.0002] train_loss: 1.145027 train_acc: 0.606557 train_f1: 0.606557 time: 0.2273s
INFO:root:Epoch: 0080 val_loss: 1.151424 val_acc: 0.613636 val_f1: 0.613636
INFO:root:Epoch: 0085 lr: [0.0002, 0.0002] train_loss: 1.133239 train_acc: 0.606557 train_f1: 0.606557 time: 0.2248s
INFO:root:Epoch: 0090 lr: [0.0002, 0.0002] train_loss: 1.136013 train_acc: 0.606557 train_f1: 0.606557 time: 0.2268s
INFO:root:Epoch: 0090 val_loss: 1.139368 val_acc: 0.613636 val_f1: 0.613636
INFO:root:Epoch: 0095 lr: [0.0002, 0.0002] train_loss: 1.126326 train_acc: 0.606557 train_f1: 0.606557 time: 0.2221s
INFO:root:Epoch: 0100 lr: [0.0002, 0.0002] train_loss: 1.119385 train_acc: 0.606557 train_f1: 0.606557 time: 0.2428s
INFO:root:Epoch: 0100 val_loss: 1.122687 val_acc: 0.613636 val_f1: 0.613636
INFO:root:Epoch: 0105 lr: [0.0002, 0.0002] train_loss: 1.106181 train_acc: 0.606557 train_f1: 0.606557 time: 0.2395s
INFO:root:Epoch: 0110 lr: [0.0002, 0.0002] train_loss: 1.093267 train_acc: 0.606557 train_f1: 0.606557 time: 0.2341s
INFO:root:Epoch: 0110 val_loss: 1.088767 val_acc: 0.613636 val_f1: 0.613636
INFO:root:Epoch: 0115 lr: [0.0002, 0.0002] train_loss: 1.078235 train_acc: 0.606557 train_f1: 0.606557 time: 0.2214s
INFO:root:Epoch: 0120 lr: [0.0002, 0.0002] train_loss: 1.052951 train_acc: 0.606557 train_f1: 0.606557 time: 0.2263s
INFO:root:Epoch: 0120 val_loss: 1.040777 val_acc: 0.613636 val_f1: 0.613636
INFO:root:Epoch: 0125 lr: [0.0002, 0.0002] train_loss: 1.023658 train_acc: 0.606557 train_f1: 0.606557 time: 0.2213s
INFO:root:Epoch: 0130 lr: [0.0002, 0.0002] train_loss: 0.990411 train_acc: 0.606557 train_f1: 0.606557 time: 0.2289s
INFO:root:Epoch: 0130 val_loss: 0.979908 val_acc: 0.613636 val_f1: 0.613636
INFO:root:Epoch: 0135 lr: [0.0002, 0.0002] train_loss: 0.956232 train_acc: 0.606557 train_f1: 0.606557 time: 0.2355s
INFO:root:Epoch: 0140 lr: [0.0002, 0.0002] train_loss: 0.924735 train_acc: 0.606557 train_f1: 0.606557 time: 0.2458s
INFO:root:Epoch: 0140 val_loss: 0.933119 val_acc: 0.613636 val_f1: 0.613636
INFO:root:Epoch: 0145 lr: [0.0002, 0.0002] train_loss: 0.887590 train_acc: 0.606557 train_f1: 0.606557 time: 0.2253s
INFO:root:Epoch: 0150 lr: [0.0002, 0.0002] train_loss: 0.844757 train_acc: 0.659836 train_f1: 0.659836 time: 0.2319s
INFO:root:Epoch: 0150 val_loss: 0.865551 val_acc: 0.727273 val_f1: 0.727273
INFO:root:Epoch: 0155 lr: [0.0002, 0.0002] train_loss: 0.816032 train_acc: 0.663934 train_f1: 0.663934 time: 0.2269s
INFO:root:Epoch: 0160 lr: [0.0002, 0.0002] train_loss: 0.796106 train_acc: 0.709016 train_f1: 0.709016 time: 0.2328s
INFO:root:Epoch: 0160 val_loss: 0.835968 val_acc: 0.727273 val_f1: 0.727273
INFO:root:Epoch: 0165 lr: [0.0002, 0.0002] train_loss: 0.768043 train_acc: 0.750000 train_f1: 0.750000 time: 0.2251s
INFO:root:Epoch: 0170 lr: [0.0002, 0.0002] train_loss: 0.754012 train_acc: 0.786885 train_f1: 0.786885 time: 0.2337s
INFO:root:Epoch: 0170 val_loss: 0.814174 val_acc: 0.727273 val_f1: 0.727273
INFO:root:Epoch: 0175 lr: [0.0002, 0.0002] train_loss: 0.738792 train_acc: 0.725410 train_f1: 0.725410 time: 0.2443s
INFO:root:Epoch: 0180 lr: [0.0002, 0.0002] train_loss: 0.710772 train_acc: 0.766393 train_f1: 0.766393 time: 0.2514s
INFO:root:Epoch: 0180 val_loss: 0.751015 val_acc: 0.727273 val_f1: 0.727273
INFO:root:Epoch: 0185 lr: [0.0002, 0.0002] train_loss: 0.714008 train_acc: 0.758197 train_f1: 0.758197 time: 0.2287s
INFO:root:Epoch: 0190 lr: [0.0002, 0.0002] train_loss: 0.665362 train_acc: 0.786885 train_f1: 0.786885 time: 0.2317s
INFO:root:Epoch: 0190 val_loss: 0.734257 val_acc: 0.727273 val_f1: 0.727273
INFO:root:Epoch: 0195 lr: [0.0002, 0.0002] train_loss: 0.647561 train_acc: 0.774590 train_f1: 0.774590 time: 0.2256s
INFO:root:Epoch: 0200 lr: [0.0002, 0.0002] train_loss: 0.639926 train_acc: 0.795082 train_f1: 0.795082 time: 0.2284s
INFO:root:Epoch: 0200 val_loss: 0.795623 val_acc: 0.727273 val_f1: 0.727273
INFO:root:Epoch: 0205 lr: [0.0002, 0.0002] train_loss: 0.647017 train_acc: 0.778689 train_f1: 0.778689 time: 0.2260s
INFO:root:Epoch: 0210 lr: [0.0002, 0.0002] train_loss: 0.634181 train_acc: 0.795082 train_f1: 0.795082 time: 0.2476s
INFO:root:Epoch: 0210 val_loss: 0.721688 val_acc: 0.727273 val_f1: 0.727273
INFO:root:Epoch: 0215 lr: [0.0002, 0.0002] train_loss: 0.619916 train_acc: 0.795082 train_f1: 0.795082 time: 0.2351s
INFO:root:Epoch: 0220 lr: [0.0002, 0.0002] train_loss: 0.611360 train_acc: 0.790984 train_f1: 0.790984 time: 0.2278s
INFO:root:Epoch: 0220 val_loss: 0.712361 val_acc: 0.727273 val_f1: 0.727273
INFO:root:Epoch: 0225 lr: [0.0002, 0.0002] train_loss: 0.598917 train_acc: 0.790984 train_f1: 0.790984 time: 0.2222s
INFO:root:Epoch: 0230 lr: [0.0002, 0.0002] train_loss: 0.593898 train_acc: 0.790984 train_f1: 0.790984 time: 0.2290s
INFO:root:Epoch: 0230 val_loss: 0.735725 val_acc: 0.727273 val_f1: 0.727273
INFO:root:Epoch: 0235 lr: [0.0002, 0.0002] train_loss: 0.589641 train_acc: 0.795082 train_f1: 0.795082 time: 0.2253s
INFO:root:Epoch: 0240 lr: [0.0002, 0.0002] train_loss: 0.583932 train_acc: 0.790984 train_f1: 0.790984 time: 0.2355s
INFO:root:Epoch: 0240 val_loss: 0.733989 val_acc: 0.727273 val_f1: 0.727273
INFO:root:Epoch: 0245 lr: [0.0002, 0.0002] train_loss: 0.583524 train_acc: 0.795082 train_f1: 0.795082 time: 0.2401s
INFO:root:Epoch: 0250 lr: [0.0002, 0.0002] train_loss: 0.572903 train_acc: 0.795082 train_f1: 0.795082 time: 0.2421s
INFO:root:Epoch: 0250 val_loss: 0.710143 val_acc: 0.727273 val_f1: 0.727273
INFO:root:Epoch: 0255 lr: [0.0002, 0.0002] train_loss: 0.570048 train_acc: 0.795082 train_f1: 0.795082 time: 0.2256s
INFO:root:Epoch: 0260 lr: [0.0002, 0.0002] train_loss: 0.571437 train_acc: 0.795082 train_f1: 0.795082 time: 0.2158s
INFO:root:Epoch: 0260 val_loss: 0.707858 val_acc: 0.727273 val_f1: 0.727273
INFO:root:Epoch: 0265 lr: [0.0002, 0.0002] train_loss: 0.565211 train_acc: 0.795082 train_f1: 0.795082 time: 0.2127s
INFO:root:Epoch: 0270 lr: [0.0002, 0.0002] train_loss: 0.562724 train_acc: 0.795082 train_f1: 0.795082 time: 0.2264s
INFO:root:Epoch: 0270 val_loss: 0.710211 val_acc: 0.727273 val_f1: 0.727273
INFO:root:Epoch: 0275 lr: [0.0002, 0.0002] train_loss: 0.555238 train_acc: 0.795082 train_f1: 0.795082 time: 0.2265s
INFO:root:Epoch: 0280 lr: [0.0002, 0.0002] train_loss: 0.554213 train_acc: 0.795082 train_f1: 0.795082 time: 0.2155s
INFO:root:Epoch: 0280 val_loss: 0.706437 val_acc: 0.727273 val_f1: 0.727273
INFO:root:Epoch: 0285 lr: [0.0002, 0.0002] train_loss: 0.552455 train_acc: 0.795082 train_f1: 0.795082 time: 0.2170s
INFO:root:Epoch: 0290 lr: [0.0002, 0.0002] train_loss: 0.553549 train_acc: 0.795082 train_f1: 0.795082 time: 0.2180s
INFO:root:Epoch: 0290 val_loss: 0.698297 val_acc: 0.727273 val_f1: 0.727273
INFO:root:Epoch: 0295 lr: [0.0002, 0.0002] train_loss: 0.544031 train_acc: 0.795082 train_f1: 0.795082 time: 0.2210s
INFO:root:Epoch: 0300 lr: [0.0002, 0.0002] train_loss: 0.553265 train_acc: 0.795082 train_f1: 0.795082 time: 0.2324s
INFO:root:Epoch: 0300 val_loss: 0.691024 val_acc: 0.727273 val_f1: 0.727273
INFO:root:Epoch: 0305 lr: [0.0002, 0.0002] train_loss: 0.538621 train_acc: 0.795082 train_f1: 0.795082 time: 0.2160s
INFO:root:Epoch: 0310 lr: [0.0002, 0.0002] train_loss: 0.536843 train_acc: 0.795082 train_f1: 0.795082 time: 0.2148s
INFO:root:Epoch: 0310 val_loss: 0.692509 val_acc: 0.727273 val_f1: 0.727273
INFO:root:Epoch: 0315 lr: [0.0002, 0.0002] train_loss: 0.534585 train_acc: 0.795082 train_f1: 0.795082 time: 0.2100s
INFO:root:Epoch: 0320 lr: [0.0002, 0.0002] train_loss: 0.532966 train_acc: 0.795082 train_f1: 0.795082 time: 0.2245s
INFO:root:Epoch: 0320 val_loss: 0.698341 val_acc: 0.727273 val_f1: 0.727273
INFO:root:Epoch: 0325 lr: [0.0002, 0.0002] train_loss: 0.526279 train_acc: 0.795082 train_f1: 0.795082 time: 0.2184s
INFO:root:Epoch: 0330 lr: [0.0002, 0.0002] train_loss: 0.526355 train_acc: 0.795082 train_f1: 0.795082 time: 0.2357s
INFO:root:Epoch: 0330 val_loss: 0.698755 val_acc: 0.727273 val_f1: 0.727273
INFO:root:Epoch: 0335 lr: [0.0002, 0.0002] train_loss: 0.522051 train_acc: 0.795082 train_f1: 0.795082 time: 0.2113s
INFO:root:Epoch: 0340 lr: [0.0002, 0.0002] train_loss: 0.522968 train_acc: 0.795082 train_f1: 0.795082 time: 0.2181s
INFO:root:Epoch: 0340 val_loss: 0.695521 val_acc: 0.727273 val_f1: 0.727273
INFO:root:Epoch: 0345 lr: [0.0002, 0.0002] train_loss: 0.526048 train_acc: 0.795082 train_f1: 0.795082 time: 0.2113s
INFO:root:Epoch: 0350 lr: [0.0002, 0.0002] train_loss: 0.514833 train_acc: 0.795082 train_f1: 0.795082 time: 0.2306s
INFO:root:Epoch: 0350 val_loss: 0.698490 val_acc: 0.727273 val_f1: 0.727273
INFO:root:Early stopping
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 87.8322s
INFO:root:Val set results: val_loss: 0.865551 val_acc: 0.727273 val_f1: 0.727273
INFO:root:Test set results: test_loss: 0.833922 test_acc: 0.772727 test_f1: 0.772727
