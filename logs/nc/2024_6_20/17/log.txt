INFO:root:Using: cuda:0
INFO:root:Using seed 18.
INFO:root:Dataset: cornell
INFO:root:Num classes: 5
INFO:root:NCModel(
  (encoder): BKNet(
    (linear_before): BLinear(in_features=1703, out_features=32, c=tensor([1.], device='cuda:0'))
    (layers): Sequential(
      (0): KPGraphConvolution(
        (net): KernelPointAggregation(
          (linears): ModuleList(
            (0): BLayer(
              (linear): BLinear(in_features=32, out_features=16, c=1)
              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))
            )
            (1): BLayer(
              (linear): BLinear(in_features=32, out_features=16, c=1)
              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))
            )
            (2): BLayer(
              (linear): BLinear(in_features=32, out_features=16, c=1)
              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))
            )
            (3): BLayer(
              (linear): BLinear(in_features=32, out_features=16, c=1)
              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))
            )
            (4): BLayer(
              (linear): BLinear(in_features=32, out_features=16, c=1)
              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))
            )
            (5): BLayer(
              (linear): BLinear(in_features=32, out_features=16, c=1)
              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))
            )
          )
        )
      )
    )
  )
  (decoder): PoincareDecoder()
)
INFO:root:Total number of parameters: 57989
INFO:root:Epoch: 0020 lr: [0.0001, 0.0001] train_loss: 1.974750 train_acc: 0.586066 train_f1: 0.586066 time: 0.0416s
INFO:root:Epoch: 0020 val_loss: 1.968603 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0040 lr: [0.0001, 0.0001] train_loss: 1.948099 train_acc: 0.586066 train_f1: 0.586066 time: 0.0422s
INFO:root:Epoch: 0040 val_loss: 1.937107 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0060 lr: [0.0001, 0.0001] train_loss: 1.921287 train_acc: 0.586066 train_f1: 0.586066 time: 0.0415s
INFO:root:Epoch: 0060 val_loss: 1.905405 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0080 lr: [0.0001, 0.0001] train_loss: 1.894215 train_acc: 0.586066 train_f1: 0.586066 time: 0.0421s
INFO:root:Epoch: 0080 val_loss: 1.873377 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0100 lr: [0.0001, 0.0001] train_loss: 1.866779 train_acc: 0.586066 train_f1: 0.586066 time: 0.0434s
INFO:root:Epoch: 0100 val_loss: 1.840901 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0120 lr: [0.0001, 0.0001] train_loss: 1.838866 train_acc: 0.586066 train_f1: 0.586066 time: 0.0430s
INFO:root:Epoch: 0120 val_loss: 1.807843 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0140 lr: [0.0001, 0.0001] train_loss: 1.810354 train_acc: 0.586066 train_f1: 0.586066 time: 0.0418s
INFO:root:Epoch: 0140 val_loss: 1.774056 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0160 lr: [0.0001, 0.0001] train_loss: 1.781101 train_acc: 0.586066 train_f1: 0.586066 time: 0.0423s
INFO:root:Epoch: 0160 val_loss: 1.739373 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0180 lr: [0.0001, 0.0001] train_loss: 1.750948 train_acc: 0.586066 train_f1: 0.586066 time: 0.0418s
INFO:root:Epoch: 0180 val_loss: 1.703601 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0200 lr: [0.0001, 0.0001] train_loss: 1.719704 train_acc: 0.586066 train_f1: 0.586066 time: 0.0435s
INFO:root:Epoch: 0200 val_loss: 1.666514 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0220 lr: [0.0001, 0.0001] train_loss: 1.687140 train_acc: 0.586066 train_f1: 0.586066 time: 0.0416s
INFO:root:Epoch: 0220 val_loss: 1.627835 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0240 lr: [0.0001, 0.0001] train_loss: 1.652974 train_acc: 0.586066 train_f1: 0.586066 time: 0.0413s
INFO:root:Epoch: 0240 val_loss: 1.587228 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0260 lr: [0.0001, 0.0001] train_loss: 1.616851 train_acc: 0.586066 train_f1: 0.586066 time: 0.0416s
INFO:root:Epoch: 0260 val_loss: 1.544264 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0280 lr: [0.0001, 0.0001] train_loss: 1.578308 train_acc: 0.586066 train_f1: 0.586066 time: 0.0412s
INFO:root:Epoch: 0280 val_loss: 1.498387 val_acc: 0.659091 val_f1: 0.659091
