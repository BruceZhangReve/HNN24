INFO:root:Using: cuda:0
INFO:root:Using seed 18.
INFO:root:Dataset: cornell
INFO:root:Num classes: 5
INFO:root:NCModel(
  (encoder): BKNet(
    (linear_before): BLinear(in_features=1703, out_features=32, c=tensor([1.], device='cuda:0'))
    (layers): Sequential(
      (0): KPGraphConvolution(
        (net): KernelPointAggregation(
          (linears): ModuleList(
            (0): BLayer(
              (linear): BLinear(in_features=32, out_features=16, c=1)
              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))
            )
            (1): BLayer(
              (linear): BLinear(in_features=32, out_features=16, c=1)
              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))
            )
            (2): BLayer(
              (linear): BLinear(in_features=32, out_features=16, c=1)
              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))
            )
            (3): BLayer(
              (linear): BLinear(in_features=32, out_features=16, c=1)
              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))
            )
            (4): BLayer(
              (linear): BLinear(in_features=32, out_features=16, c=1)
              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))
            )
            (5): BLayer(
              (linear): BLinear(in_features=32, out_features=16, c=1)
              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))
            )
          )
        )
      )
    )
  )
  (decoder): PoincareDecoder()
)
INFO:root:Total number of parameters: 57989
INFO:root:Epoch: 0020 lr: [0.0001, 0.0001] train_loss: 3.974750 train_acc: 0.586066 train_f1: 0.586066 time: 0.0508s
INFO:root:Epoch: 0020 val_loss: 3.968603 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0040 lr: [0.0001, 0.0001] train_loss: 3.948099 train_acc: 0.586066 train_f1: 0.586066 time: 0.0483s
INFO:root:Epoch: 0040 val_loss: 3.937107 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0060 lr: [0.0001, 0.0001] train_loss: 3.921287 train_acc: 0.586066 train_f1: 0.586066 time: 0.0539s
INFO:root:Epoch: 0060 val_loss: 3.905405 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0080 lr: [0.0001, 0.0001] train_loss: 3.894215 train_acc: 0.586066 train_f1: 0.586066 time: 0.0538s
INFO:root:Epoch: 0080 val_loss: 3.873377 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0100 lr: [0.0001, 0.0001] train_loss: 3.866779 train_acc: 0.586066 train_f1: 0.586066 time: 0.0541s
INFO:root:Epoch: 0100 val_loss: 3.840901 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0120 lr: [0.0001, 0.0001] train_loss: 3.838866 train_acc: 0.586066 train_f1: 0.586066 time: 0.0541s
INFO:root:Epoch: 0120 val_loss: 3.807843 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0140 lr: [0.0001, 0.0001] train_loss: 3.810354 train_acc: 0.586066 train_f1: 0.586066 time: 0.0539s
INFO:root:Epoch: 0140 val_loss: 3.774056 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0160 lr: [0.0001, 0.0001] train_loss: 3.781101 train_acc: 0.586066 train_f1: 0.586066 time: 0.0569s
INFO:root:Epoch: 0160 val_loss: 3.739373 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0180 lr: [0.0001, 0.0001] train_loss: 3.750948 train_acc: 0.586066 train_f1: 0.586066 time: 0.0548s
INFO:root:Epoch: 0180 val_loss: 3.703601 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0200 lr: [0.0001, 0.0001] train_loss: 3.719704 train_acc: 0.586066 train_f1: 0.586066 time: 0.0564s
INFO:root:Epoch: 0200 val_loss: 3.666514 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0220 lr: [0.0001, 0.0001] train_loss: 3.687140 train_acc: 0.586066 train_f1: 0.586066 time: 0.0560s
INFO:root:Epoch: 0220 val_loss: 3.627835 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0240 lr: [0.0001, 0.0001] train_loss: 3.652974 train_acc: 0.586066 train_f1: 0.586066 time: 0.0548s
INFO:root:Epoch: 0240 val_loss: 3.587228 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0260 lr: [0.0001, 0.0001] train_loss: 3.616851 train_acc: 0.586066 train_f1: 0.586066 time: 0.0453s
INFO:root:Epoch: 0260 val_loss: 3.544264 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0280 lr: [0.0001, 0.0001] train_loss: 3.578308 train_acc: 0.586066 train_f1: 0.586066 time: 0.0464s
INFO:root:Epoch: 0280 val_loss: 3.498387 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0300 lr: [0.0001, 0.0001] train_loss: 3.536731 train_acc: 0.586066 train_f1: 0.586066 time: 0.0451s
INFO:root:Epoch: 0300 val_loss: 3.448854 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0320 lr: [0.0001, 0.0001] train_loss: 3.491265 train_acc: 0.586066 train_f1: 0.586066 time: 0.0463s
INFO:root:Epoch: 0320 val_loss: 3.394632 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0340 lr: [0.0001, 0.0001] train_loss: 3.440678 train_acc: 0.586066 train_f1: 0.586066 time: 0.0460s
INFO:root:Epoch: 0340 val_loss: 3.334225 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0360 lr: [0.0001, 0.0001] train_loss: 3.383083 train_acc: 0.586066 train_f1: 0.586066 time: 0.0457s
INFO:root:Epoch: 0360 val_loss: 3.265340 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0380 lr: [0.0001, 0.0001] train_loss: 3.315382 train_acc: 0.586066 train_f1: 0.586066 time: 0.0453s
INFO:root:Epoch: 0380 val_loss: 3.184189 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0400 lr: [0.0001, 0.0001] train_loss: 3.231929 train_acc: 0.586066 train_f1: 0.586066 time: 0.0464s
INFO:root:Epoch: 0400 val_loss: 3.083831 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0420 lr: [0.0001, 0.0001] train_loss: 3.120684 train_acc: 0.586066 train_f1: 0.586066 time: 0.0459s
INFO:root:Epoch: 0420 val_loss: 2.949311 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0440 lr: [0.0001, 0.0001] train_loss: 2.947671 train_acc: 0.586066 train_f1: 0.586066 time: 0.0454s
INFO:root:Epoch: 0440 val_loss: 2.737558 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0460 lr: [0.0001, 0.0001] train_loss: 2.511416 train_acc: 0.586066 train_f1: 0.586066 time: 0.0462s
INFO:root:Epoch: 0460 val_loss: 2.173023 val_acc: 0.659091 val_f1: 0.659091
