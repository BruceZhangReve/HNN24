INFO:root:Using: cuda:0
INFO:root:Using seed 8.
INFO:root:Dataset: cornell
INFO:root:Num classes: 5
INFO:root:NCModel(
  (encoder): BKNet(
    (linear_before): BLinear(in_features=1703, out_features=32, c=tensor([1.], device='cuda:0'))
    (layers): Sequential(
      (0): KPGraphConvolution(
        (net): KernelPointAggregation(
          (linears): ModuleList(
            (0): BLayer(
              (linear): BLinear(in_features=32, out_features=16, c=1)
              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))
            )
            (1): BLayer(
              (linear): BLinear(in_features=32, out_features=16, c=1)
              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))
            )
            (2): BLayer(
              (linear): BLinear(in_features=32, out_features=16, c=1)
              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))
            )
            (3): BLayer(
              (linear): BLinear(in_features=32, out_features=16, c=1)
              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))
            )
            (4): BLayer(
              (linear): BLinear(in_features=32, out_features=16, c=1)
              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))
            )
            (5): BLayer(
              (linear): BLinear(in_features=32, out_features=16, c=1)
              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))
            )
          )
        )
      )
    )
  )
  (decoder): PoincareDecoder()
)
INFO:root:Total number of parameters: 57989
INFO:root:Epoch: 0005 lr: [0.001, 0.001] train_loss: 19.951102 train_acc: 0.586066 train_f1: 0.586066 time: 0.0497s
INFO:root:Epoch: 0005 val_loss: 19.927472 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0010 lr: [0.001, 0.001] train_loss: 19.889648 train_acc: 0.586066 train_f1: 0.586066 time: 0.0485s
INFO:root:Epoch: 0010 val_loss: 19.854396 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0015 lr: [0.001, 0.001] train_loss: 19.827145 train_acc: 0.586066 train_f1: 0.586066 time: 0.0478s
INFO:root:Epoch: 0015 val_loss: 19.779875 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0020 lr: [0.001, 0.001] train_loss: 19.762688 train_acc: 0.586066 train_f1: 0.586066 time: 0.0482s
INFO:root:Epoch: 0020 val_loss: 19.702777 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0025 lr: [0.001, 0.001] train_loss: 19.695088 train_acc: 0.586066 train_f1: 0.586066 time: 0.0484s
INFO:root:Epoch: 0025 val_loss: 19.621594 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0030 lr: [0.001, 0.001] train_loss: 19.622689 train_acc: 0.586066 train_f1: 0.586066 time: 0.0477s
INFO:root:Epoch: 0030 val_loss: 19.534203 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0035 lr: [0.001, 0.001] train_loss: 19.543053 train_acc: 0.586066 train_f1: 0.586066 time: 0.0529s
INFO:root:Epoch: 0035 val_loss: 19.437444 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0040 lr: [0.001, 0.001] train_loss: 19.452300 train_acc: 0.586066 train_f1: 0.586066 time: 0.0491s
INFO:root:Epoch: 0040 val_loss: 19.326189 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0045 lr: [0.001, 0.001] train_loss: 19.343524 train_acc: 0.586066 train_f1: 0.586066 time: 0.0475s
INFO:root:Epoch: 0045 val_loss: 19.191065 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0050 lr: [0.001, 0.001] train_loss: 19.202298 train_acc: 0.586066 train_f1: 0.586066 time: 0.0484s
INFO:root:Epoch: 0050 val_loss: 19.011653 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0055 lr: [0.001, 0.001] train_loss: 18.989086 train_acc: 0.586066 train_f1: 0.586066 time: 0.0486s
INFO:root:Epoch: 0055 val_loss: 18.727229 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0060 lr: [0.001, 0.001] train_loss: 18.492688 train_acc: 0.586066 train_f1: 0.586066 time: 0.0479s
INFO:root:Epoch: 0060 val_loss: 17.885572 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0065 lr: [0.001, 0.001] train_loss: 18.032400 train_acc: 0.586066 train_f1: 0.586066 time: 0.0476s
INFO:root:Epoch: 0065 val_loss: 17.956019 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0070 lr: [0.001, 0.001] train_loss: 18.622503 train_acc: 0.586066 train_f1: 0.586066 time: 0.0486s
INFO:root:Epoch: 0070 val_loss: 18.412134 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0075 lr: [0.001, 0.001] train_loss: 18.760573 train_acc: 0.586066 train_f1: 0.586066 time: 0.0473s
INFO:root:Epoch: 0075 val_loss: 18.547444 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0080 lr: [0.001, 0.001] train_loss: 18.815736 train_acc: 0.586066 train_f1: 0.586066 time: 0.0522s
INFO:root:Epoch: 0080 val_loss: 18.602787 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0085 lr: [0.001, 0.001] train_loss: 18.837758 train_acc: 0.586066 train_f1: 0.586066 time: 0.0489s
INFO:root:Epoch: 0085 val_loss: 18.624238 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0090 lr: [0.001, 0.001] train_loss: 18.843557 train_acc: 0.586066 train_f1: 0.586066 time: 0.0486s
INFO:root:Epoch: 0090 val_loss: 18.628699 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0095 lr: [0.001, 0.001] train_loss: 18.840703 train_acc: 0.586066 train_f1: 0.586066 time: 0.0471s
INFO:root:Epoch: 0095 val_loss: 18.624000 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0100 lr: [0.001, 0.001] train_loss: 18.833062 train_acc: 0.586066 train_f1: 0.586066 time: 0.0510s
INFO:root:Epoch: 0100 val_loss: 18.614210 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0105 lr: [0.001, 0.001] train_loss: 18.822747 train_acc: 0.586066 train_f1: 0.586066 time: 0.0483s
INFO:root:Epoch: 0105 val_loss: 18.601578 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0110 lr: [0.001, 0.001] train_loss: 18.810963 train_acc: 0.586066 train_f1: 0.586066 time: 0.0474s
INFO:root:Epoch: 0110 val_loss: 18.587391 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0115 lr: [0.001, 0.001] train_loss: 18.798406 train_acc: 0.586066 train_f1: 0.586066 time: 0.0488s
INFO:root:Epoch: 0115 val_loss: 18.572395 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0120 lr: [0.001, 0.001] train_loss: 18.785484 train_acc: 0.586066 train_f1: 0.586066 time: 0.0515s
INFO:root:Epoch: 0120 val_loss: 18.557029 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0125 lr: [0.001, 0.001] train_loss: 18.772435 train_acc: 0.586066 train_f1: 0.586066 time: 0.0505s
INFO:root:Epoch: 0125 val_loss: 18.541547 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0130 lr: [0.001, 0.001] train_loss: 18.759396 train_acc: 0.586066 train_f1: 0.586066 time: 0.0480s
INFO:root:Epoch: 0130 val_loss: 18.526097 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0135 lr: [0.001, 0.001] train_loss: 18.746447 train_acc: 0.586066 train_f1: 0.586066 time: 0.0493s
INFO:root:Epoch: 0135 val_loss: 18.510760 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0140 lr: [0.001, 0.001] train_loss: 18.733628 train_acc: 0.586066 train_f1: 0.586066 time: 0.0482s
INFO:root:Epoch: 0140 val_loss: 18.495583 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0145 lr: [0.001, 0.001] train_loss: 18.720961 train_acc: 0.586066 train_f1: 0.586066 time: 0.0494s
INFO:root:Epoch: 0145 val_loss: 18.480588 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0150 lr: [0.001, 0.001] train_loss: 18.708456 train_acc: 0.586066 train_f1: 0.586066 time: 0.0475s
INFO:root:Epoch: 0150 val_loss: 18.465782 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0155 lr: [0.001, 0.001] train_loss: 18.696114 train_acc: 0.586066 train_f1: 0.586066 time: 0.0483s
INFO:root:Epoch: 0155 val_loss: 18.451167 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0160 lr: [0.001, 0.001] train_loss: 18.683933 train_acc: 0.586066 train_f1: 0.586066 time: 0.0485s
INFO:root:Epoch: 0160 val_loss: 18.436741 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0165 lr: [0.001, 0.001] train_loss: 18.671909 train_acc: 0.586066 train_f1: 0.586066 time: 0.0486s
INFO:root:Epoch: 0165 val_loss: 18.422498 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0170 lr: [0.001, 0.001] train_loss: 18.660036 train_acc: 0.586066 train_f1: 0.586066 time: 0.0491s
INFO:root:Epoch: 0170 val_loss: 18.408431 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0175 lr: [0.001, 0.001] train_loss: 18.648309 train_acc: 0.586066 train_f1: 0.586066 time: 0.0474s
INFO:root:Epoch: 0175 val_loss: 18.394533 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0180 lr: [0.001, 0.001] train_loss: 18.636722 train_acc: 0.586066 train_f1: 0.586066 time: 0.0485s
INFO:root:Epoch: 0180 val_loss: 18.380797 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0185 lr: [0.001, 0.001] train_loss: 18.625267 train_acc: 0.586066 train_f1: 0.586066 time: 0.0475s
INFO:root:Epoch: 0185 val_loss: 18.367217 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0190 lr: [0.001, 0.001] train_loss: 18.613940 train_acc: 0.586066 train_f1: 0.586066 time: 0.0480s
INFO:root:Epoch: 0190 val_loss: 18.353784 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0195 lr: [0.001, 0.001] train_loss: 18.602735 train_acc: 0.586066 train_f1: 0.586066 time: 0.0499s
INFO:root:Epoch: 0195 val_loss: 18.340494 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0200 lr: [0.001, 0.001] train_loss: 18.591647 train_acc: 0.586066 train_f1: 0.586066 time: 0.0492s
INFO:root:Epoch: 0200 val_loss: 18.327339 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0205 lr: [0.001, 0.001] train_loss: 18.580671 train_acc: 0.586066 train_f1: 0.586066 time: 0.0476s
INFO:root:Epoch: 0205 val_loss: 18.314314 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0210 lr: [0.001, 0.001] train_loss: 18.569802 train_acc: 0.586066 train_f1: 0.586066 time: 0.0481s
INFO:root:Epoch: 0210 val_loss: 18.301413 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0215 lr: [0.001, 0.001] train_loss: 18.559035 train_acc: 0.586066 train_f1: 0.586066 time: 0.0517s
INFO:root:Epoch: 0215 val_loss: 18.288631 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0220 lr: [0.001, 0.001] train_loss: 18.548366 train_acc: 0.586066 train_f1: 0.586066 time: 0.0471s
INFO:root:Epoch: 0220 val_loss: 18.275963 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0225 lr: [0.001, 0.001] train_loss: 18.537792 train_acc: 0.586066 train_f1: 0.586066 time: 0.0479s
INFO:root:Epoch: 0225 val_loss: 18.263405 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0230 lr: [0.001, 0.001] train_loss: 18.527308 train_acc: 0.586066 train_f1: 0.586066 time: 0.0483s
INFO:root:Epoch: 0230 val_loss: 18.250952 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0235 lr: [0.001, 0.001] train_loss: 18.516910 train_acc: 0.586066 train_f1: 0.586066 time: 0.0518s
INFO:root:Epoch: 0235 val_loss: 18.238599 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0240 lr: [0.001, 0.001] train_loss: 18.506596 train_acc: 0.586066 train_f1: 0.586066 time: 0.0506s
INFO:root:Epoch: 0240 val_loss: 18.226343 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0245 lr: [0.001, 0.001] train_loss: 18.496361 train_acc: 0.586066 train_f1: 0.586066 time: 0.0476s
INFO:root:Epoch: 0245 val_loss: 18.214180 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0250 lr: [0.001, 0.001] train_loss: 18.486204 train_acc: 0.586066 train_f1: 0.586066 time: 0.0481s
INFO:root:Epoch: 0250 val_loss: 18.202106 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Early stopping
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 17.9419s
INFO:root:Val set results: val_loss: 19.985503 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Test set results: test_loss: 19.984778 test_acc: 0.681818 test_f1: 0.681818
