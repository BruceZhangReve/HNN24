INFO:root:Using: cuda:0
INFO:root:Using seed 18.
INFO:root:Dataset: cornell
INFO:root:Num classes: 5
INFO:root:NCModel(
  (encoder): BKNet(
    (linear_before): BLinear(in_features=1703, out_features=32, c=tensor([1.], device='cuda:0'))
    (layers): Sequential(
      (0): KPGraphConvolution(
        (net): KernelPointAggregation(
          (linears): ModuleList(
            (0): BLayer(
              (linear): BLinear(in_features=32, out_features=16, c=1)
              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))
            )
            (1): BLayer(
              (linear): BLinear(in_features=32, out_features=16, c=1)
              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))
            )
            (2): BLayer(
              (linear): BLinear(in_features=32, out_features=16, c=1)
              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))
            )
            (3): BLayer(
              (linear): BLinear(in_features=32, out_features=16, c=1)
              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))
            )
            (4): BLayer(
              (linear): BLinear(in_features=32, out_features=16, c=1)
              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))
            )
            (5): BLayer(
              (linear): BLinear(in_features=32, out_features=16, c=1)
              (act): BAct(c_in=tensor([1.], device='cuda:0'), c_out=tensor([1.], device='cuda:0'))
            )
          )
        )
      )
    )
  )
  (decoder): PoincareDecoder()
)
INFO:root:Total number of parameters: 57989
INFO:root:Epoch: 0020 lr: [0.0001, 0.0001] train_loss: 0.974750 train_acc: 0.586066 train_f1: 0.586066 time: 0.0456s
INFO:root:Epoch: 0020 val_loss: 0.968603 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0040 lr: [0.0001, 0.0001] train_loss: 0.948099 train_acc: 0.586066 train_f1: 0.586066 time: 0.0440s
INFO:root:Epoch: 0040 val_loss: 0.937107 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0060 lr: [0.0001, 0.0001] train_loss: 0.921287 train_acc: 0.586066 train_f1: 0.586066 time: 0.0463s
INFO:root:Epoch: 0060 val_loss: 0.905405 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0080 lr: [0.0001, 0.0001] train_loss: 0.894215 train_acc: 0.586066 train_f1: 0.586066 time: 0.0450s
INFO:root:Epoch: 0080 val_loss: 0.873377 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0100 lr: [0.0001, 0.0001] train_loss: 0.866779 train_acc: 0.586066 train_f1: 0.586066 time: 0.0438s
INFO:root:Epoch: 0100 val_loss: 0.840901 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0120 lr: [0.0001, 0.0001] train_loss: 0.838866 train_acc: 0.586066 train_f1: 0.586066 time: 0.0487s
INFO:root:Epoch: 0120 val_loss: 0.807843 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0140 lr: [0.0001, 0.0001] train_loss: 0.810354 train_acc: 0.586066 train_f1: 0.586066 time: 0.0487s
INFO:root:Epoch: 0140 val_loss: 0.774056 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0160 lr: [0.0001, 0.0001] train_loss: 0.781101 train_acc: 0.586066 train_f1: 0.586066 time: 0.0522s
INFO:root:Epoch: 0160 val_loss: 0.739373 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0180 lr: [0.0001, 0.0001] train_loss: 0.750948 train_acc: 0.586066 train_f1: 0.586066 time: 0.0515s
INFO:root:Epoch: 0180 val_loss: 0.703601 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0200 lr: [0.0001, 0.0001] train_loss: 0.719704 train_acc: 0.586066 train_f1: 0.586066 time: 0.0486s
INFO:root:Epoch: 0200 val_loss: 0.666514 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0220 lr: [0.0001, 0.0001] train_loss: 0.687140 train_acc: 0.586066 train_f1: 0.586066 time: 0.0489s
INFO:root:Epoch: 0220 val_loss: 0.627835 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0240 lr: [0.0001, 0.0001] train_loss: 0.652974 train_acc: 0.586066 train_f1: 0.586066 time: 0.0491s
INFO:root:Epoch: 0240 val_loss: 0.587228 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0260 lr: [0.0001, 0.0001] train_loss: 0.617608 train_acc: 0.586066 train_f1: 0.586066 time: 0.0499s
INFO:root:Epoch: 0260 val_loss: 0.545836 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0280 lr: [0.0001, 0.0001] train_loss: 0.611167 train_acc: 0.586066 train_f1: 0.586066 time: 0.0460s
INFO:root:Epoch: 0280 val_loss: 0.538587 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0300 lr: [0.0001, 0.0001] train_loss: 0.609683 train_acc: 0.586066 train_f1: 0.586066 time: 0.0463s
INFO:root:Epoch: 0300 val_loss: 0.537587 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0320 lr: [0.0001, 0.0001] train_loss: 0.608027 train_acc: 0.586066 train_f1: 0.586066 time: 0.0455s
INFO:root:Epoch: 0320 val_loss: 0.536412 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0340 lr: [0.0001, 0.0001] train_loss: 0.606429 train_acc: 0.586066 train_f1: 0.586066 time: 0.0454s
INFO:root:Epoch: 0340 val_loss: 0.535424 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0360 lr: [0.0001, 0.0001] train_loss: 0.605031 train_acc: 0.586066 train_f1: 0.586066 time: 0.0462s
INFO:root:Epoch: 0360 val_loss: 0.534362 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0380 lr: [0.0001, 0.0001] train_loss: 0.603651 train_acc: 0.586066 train_f1: 0.586066 time: 0.0470s
INFO:root:Epoch: 0380 val_loss: 0.533202 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0400 lr: [0.0001, 0.0001] train_loss: 0.602239 train_acc: 0.586066 train_f1: 0.586066 time: 0.0479s
INFO:root:Epoch: 0400 val_loss: 0.532035 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0420 lr: [0.0001, 0.0001] train_loss: 0.600823 train_acc: 0.586066 train_f1: 0.586066 time: 0.0471s
INFO:root:Epoch: 0420 val_loss: 0.530841 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0440 lr: [0.0001, 0.0001] train_loss: 0.599394 train_acc: 0.586066 train_f1: 0.586066 time: 0.0457s
INFO:root:Epoch: 0440 val_loss: 0.529629 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0460 lr: [0.0001, 0.0001] train_loss: 0.597932 train_acc: 0.586066 train_f1: 0.586066 time: 0.0463s
INFO:root:Epoch: 0460 val_loss: 0.528421 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0480 lr: [0.0001, 0.0001] train_loss: 0.596466 train_acc: 0.586066 train_f1: 0.586066 time: 0.0476s
INFO:root:Epoch: 0480 val_loss: 0.527214 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0500 lr: [0.0001, 0.0001] train_loss: 0.594983 train_acc: 0.586066 train_f1: 0.586066 time: 0.0461s
INFO:root:Epoch: 0500 val_loss: 0.525937 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0520 lr: [0.0001, 0.0001] train_loss: 0.593447 train_acc: 0.586066 train_f1: 0.586066 time: 0.0477s
INFO:root:Epoch: 0520 val_loss: 0.524596 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0540 lr: [0.0001, 0.0001] train_loss: 0.591910 train_acc: 0.586066 train_f1: 0.586066 time: 0.0499s
INFO:root:Epoch: 0540 val_loss: 0.523329 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0560 lr: [0.0001, 0.0001] train_loss: 0.590344 train_acc: 0.586066 train_f1: 0.586066 time: 0.0454s
INFO:root:Epoch: 0560 val_loss: 0.521994 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0580 lr: [0.0001, 0.0001] train_loss: 0.588790 train_acc: 0.586066 train_f1: 0.586066 time: 0.0468s
INFO:root:Epoch: 0580 val_loss: 0.520666 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0600 lr: [0.0001, 0.0001] train_loss: 0.587187 train_acc: 0.586066 train_f1: 0.586066 time: 0.0455s
INFO:root:Epoch: 0600 val_loss: 0.519316 val_acc: 0.659091 val_f1: 0.659091
INFO:root:Epoch: 0620 lr: [0.0001, 0.0001] train_loss: 0.585620 train_acc: 0.586066 train_f1: 0.586066 time: 0.0464s
INFO:root:Epoch: 0620 val_loss: 0.517960 val_acc: 0.659091 val_f1: 0.659091
