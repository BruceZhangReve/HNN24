{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n",
      "Using seed 1234.\n",
      "Dataset: cornell\n",
      "Num classes: 5\n",
      "dict_keys(['adj_train', 'features', 'labels', 'idx_train', 'idx_val', 'idx_test', 'adj_train_norm'])\n",
      "(183, 183)\n",
      "torch.Size([183, 1703])\n",
      "NCModel(\n",
      "  (encoder): BL(\n",
      "    (layers): Sequential(\n",
      "      (0): BLinear(in_features=1703, out_features=16, c=tensor([1.], device='cuda:0'), use_bias=1, act=<function relu at 0x7fc996bdb250>)\n",
      "      (1): BLinear(in_features=16, out_features=16, c=tensor([1.], device='cuda:0'), use_bias=1, act=<function relu at 0x7fc996bdb250>)\n",
      "    )\n",
      "  )\n",
      "  (decoder): PoincareDecoder()\n",
      ")\n",
      "Total number of parameters: 27637\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from types import SimpleNamespace\n",
    "import sys\n",
    "sys.path.append('/data/lige/HKN')# Please change accordingly!\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from geoopt import ManifoldParameter as geoopt_ManifoldParameter\n",
    "from manifolds.base import ManifoldParameter as base_ManifoldParameter\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "from optim import RiemannianAdam, RiemannianSGD\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from config import parser\n",
    "from models.base_models import NCModel, LPModel, GCModel\n",
    "from utils.data_utils import load_data, get_nei, GCDataset, split_batch\n",
    "from utils.train_utils import get_dir_name, format_metrics\n",
    "from utils.eval_utils import acc_f1\n",
    "\n",
    "from geoopt import ManifoldParameter as geoopt_ManifoldParameter\n",
    "from manifolds.base import ManifoldParameter as base_ManifoldParameter\n",
    "\n",
    "config_args = {\n",
    "    'training_config': {\n",
    "        'use_geoopt': (False, \"which manifold class to use, if false then use basd.manifold\"),\n",
    "        'lr': (0.001, 'learning rate'),\n",
    "        'dropout': (0.2, 'dropout probability'),\n",
    "        'cuda': (0, 'which cuda device to use (-1 for cpu training)'),\n",
    "        'epochs': (100, 'maximum number of epochs to train for'),\n",
    "        'weight_decay': (0., 'l2 regularization strength'),\n",
    "        'optimizer': ('radam', 'which optimizer to use, can be any of [rsgd, radam]'),\n",
    "        'momentum': (0.999, 'momentum in optimizer'),\n",
    "        'patience': (150, 'patience for early stopping'),\n",
    "        'seed': (1234, 'seed for training'),\n",
    "        'log_freq': (1, 'how often to compute print train/val metrics (in epochs)'),\n",
    "        'eval_freq': (1, 'how often to compute val metrics (in epochs)'),\n",
    "        'save': (0, '1 to save model and logs and 0 otherwise'),\n",
    "        'save_dir': (None, 'path to save training logs and model weights (defaults to logs/task/date/run/)'),\n",
    "        'sweep_c': (0, ''),\n",
    "        'lr_reduce_freq': (None, 'reduce lr every lr-reduce-freq or None to keep lr constant'),\n",
    "        'gamma': (0.5, 'gamma for lr scheduler'),\n",
    "        'print_epoch': (True, ''),\n",
    "        'grad_clip': (None, 'max norm for gradient clipping, or None for no gradient clipping'),\n",
    "        'min_epochs': (100, 'do not early stop before min-epochs')\n",
    "    },\n",
    "    'model_config': {\n",
    "        'task': ('nc', 'which tasks to train on, can be any of [lp, nc]'),\n",
    "        'model': ('BL', 'which encoder to use, can be any of [Shallow, MLP, HNN, GCN, GAT, HyperGCN, HyboNet,BKN,BL]'),\n",
    "        'dim': (16, 'embedding dimension'),\n",
    "        'manifold': ('PoincareBall', 'which manifold to use, can be any of [Euclidean, Hyperboloid, PoincareBall, Lorentz]'),\n",
    "        'c': (1.0, 'hyperbolic radius, set to None for trainable curvature'),\n",
    "        'r': (2., 'fermi-dirac decoder parameter for lp'),\n",
    "        't': (1., 'fermi-dirac decoder parameter for lp'),\n",
    "        'margin': (2., 'margin of MarginLoss'),\n",
    "        'pretrained_embeddings': (None, 'path to pretrained embeddings (.npy file) for Shallow node classification'),\n",
    "        'pos_weight': (0, 'whether to upweight positive class in node classification tasks'),\n",
    "        'num_layers': (3, 'number of hidden layers in encoder'),\n",
    "        'bias': (1, 'whether to use bias (1) or not (0)'),\n",
    "        'act': ('relu', 'which activation function to use (or None for no activation)'),\n",
    "        'n_heads': (4, 'number of attention heads for graph attention networks, must be a divisor dim'),\n",
    "        'alpha': (0.2, 'alpha for leakyrelu in graph attention networks'),\n",
    "        'double_precision': ('1', 'whether to use double precision'),\n",
    "        'use_att': (0, 'whether to use hyperbolic attention or not'),\n",
    "        'local_agg': (0, 'whether to local tangent space aggregation or not'),\n",
    "        'kernel_size': (8, 'number of kernels'),\n",
    "        'KP_extent': (0.66, 'influence radius of each kernel point'),\n",
    "        'radius': (1, 'radius used for kernel point init'),\n",
    "        'deformable': (False, 'deformable kernel'),\n",
    "        'linear_before': (64, 'dim of linear before gcn')#64\n",
    "    },\n",
    "    'data_config': {\n",
    "        'dataset': ('cornell', 'which dataset to use'),\n",
    "        'batch_size': (32, 'batch size for gc'),\n",
    "        'val_prop': (0.05, 'proportion of validation edges for link prediction'),\n",
    "        'test_prop': (0.1, 'proportion of test edges for link prediction'),\n",
    "        'use_feats': (1, 'whether to use node features or not'),\n",
    "        'normalize_feats': (1, 'whether to normalize input node features'),\n",
    "        'normalize_adj': (1, 'whether to row-normalize the adjacency matrix'),\n",
    "        'split_seed': (1234, 'seed for data splits (train/test/val)'),\n",
    "        'split_graph': (False, 'whether to split the graph')\n",
    "    }\n",
    "}\n",
    "\n",
    "# 将所有参数转换为 SimpleNamespace\n",
    "args = SimpleNamespace(\n",
    "    **{k: v[0] for config in config_args.values() for k, v in config.items()}\n",
    ")\n",
    "\n",
    "#choose which manifold class to follow \n",
    "if args.use_geoopt == False:\n",
    "    ManifoldParameter = base_ManifoldParameter\n",
    "else:\n",
    "    ManifoldParameter = geoopt_ManifoldParameter\n",
    "np.random.seed(args.seed)#args.seed\n",
    "torch.manual_seed(args.seed)#args.seed\n",
    "if int(args.cuda):#args.double_precision\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "if int(args.cuda) >= 0:#args.cuda\n",
    "    torch.cuda.manual_seed(args.seed)#args.seed\n",
    "args.device = 'cuda:' + str(args.cuda) if int(args.cuda) >= 0 else 'cpu' #args.device actually,<-args.cuda\n",
    "args.patience = args.epochs if not args.patience else args.patience #args.patience<-args.epochs|args.patience\n",
    "\n",
    "print(f'Using: {args.device}')\n",
    "print(\"Using seed {}.\".format(args.seed))\n",
    "print(f\"Dataset: {args.dataset}\")\n",
    "\n",
    "# Load data\n",
    "data = load_data(args, os.path.join('data', args.dataset))\n",
    "if args.task == 'gc':\n",
    "    args.n_nodes, args.feat_dim = data['features'][0].shape\n",
    "else:\n",
    "    args.n_nodes, args.feat_dim = data['features'].shape\n",
    "if args.task == 'nc':\n",
    "    Model = NCModel\n",
    "    args.n_classes = int(data['labels'].max() + 1)\n",
    "    args.data = data\n",
    "    print(f'Num classes: {args.n_classes}')\n",
    "elif args.task == 'gc':\n",
    "    Model = GCModel\n",
    "    args.n_classes = int(data['labels'].max() + 1)\n",
    "    print(f'Num classes: {args.n_classes}')\n",
    "else:\n",
    "    args.nb_false_edges = len(data['train_edges_false'])\n",
    "    args.nb_edges = len(data['train_edges'])\n",
    "    if args.task == 'lp':\n",
    "        Model = LPModel\n",
    "        args.n_classes = 2\n",
    "\n",
    "if not args.lr_reduce_freq:\n",
    "    args.lr_reduce_freq = args.epochs\n",
    "\n",
    "\n",
    "###A simple check on data\n",
    "print(data.keys())\n",
    "print(data['adj_train'].todense().shape)\n",
    "print(data['features'].shape)\n",
    "###A simple check on data\n",
    "\n",
    "# Model and optimizer\n",
    "model = Model(args)\n",
    "print(str(model))\n",
    "no_decay = ['bias', 'scale']\n",
    "optimizer_grouped_parameters = [{\n",
    "    'params': [\n",
    "        p for n, p in model.named_parameters()\n",
    "        if p.requires_grad and not any(\n",
    "            nd in n\n",
    "            for nd in no_decay) and not isinstance(p, ManifoldParameter)\n",
    "    ],\n",
    "    'weight_decay':\n",
    "    args.weight_decay\n",
    "}, {\n",
    "    'params': [\n",
    "        p for n, p in model.named_parameters() if p.requires_grad and any(\n",
    "            nd in n\n",
    "            for nd in no_decay) or isinstance(p, ManifoldParameter)\n",
    "    ],\n",
    "    'weight_decay':\n",
    "    0.0\n",
    "}]\n",
    "if args.optimizer == 'radam':\n",
    "    optimizer = RiemannianAdam(params=optimizer_grouped_parameters,\n",
    "                                lr=args.lr,\n",
    "                                stabilize=10)\n",
    "elif args.optimizer == 'rsgd':\n",
    "    optimizer = RiemannianSGD(params=optimizer_grouped_parameters,\n",
    "                                lr=args.lr,\n",
    "                                stabilize=10)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=int(\n",
    "                                                args.lr_reduce_freq),\n",
    "                                                gamma=float(args.gamma))\n",
    "tot_params = sum([np.prod(p.size()) for p in model.parameters()])\n",
    "model = model.to(args.device)\n",
    "for x, val in data.items():\n",
    "    if torch.is_tensor(data[x]):\n",
    "        data[x] = data[x].to(args.device)\n",
    "print(f\"Total number of parameters: {tot_params}\")\n",
    "\n",
    "# Train model for nc:\n",
    "t_total = time.time()\n",
    "counter = 0\n",
    "best_val_metrics = model.init_metric_dict()\n",
    "best_test_metrics = None\n",
    "best_emb = None\n",
    "if args.n_classes > 2:\n",
    "    f1_average = 'micro'\n",
    "else:\n",
    "    f1_average = 'binary'\n",
    "\n",
    "if args.model == 'HKPNet':\n",
    "    nei, nei_mask = get_nei(data['adj_train'])\n",
    "    nei = nei.to(args.device)\n",
    "    nei_mask = nei_mask.to(args.device)\n",
    "elif args.model == 'BKNet':\n",
    "    nei, nei_mask = get_nei(data['adj_train'])\n",
    "    nei = nei.to(args.device)\n",
    "    nei_mask = nei_mask.to(args.device) #nei/nei_mask on cuda now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data['labels'])\n",
    "#embeddings=model.encode(data['features'],(nei,nei_mask))#(n,d')->(183,32)\n",
    "#print(embeddings)\n",
    "#print(embeddings[3].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndecoded_values=[]\\nembedded_values=[]\\nmodel_linear_weight=[]\\n#model.encoder.layers[0].net.linears[0].linear.weight\\nfor epoch in range(args.epochs):\\n    t = time.time()\\n    model.train()\\n    optimizer.zero_grad()\\n    if args.model == \\'HKPNet\\':\\n        embeddings = model.encode(data[\\'features\\'], (nei, nei_mask))\\n        # print(embeddings.isnan().sum())\\n    elif args.model == \\'BKNet\\':\\n        #print(data[\\'features\\'].dtype)#It\\'s already torch.float64\\n        embeddings = model.encode(data[\\'features\\'], (nei, nei_mask))#if correctly, embeddings on cuda as well\\n    else:\\n        embeddings = model.encode(data[\\'features\\'], data[\\'adj_train_norm\\'])\\n    train_metrics = model.compute_metrics(embeddings, data, \\'train\\')\\n    #Check decoded_values and embeddings\\n    embedded_values.append(embeddings)\\n    idx = data[f\\'idx_train\\']\\n    output = model.decode(embeddings, data[\\'adj_train_norm\\'], idx)\\n    decoded_values.append(output)\\n    model_linear_weight.append(model.encoder.layers[0].net.linears[0].linear.weight)\\n    #finished\\n    train_metrics[\\'loss\\'].backward()\\n    if args.grad_clip is not None:\\n        torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\\n    optimizer.step()\\n    lr_scheduler.step()\\n    if (epoch + 1) % args.log_freq == 0:\\n        print(\" \".join([\\n            \\'Epoch: {:04d}\\'.format(epoch + 1),\\n            \\'lr: {}\\'.format(lr_scheduler.get_last_lr()),\\n            format_metrics(train_metrics, \\'train\\'),\\n            \\'time: {:.4f}s\\'.format(time.time() - t)\\n        ]))\\n    with torch.no_grad():\\n        if (epoch + 1) % args.eval_freq == 0:\\n            model.eval()\\n            if args.model == \\'HKPNet\\':\\n                embeddings = model.encode(data[\\'features\\'], (nei, nei_mask))\\n            elif args.model == \\'BKNet\\':\\n                embeddings = model.encode(data[\\'features\\'], (nei, nei_mask))\\n            else:\\n                embeddings = model.encode(data[\\'features\\'],\\n                                        data[\\'adj_train_norm\\'])\\n            val_metrics = model.compute_metrics(embeddings, data, \\'val\\')\\n            if (epoch + 1) % args.log_freq == 0:\\n                print(\" \".join([\\n                    \\'Epoch: {:04d}\\'.format(epoch + 1),\\n                    format_metrics(val_metrics, \\'val\\')\\n                ]))\\n            if model.has_improved(best_val_metrics, val_metrics):\\n                best_test_metrics = model.compute_metrics(\\n                    embeddings, data, \\'test\\')\\n                best_emb = embeddings.cpu()\\n                if args.save:\\n                    np.save(os.path.join(save_dir, \\'embeddings.npy\\'),\\n                            best_emb.detach().numpy())\\n                best_val_metrics = val_metrics\\n                counter = 0\\n            else:\\n                counter += 1\\n                if counter == args.patience and epoch > args.min_epochs:\\n                    print(\"Early stopping\")\\n                    break\\n\\nprint(\"Optimization Finished!\")\\nprint(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\\nif not best_test_metrics:\\n    model.eval()\\n    best_emb = model.encode(data[\\'features\\'], data[\\'adj_train_norm\\'])\\n    best_test_metrics = model.compute_metrics(best_emb, data, \\'test\\')\\nprint(\" \".join(\\n    [\"Val set results:\",\\n    format_metrics(best_val_metrics, \\'val\\')]))\\nprint(\" \".join(\\n    [\"Test set results:\",\\n    format_metrics(best_test_metrics, \\'test\\')]))\\nif args.save:\\n    np.save(os.path.join(save_dir, \\'embeddings.npy\\'),\\n            best_emb.cpu().detach().numpy())\\n    if hasattr(model.encoder, \\'att_adj\\'):\\n        filename = os.path.join(save_dir, args.dataset + \\'_att_adj.p\\')\\n        pickle.dump(model.encoder.att_adj.cpu().to_dense(),\\n                    open(filename, \\'wb\\'))\\n        print(\\'Dumped attention adj: \\' + filename)\\n\\n    torch.save(model.state_dict(), os.path.join(save_dir, \\'model.pth\\'))\\n    json.dump(vars(args), open(os.path.join(save_dir, \\'config.json\\'), \\'w\\'))\\n    logging.info(f\"Saved model in {save_dir}\")\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "decoded_values=[]\n",
    "embedded_values=[]\n",
    "model_linear_weight=[]\n",
    "#model.encoder.layers[0].net.linears[0].linear.weight\n",
    "for epoch in range(args.epochs):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    if args.model == 'HKPNet':\n",
    "        embeddings = model.encode(data['features'], (nei, nei_mask))\n",
    "        # print(embeddings.isnan().sum())\n",
    "    elif args.model == 'BKNet':\n",
    "        #print(data['features'].dtype)#It's already torch.float64\n",
    "        embeddings = model.encode(data['features'], (nei, nei_mask))#if correctly, embeddings on cuda as well\n",
    "    else:\n",
    "        embeddings = model.encode(data['features'], data['adj_train_norm'])\n",
    "    train_metrics = model.compute_metrics(embeddings, data, 'train')\n",
    "    #Check decoded_values and embeddings\n",
    "    embedded_values.append(embeddings)\n",
    "    idx = data[f'idx_train']\n",
    "    output = model.decode(embeddings, data['adj_train_norm'], idx)\n",
    "    decoded_values.append(output)\n",
    "    model_linear_weight.append(model.encoder.layers[0].net.linears[0].linear.weight)\n",
    "    #finished\n",
    "    train_metrics['loss'].backward()\n",
    "    if args.grad_clip is not None:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    if (epoch + 1) % args.log_freq == 0:\n",
    "        print(\" \".join([\n",
    "            'Epoch: {:04d}'.format(epoch + 1),\n",
    "            'lr: {}'.format(lr_scheduler.get_last_lr()),\n",
    "            format_metrics(train_metrics, 'train'),\n",
    "            'time: {:.4f}s'.format(time.time() - t)\n",
    "        ]))\n",
    "    with torch.no_grad():\n",
    "        if (epoch + 1) % args.eval_freq == 0:\n",
    "            model.eval()\n",
    "            if args.model == 'HKPNet':\n",
    "                embeddings = model.encode(data['features'], (nei, nei_mask))\n",
    "            elif args.model == 'BKNet':\n",
    "                embeddings = model.encode(data['features'], (nei, nei_mask))\n",
    "            else:\n",
    "                embeddings = model.encode(data['features'],\n",
    "                                        data['adj_train_norm'])\n",
    "            val_metrics = model.compute_metrics(embeddings, data, 'val')\n",
    "            if (epoch + 1) % args.log_freq == 0:\n",
    "                print(\" \".join([\n",
    "                    'Epoch: {:04d}'.format(epoch + 1),\n",
    "                    format_metrics(val_metrics, 'val')\n",
    "                ]))\n",
    "            if model.has_improved(best_val_metrics, val_metrics):\n",
    "                best_test_metrics = model.compute_metrics(\n",
    "                    embeddings, data, 'test')\n",
    "                best_emb = embeddings.cpu()\n",
    "                if args.save:\n",
    "                    np.save(os.path.join(save_dir, 'embeddings.npy'),\n",
    "                            best_emb.detach().numpy())\n",
    "                best_val_metrics = val_metrics\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter == args.patience and epoch > args.min_epochs:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "if not best_test_metrics:\n",
    "    model.eval()\n",
    "    best_emb = model.encode(data['features'], data['adj_train_norm'])\n",
    "    best_test_metrics = model.compute_metrics(best_emb, data, 'test')\n",
    "print(\" \".join(\n",
    "    [\"Val set results:\",\n",
    "    format_metrics(best_val_metrics, 'val')]))\n",
    "print(\" \".join(\n",
    "    [\"Test set results:\",\n",
    "    format_metrics(best_test_metrics, 'test')]))\n",
    "if args.save:\n",
    "    np.save(os.path.join(save_dir, 'embeddings.npy'),\n",
    "            best_emb.cpu().detach().numpy())\n",
    "    if hasattr(model.encoder, 'att_adj'):\n",
    "        filename = os.path.join(save_dir, args.dataset + '_att_adj.p')\n",
    "        pickle.dump(model.encoder.att_adj.cpu().to_dense(),\n",
    "                    open(filename, 'wb'))\n",
    "        print('Dumped attention adj: ' + filename)\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, 'model.pth'))\n",
    "    json.dump(vars(args), open(os.path.join(save_dir, 'config.json'), 'w'))\n",
    "    logging.info(f\"Saved model in {save_dir}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if len(data['idx_train']) != len(set(data['idx_train'])): print(\"Has Duplicates\")\n",
    "#print(data['features'].shape)\n",
    "#print(data['features'][data['idx_train']].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data['features'][data['idx_train']].shape)\n",
    "#print(data['labels'][data['idx_train']].shape)\n",
    "\n",
    "#print(embedded_values[0].shape)\n",
    "#print(decoded_values[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer=0\n",
    "#print(decoded_values[layer][1],\"\\n\",decoded_values[layer][10],\"\\n\",decoded_values[layer][20],\"\\n\")\n",
    "##layer=4\n",
    "#print(decoded_values[layer][1],\"\\n\",decoded_values[layer][10],\"\\n\",decoded_values[layer][20],\"\\n\")\n",
    "\n",
    "#layer=0\n",
    "#print(embedded_values[layer][1],\"\\n\",embedded_values[layer][10],\"\\n\",embedded_values[layer][20],\"\\n\")\n",
    "#layer=10\n",
    "#print(embedded_values[layer][1],\"\\n\",embedded_values[layer][10],\"\\n\",embedded_values[layer][20],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model.encoder.layers[0].net.linears[0])\n",
    "#print(model_linear_weight[0] == model_linear_weight[20])\n",
    "#print(model_linear_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Parameter Updating Correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'norm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m], (nei, nei_mask))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBKNet\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 19\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnei\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnei_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madj_train_norm\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/data/lige/HKN/models/base_models.py:52\u001b[0m, in \u001b[0;36mBaseModel.encode\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanifold\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPoincareBall\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     51\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanifold\u001b[38;5;241m.\u001b[39mexpmap0(x,c\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc)\u001b[38;5;66;03m#Using manifold.base\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#Note: h is the updated feature points matrix of shape (n,d')\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h\n",
      "File \u001b[0;32m/data/lige/HKN/models/encoders.py:380\u001b[0m, in \u001b[0;36mBKNet.encode\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m    378\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_before(x)\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m#print(f\"adj: {adj}\")\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBKNet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/lige/HKN/models/encoders.py:35\u001b[0m, in \u001b[0;36mEncoder.encode\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m     33\u001b[0m     nei, nei_mask \u001b[38;5;241m=\u001b[39m adj\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m (x, nei, nei_mask)\n\u001b[0;32m---> 35\u001b[0m     output, _, __ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m#Actually corresponds to (h, nei, nei_mask), but we only need h as output\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_graph:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#Not sure what this means\u001b[39;00m\n",
      "File \u001b[0;32m/data/lige/HKN/layers/B_layers.py:181\u001b[0m, in \u001b[0;36mBLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    179\u001b[0m drop_weight \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# Perform Mobius matrix-vector multiplication\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m mv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanifold\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmobius_matvec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrop_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Project the result\u001b[39;00m\n\u001b[1;32m    183\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanifold\u001b[38;5;241m.\u001b[39mproj(mv, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc)\n",
      "File \u001b[0;32m/data/lige/HKN/manifolds/poincare.py:98\u001b[0m, in \u001b[0;36mPoincareBall.mobius_matvec\u001b[0;34m(self, m, x, c)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmobius_matvec\u001b[39m(\u001b[38;5;28mself\u001b[39m, m, x, c):\n\u001b[1;32m     97\u001b[0m     sqrt_c \u001b[38;5;241m=\u001b[39m c \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m---> 98\u001b[0m     x_norm \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp_min(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_norm)\n\u001b[1;32m     99\u001b[0m     mx \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m@\u001b[39m m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    100\u001b[0m     mx_norm \u001b[38;5;241m=\u001b[39m mx\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp_min(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_norm)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'norm'"
     ]
    }
   ],
   "source": [
    "decoded_values=[]\n",
    "embedded_values=[]\n",
    "model_linear_weight=[]\n",
    "\n",
    "# 检查权重更新\n",
    "def check_weights(model, epoch):\n",
    "    print(f\"Epoch {epoch + 1} - Model Linear Weight:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name}: {param.data}\")\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    if args.model == 'HKPNet':\n",
    "        embeddings = model.encode(data['features'], (nei, nei_mask))\n",
    "    elif args.model == 'BKNet':\n",
    "        embeddings = model.encode(data['features'], (nei, nei_mask))\n",
    "    else:\n",
    "        embeddings = model.encode(data['features'], data['adj_train_norm'])\n",
    "    train_metrics = model.compute_metrics(embeddings, data, 'train')\n",
    "    \n",
    "    # 检查 decoded_values 和 embeddings\n",
    "    embedded_values.append(embeddings)\n",
    "    idx = data[f'idx_train']\n",
    "    output = model.decode(embeddings, data['adj_train_norm'], idx)\n",
    "    decoded_values.append(output)\n",
    "    model_linear_weight.append(model.encoder.layers[0].net.linears[0].weight.clone())\n",
    "    \n",
    "    # 检查梯度是否被正确计算\n",
    "    train_metrics['loss'].backward()\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            print(f\"Grad of {name}: {param.grad.abs().mean()}\")\n",
    "    \n",
    "    if args.grad_clip is not None:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # 打印和检查权重是否被更新\n",
    "    check_weights(model, epoch)\n",
    "    \n",
    "    if (epoch + 1) % args.log_freq == 0:\n",
    "        print(\" \".join([\n",
    "            'Epoch: {:04d}'.format(epoch + 1),\n",
    "            'lr: {}'.format(lr_scheduler.get_last_lr()),\n",
    "            format_metrics(train_metrics, 'train'),\n",
    "            'time: {:.4f}s'.format(time.time() - t)\n",
    "        ]))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if (epoch + 1) % args.eval_freq == 0:\n",
    "            model.eval()\n",
    "            if args.model == 'HKPNet':\n",
    "                embeddings = model.encode(data['features'], (nei, nei_mask))\n",
    "            elif args.model == 'BKNet':\n",
    "                embeddings = model.encode(data['features'], (nei, nei_mask))\n",
    "            else:\n",
    "                embeddings = model.encode(data['features'],\n",
    "                                        data['adj_train_norm'])\n",
    "            val_metrics = model.compute_metrics(embeddings, data, 'val')\n",
    "            if (epoch + 1) % args.log_freq == 0:\n",
    "                print(\" \".join([\n",
    "                    'Epoch: {:04d}'.format(epoch + 1),\n",
    "                    format_metrics(val_metrics, 'val')\n",
    "                ]))\n",
    "            if model.has_improved(best_val_metrics, val_metrics):\n",
    "                best_test_metrics = model.compute_metrics(\n",
    "                    embeddings, data, 'test')\n",
    "                best_emb = embeddings.cpu()\n",
    "                if args.save:\n",
    "                    np.save(os.path.join(save_dir, 'embeddings.npy'),\n",
    "                            best_emb.detach().numpy())\n",
    "                best_val_metrics = val_metrics\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter == args.patience and epoch > args.min_epochs:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "if not best_test_metrics:\n",
    "    model.eval()\n",
    "    best_emb = model.encode(data['features'], data['adj_train_norm'])\n",
    "    best_test_metrics = model.compute_metrics(best_emb, data, 'test')\n",
    "print(\" \".join(\n",
    "    [\"Val set results:\",\n",
    "    format_metrics(best_val_metrics, 'val')]))\n",
    "print(\" \".join(\n",
    "    [\"Test set results:\",\n",
    "    format_metrics(best_test_metrics, 'test')]))\n",
    "if args.save:\n",
    "    np.save(os.path.join(save_dir, 'embeddings.npy'),\n",
    "            best_emb.cpu().detach().numpy())\n",
    "    if hasattr(model.encoder, 'att_adj'):\n",
    "        filename = os.path.join(save_dir, args.dataset + '_att_adj.p')\n",
    "        pickle.dump(model.encoder.att_adj.cpu().to_dense(),\n",
    "                    open(filename, 'wb'))\n",
    "        print('Dumped attention adj: ' + filename)\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, 'model.pth'))\n",
    "    json.dump(vars(args), open(os.path.join(save_dir, 'config.json'), 'w'))\n",
    "    logging.info(f\"Saved model in {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.encoder.layers[0].net.linears[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_before.weight: requires_grad=True\n",
      "linear_before.bias: requires_grad=True\n",
      "layers.0.net.kernel_tangents: requires_grad=False\n",
      "layers.0.net.linears.0.weight: requires_grad=True\n",
      "layers.0.net.linears.0.bias: requires_grad=True\n",
      "layers.0.net.linears.1.weight: requires_grad=True\n",
      "layers.0.net.linears.1.bias: requires_grad=True\n",
      "layers.0.net.linears.2.weight: requires_grad=True\n",
      "layers.0.net.linears.2.bias: requires_grad=True\n",
      "layers.0.net.linears.3.weight: requires_grad=True\n",
      "layers.0.net.linears.3.bias: requires_grad=True\n",
      "layers.0.net.linears.4.weight: requires_grad=True\n",
      "layers.0.net.linears.4.bias: requires_grad=True\n",
      "layers.0.net.linears.5.weight: requires_grad=True\n",
      "layers.0.net.linears.5.bias: requires_grad=True\n",
      "layers.0.net.linears.6.weight: requires_grad=True\n",
      "layers.0.net.linears.6.bias: requires_grad=True\n",
      "layers.0.net.linears.7.weight: requires_grad=True\n",
      "layers.0.net.linears.7.bias: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "# 检查 encoder 参数的 requires_grad 属性\n",
    "for name, param in model.encoder.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HKNet",
   "language": "python",
   "name": "hknet"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
