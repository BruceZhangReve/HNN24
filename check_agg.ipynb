{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/lige/miniconda3/envs/HKNet/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 设定种子\n",
    "set_seed(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n",
      "Using seed 18.\n",
      "Dataset: wisconsin\n",
      "Num classes: 5\n",
      "dict_keys(['adj_train', 'features', 'labels', 'idx_train', 'idx_val', 'idx_test', 'adj_train_norm'])\n",
      "(251, 251)\n",
      "torch.Size([251, 1703])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/lige/HKN/utils/data_utils.py:347: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G, sorted(G.nodes()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCModel(\n",
      "  (encoder): BKNet(\n",
      "    (linear_before): BLinear(\n",
      "      in_features=1703, out_features=64, c=tensor([1.], device='cuda:0'), use_bias=1, act=None, dropout_rate=0.25\n",
      "      (dropout): Dropout(p=0.25, inplace=False)\n",
      "      (E_linear): Linear(in_features=1703, out_features=64, bias=False)\n",
      "    )\n",
      "    (layers): Sequential(\n",
      "      (0): KPGraphConvolution(\n",
      "        (net): KernelPointAggregation(\n",
      "          (linears): ModuleList(\n",
      "            (0): BLinear(\n",
      "              in_features=64, out_features=32, c=tensor([1.], device='cuda:0'), use_bias=1, act=None, dropout_rate=0.25\n",
      "              (dropout): Dropout(p=0.25, inplace=False)\n",
      "              (E_linear): Linear(in_features=64, out_features=32, bias=False)\n",
      "            )\n",
      "            (1): BLinear(\n",
      "              in_features=64, out_features=32, c=tensor([1.], device='cuda:0'), use_bias=1, act=None, dropout_rate=0.25\n",
      "              (dropout): Dropout(p=0.25, inplace=False)\n",
      "              (E_linear): Linear(in_features=64, out_features=32, bias=False)\n",
      "            )\n",
      "            (2): BLinear(\n",
      "              in_features=64, out_features=32, c=tensor([1.], device='cuda:0'), use_bias=1, act=None, dropout_rate=0.25\n",
      "              (dropout): Dropout(p=0.25, inplace=False)\n",
      "              (E_linear): Linear(in_features=64, out_features=32, bias=False)\n",
      "            )\n",
      "            (3): BLinear(\n",
      "              in_features=64, out_features=32, c=tensor([1.], device='cuda:0'), use_bias=1, act=None, dropout_rate=0.25\n",
      "              (dropout): Dropout(p=0.25, inplace=False)\n",
      "              (E_linear): Linear(in_features=64, out_features=32, bias=False)\n",
      "            )\n",
      "            (4): BLinear(\n",
      "              in_features=64, out_features=32, c=tensor([1.], device='cuda:0'), use_bias=1, act=None, dropout_rate=0.25\n",
      "              (dropout): Dropout(p=0.25, inplace=False)\n",
      "              (E_linear): Linear(in_features=64, out_features=32, bias=False)\n",
      "            )\n",
      "            (5): BLinear(\n",
      "              in_features=64, out_features=32, c=tensor([1.], device='cuda:0'), use_bias=1, act=None, dropout_rate=0.25\n",
      "              (dropout): Dropout(p=0.25, inplace=False)\n",
      "              (E_linear): Linear(in_features=64, out_features=32, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): BAct(c=tensor([1.], device='cuda:0'), act=<function relu at 0x7f4435981360>)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): PoincareDecoder()\n",
      ")\n",
      "Total number of parameters: 122117\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from types import SimpleNamespace\n",
    "import sys\n",
    "sys.path.append('/data/lige/HKN')# Please change accordingly!\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from geoopt import ManifoldParameter as geoopt_ManifoldParameter\n",
    "from manifolds.base import ManifoldParameter as base_ManifoldParameter\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "from optim import RiemannianAdam, RiemannianSGD\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from config import parser\n",
    "from models.base_models import NCModel, LPModel, GCModel\n",
    "from utils.data_utils import load_data, get_nei, GCDataset, split_batch\n",
    "from utils.train_utils import get_dir_name, format_metrics\n",
    "from utils.eval_utils import acc_f1\n",
    "\n",
    "from geoopt import ManifoldParameter as geoopt_ManifoldParameter\n",
    "from manifolds.base import ManifoldParameter as base_ManifoldParameter\n",
    "\n",
    "#import torch.nn.functional as F\n",
    "\n",
    "\n",
    "config_args = {\n",
    "    'training_config': {\n",
    "        'lr': (1e-4, 'learning rate'),\n",
    "        'dropout': (0.25, 'dropout probability'),\n",
    "        'cuda': (0, 'which cuda device to use (-1 for cpu training)'),\n",
    "        'epochs': (1000, 'maximum number of epochs to train for'),\n",
    "        'weight_decay': (1e-4, 'l2 regularization strength'),\n",
    "        'optimizer': ('radam', 'which optimizer to use, can be any of [rsgd, radam]'),\n",
    "        'momentum': (0.999, 'momentum in optimizer'),\n",
    "        'patience': (15, 'patience for early stopping'),\n",
    "        'seed': (18, 'seed for training'),\n",
    "        'log_freq': (1, 'how often to compute print train/val metrics (in epochs)'),\n",
    "        'eval_freq': (1, 'how often to compute val metrics (in epochs)'),\n",
    "        'save': (0, '1 to save model and logs and 0 otherwise'),\n",
    "        'save_dir': (None, 'path to save training logs and model weights (defaults to logs/task/date/run/)'),\n",
    "        'sweep_c': (0, ''),\n",
    "        'lr_reduce_freq': (None, 'reduce lr every lr-reduce-freq or None to keep lr constant'),\n",
    "        'gamma': (0.5, 'gamma for lr scheduler'),\n",
    "        'print_epoch': (True, ''),\n",
    "        'grad_clip': (None, 'max norm for gradient clipping, or None for no gradient clipping'),\n",
    "        'min_epochs': (300, 'do not early stop before min-epochs')\n",
    "    },\n",
    "    'model_config': {\n",
    "        'use_geoopt': (False, \"which manifold class to use, if false then use basd.manifold\"),\n",
    "        'AggKlein':(False, \"if false, then use hyperboloid centorid for aggregation\"),\n",
    "        'corr': (0,'0: d(x_i ominus x, x_k), 1: d(x_ik,x_k)'),\n",
    "        'task': ('nc', 'which tasks to train on, can be any of [lp, nc]'),\n",
    "        'model': ('BKNet', 'which encoder to use, can be any of [Shallow, MLP, HNN, GCN, GAT, HyperGCN, HyboNet,BKNet,BMLP]'),\n",
    "        'dim': (32, 'embedding dimension'),\n",
    "        'manifold': ('PoincareBall', 'which manifold to use, can be any of [Euclidean, Hyperboloid, PoincareBall, Lorentz]'),\n",
    "        'c': (1.0, 'hyperbolic radius, set to None for trainable curvature'),\n",
    "        'r': (2., 'fermi-dirac decoder parameter for lp'),\n",
    "        't': (1., 'fermi-dirac decoder parameter for lp'),\n",
    "        'margin': (2., 'margin of MarginLoss'),\n",
    "        'pretrained_embeddings': (None, 'path to pretrained embeddings (.npy file) for Shallow node classification'),\n",
    "        'pos_weight': (0, 'whether to upweight positive class in node classification tasks'),\n",
    "        'num_layers': (2, 'number of hidden layers in encoder'),\n",
    "        'bias': (1, 'whether to use bias (1) or not (0)'),\n",
    "        'act': ('relu', 'which activation function to use (or None for no activation)'),\n",
    "        'n_heads': (4, 'number of attention heads for graph attention networks, must be a divisor dim'),\n",
    "        'alpha': (0.2, 'alpha for leakyrelu in graph attention networks'),\n",
    "        'double_precision': ('1', 'whether to use double precision'),\n",
    "        'use_att': (0, 'whether to use hyperbolic attention or not'),\n",
    "        'local_agg': (0, 'whether to local tangent space aggregation or not'),\n",
    "        'kernel_size': (6, 'number of kernels'),\n",
    "        'KP_extent': (0.66, 'influence radius of each kernel point'),\n",
    "        'radius': (1, 'radius used for kernel point init'),\n",
    "        'deformable': (False, 'deformable kernel'),\n",
    "        'linear_before': (64, 'dim of linear before gcn')#64\n",
    "    },\n",
    "    'data_config': {\n",
    "        'dataset': ('wisconsin', 'which dataset to use(cornell,wisconsin,squirrel,cora)'),\n",
    "        'batch_size': (32, 'batch size for gc'),\n",
    "        'val_prop': (0.05, 'proportion of validation edges for link prediction'),\n",
    "        'test_prop': (0.1, 'proportion of test edges for link prediction'),\n",
    "        'use_feats': (1, 'whether to use node features or not'),\n",
    "        'normalize_feats': (1, 'whether to normalize input node features'),\n",
    "        'normalize_adj': (1, 'whether to row-normalize the adjacency matrix'),\n",
    "        'split_seed': (1234, 'seed for data splits (train/test/val)'),\n",
    "        'split_graph': (False, 'whether to split the graph')\n",
    "    }\n",
    "}\n",
    "\n",
    "# 将所有参数转换为 SimpleNamespace\n",
    "args = SimpleNamespace(\n",
    "    **{k: v[0] for config in config_args.values() for k, v in config.items()}\n",
    ")\n",
    "\n",
    "#choose which manifold class to follow \n",
    "if args.use_geoopt == False:\n",
    "    ManifoldParameter = base_ManifoldParameter\n",
    "else:\n",
    "    ManifoldParameter = geoopt_ManifoldParameter\n",
    "np.random.seed(args.seed)#args.seed\n",
    "torch.manual_seed(args.seed)#args.seed\n",
    "if int(args.cuda):#args.double_precision\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "if int(args.cuda) >= 0:#args.cuda\n",
    "    torch.cuda.manual_seed(args.seed)#args.seed\n",
    "args.device = 'cuda:' + str(args.cuda) if int(args.cuda) >= 0 else 'cpu' #args.device actually,<-args.cuda\n",
    "args.patience = args.epochs if not args.patience else args.patience #args.patience<-args.epochs|args.patience\n",
    "\n",
    "print(f'Using: {args.device}')\n",
    "print(\"Using seed {}.\".format(args.seed))\n",
    "print(f\"Dataset: {args.dataset}\")\n",
    "\n",
    "# Load data\n",
    "data = load_data(args, os.path.join('data', args.dataset))\n",
    "if args.task == 'gc':\n",
    "    args.n_nodes, args.feat_dim = data['features'][0].shape\n",
    "else:\n",
    "    args.n_nodes, args.feat_dim = data['features'].shape\n",
    "if args.task == 'nc':\n",
    "    Model = NCModel\n",
    "    args.n_classes = int(data['labels'].max() + 1)\n",
    "    args.data = data\n",
    "    print(f'Num classes: {args.n_classes}')\n",
    "elif args.task == 'gc':\n",
    "    Model = GCModel\n",
    "    args.n_classes = int(data['labels'].max() + 1)\n",
    "    print(f'Num classes: {args.n_classes}')\n",
    "else:\n",
    "    args.nb_false_edges = len(data['train_edges_false'])\n",
    "    args.nb_edges = len(data['train_edges'])\n",
    "    if args.task == 'lp':\n",
    "        Model = LPModel\n",
    "        args.n_classes = 2\n",
    "\n",
    "if not args.lr_reduce_freq:\n",
    "    args.lr_reduce_freq = args.epochs\n",
    "\n",
    "\n",
    "###A simple check on data\n",
    "print(data.keys())\n",
    "print(data['adj_train'].todense().shape)\n",
    "print(data['features'].shape)\n",
    "###A simple check on data\n",
    "\n",
    "# Model and optimizer\n",
    "model = Model(args)\n",
    "print(str(model))\n",
    "no_decay = ['bias', 'scale']\n",
    "optimizer_grouped_parameters = [{\n",
    "    'params': [\n",
    "        p for n, p in model.named_parameters()\n",
    "        if p.requires_grad and not any(\n",
    "            nd in n\n",
    "            for nd in no_decay) and not isinstance(p, ManifoldParameter)\n",
    "    ],\n",
    "    'weight_decay':\n",
    "    args.weight_decay\n",
    "}, {\n",
    "    'params': [\n",
    "        p for n, p in model.named_parameters() if p.requires_grad and any(\n",
    "            nd in n\n",
    "            for nd in no_decay) or isinstance(p, ManifoldParameter)\n",
    "    ],\n",
    "    'weight_decay':\n",
    "    0.0\n",
    "}]\n",
    "if args.optimizer == 'radam':\n",
    "    optimizer = RiemannianAdam(params=optimizer_grouped_parameters,\n",
    "                                lr=args.lr,\n",
    "                                stabilize=10)\n",
    "elif args.optimizer == 'rsgd':\n",
    "    optimizer = RiemannianSGD(params=optimizer_grouped_parameters,\n",
    "                                lr=args.lr,\n",
    "                                stabilize=10)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=int(\n",
    "                                                args.lr_reduce_freq),\n",
    "                                                gamma=float(args.gamma))\n",
    "tot_params = sum([np.prod(p.size()) for p in model.parameters()])\n",
    "model = model.to(args.device)\n",
    "for x, val in data.items():\n",
    "    if torch.is_tensor(data[x]):\n",
    "        data[x] = data[x].to(args.device)\n",
    "print(f\"Total number of parameters: {tot_params}\")\n",
    "\n",
    "# Train model for nc:\n",
    "t_total = time.time()\n",
    "counter = 0\n",
    "best_val_metrics = model.init_metric_dict()\n",
    "best_test_metrics = None\n",
    "best_emb = None\n",
    "if args.n_classes > 2:\n",
    "    f1_average = 'micro'\n",
    "else:\n",
    "    f1_average = 'binary'\n",
    "\n",
    "if args.model == 'HKPNet':\n",
    "    nei, nei_mask = get_nei(data['adj_train'])\n",
    "    nei = nei.to(args.device)\n",
    "    nei_mask = nei_mask.to(args.device)\n",
    "elif args.model == 'BKNet':\n",
    "    nei, nei_mask = get_nei(data['adj_train'])\n",
    "    nei = nei.to(args.device)\n",
    "    nei_mask = nei_mask.to(args.device) #nei/nei_mask on cuda now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check whether nei,nei_mask,x_nei are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([251, 122]) torch.Size([251, 122])\n",
      "This construction is different from what we usually have as an adjacency matrix.\n",
      "Checked it's correct\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Basic datas:\n",
    "    Parameters:\n",
    "    adj (scipy.sparse matrix): A sparse adjacency matrix representing the graph.\n",
    "\n",
    "    Returns:\n",
    "    nei (torch.Tensor): A tensor of shape (n, num_nei) where each row contains the indices of\n",
    "                        the neighbors of the corresponding node, padded with 0s to the maximum \n",
    "                        number of neighbors.\n",
    "    nei_mask (torch.Tensor): A mask tensor of the same shape as nei, where a value of 1 indicates\n",
    "                             a valid neighbor index and 0 indicates padding.\n",
    "\n",
    "    Example:\n",
    "    Given an adjacency matrix adj:\n",
    "    [[0, 1, 0, 0],\n",
    "     [1, 0, 1, 0],\n",
    "     [0, 1, 0, 1],\n",
    "     [0, 0, 1, 0]]\n",
    "\n",
    "    The graph is:\n",
    "    Node 0 is connected to Node 1\n",
    "    Node 1 is connected to Node 0 and Node 2\n",
    "    Node 2 is connected to Node 1 and Node 3\n",
    "    Node 3 is connected to Node 2\n",
    "\n",
    "    n = 4  # Number of nodes\n",
    "    num_nei = 2  # Maximum number of neighbors (Node 1 and Node 2 have the most neighbors, 2)\n",
    "\n",
    "    adj_list = [[1], [0, 2], [1, 3], [2]]\n",
    "\n",
    "    The function generates:\n",
    "    nei = [\n",
    "        [1, 0],    # Neighbors of Node 0 (padded with 0)\n",
    "        [0, 2],    # Neighbors of Node 1\n",
    "        [1, 3],    # Neighbors of Node 2\n",
    "        [2, 0]     # Neighbors of Node 3 (padded with 0)\n",
    "    ]\n",
    "\n",
    "    nei_mask = [\n",
    "        [1, 0],    # Mask for Node 0 (1 for valid neighbor, 0 for padding)\n",
    "        [1, 1],    # Mask for Node 1\n",
    "        [1, 1],    # Mask for Node 2\n",
    "        [1, 0]     # Mask for Node 3 (1 for valid neighbor, 0 for padding)\n",
    "    ]\n",
    "    It tells which are paddings and which are not!\n",
    "\"\"\"\n",
    "#This construction is different from what we usually have as an adjacency matrix\n",
    "print(nei.shape,nei_mask.shape)\n",
    "print(\"This construction is different from what we usually have as an adjacency matrix.\")\n",
    "print(\"Checked it's correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check steps before aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([251, 1703])\n",
      "tensor(1., device='cuda:0')\n",
      "tensor(0.0489, device='cuda:0')\n",
      "torch.Size([251, 64]) torch.Size([251, 122]) torch.Size([251, 122])\n",
      "torch.Size([251, 122, 64])\n",
      "True True\n",
      "Previous x_nei[1]:\n",
      " tensor([[-0.7014,  0.0036, -0.7042,  ...,  0.0049, -0.0241, -0.0178],\n",
      "        [-0.7014,  0.0036, -0.7042,  ...,  0.0049, -0.0241, -0.0178],\n",
      "        [-0.7014,  0.0036, -0.7042,  ...,  0.0049, -0.0241, -0.0178],\n",
      "        ...,\n",
      "        [-0.7014,  0.0036, -0.7042,  ...,  0.0049, -0.0241, -0.0178],\n",
      "        [-0.7014,  0.0036, -0.7042,  ...,  0.0049, -0.0241, -0.0178],\n",
      "        [-0.7014,  0.0036, -0.7042,  ...,  0.0049, -0.0241, -0.0178]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "Previous x_nei[1][2] Euclidean norm tensor(1.0000, device='cuda:0', dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "Translated x_nei[1]:\n",
      " tensor([[7.2325e-01, 1.6436e-16, 6.8446e-01,  ..., 1.7814e-03, 5.9243e-03,\n",
      "         3.1975e-02],\n",
      "        [7.2325e-01, 1.6436e-16, 6.8446e-01,  ..., 1.7814e-03, 5.9243e-03,\n",
      "         3.1975e-02],\n",
      "        [7.2325e-01, 1.6436e-16, 6.8446e-01,  ..., 1.7814e-03, 5.9243e-03,\n",
      "         3.1975e-02],\n",
      "        ...,\n",
      "        [7.2325e-01, 1.6436e-16, 6.8446e-01,  ..., 1.7814e-03, 5.9243e-03,\n",
      "         3.1975e-02],\n",
      "        [7.2325e-01, 1.6436e-16, 6.8446e-01,  ..., 1.7814e-03, 5.9243e-03,\n",
      "         3.1975e-02],\n",
      "        [7.2325e-01, 1.6436e-16, 6.8446e-01,  ..., 1.7814e-03, 5.9243e-03,\n",
      "         3.1975e-02]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Translated x_nei[1][2] Euclidean norm tensor(1.0000, device='cuda:0', dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "Since we are moving neighborhood to the origin kernel, each kernels (k,d') should be identical\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#import func_outof_class as foc\n",
    "import layers.B_layers as B_layers\n",
    "print(data['features'].shape)\n",
    "print(torch.norm(data['features'], p=2, dim=1).max())\n",
    "print(torch.norm(data['features'], p=2, dim=1).min())\n",
    "\n",
    "x = data['features'] #(n,d)\n",
    "x_nei = B_layers.gather(x, nei) #(n,nei_num,d)\n",
    "\n",
    "x=x.to(torch.float64)\n",
    "x_tan = model.manifold.proj_tan0(x, model.c)\n",
    "x_hyp = model.manifold.expmap0(x_tan, c=model.c)\n",
    "x = model.manifold.proj(x_hyp, c=model.c)\n",
    "\n",
    "x=model.encoder.linear_before(x)\n",
    "\n",
    "nei, nei_mask = (nei, nei_mask) #(nei, nei_mask) is adj in the train.py code\n",
    "print(x.shape,nei.shape,nei_mask.shape)\n",
    "input = (x, nei, nei_mask)\n",
    "#model.encoder.layers.forward(input) This is how data flows, but we'll break it done here\n",
    "#model.encoder.layers[0].net #This is the KernelPointAggregation Module\n",
    "x_nei = B_layers.gather(x, nei) #(n,nei_num,d')\n",
    "print(x_nei.shape)\n",
    "\n",
    "#Let's check the correspondence is correct\n",
    "print(torch.equal(x[nei[0][0]],x_nei[0][0]),torch.equal(x[nei[-1][1]],x_nei[-1][1]))\n",
    "#Note: although it seems many rows have numbers, most are paddings that will be deactivated by nei_mask\n",
    "\n",
    "#if transp:\n",
    "print(\"Previous x_nei[1]:\\n\",x_nei[1])\n",
    "print(\"Previous x_nei[1][2] Euclidean norm\",torch.norm(x_nei[1],dim=-1)[2])\n",
    "x, x_nei = model.encoder.layers[0].net.transport_x(x, x_nei)\n",
    "print(\"Translated x_nei[1]:\\n\",x_nei[1])\n",
    "print(\"Translated x_nei[1][2] Euclidean norm\",torch.norm(x_nei[1],dim=-1)[2])\n",
    "\n",
    "\n",
    "n, nei_num, d = x_nei.shape\n",
    "\n",
    "kernels=model.encoder.layers[0].net.get_kernel_pos(x, nei, nei_mask, sample=False, sample_num=16, transp= False)#(n,k,d')\n",
    "print(\"Since we are moving neighborhood to the origin kernel, each kernels (k,d') should be identical\")\n",
    "print(torch.equal(kernels[0],kernels[-1]))\n",
    "\n",
    "#The following function might be incorrect!!!!!\n",
    "\n",
    "#x_nei_kernel_dis=model.encoder.layers[0].net.get_nei_kernel_dis(kernels,x_nei)# (n, k, nei_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "So that we've checked this get_nei_kernel_dis func is correct\n"
     ]
    }
   ],
   "source": [
    "import math as math\n",
    "print(math.isclose(torch.sqrt(model.manifold.sqdist(x_nei[0][0],kernels[0][1],c=model.c)).item(),\\\n",
    "model.encoder.layers[0].net.get_nei_kernel_dis(kernels,x_nei)[0][1][0].item(),abs_tol=1e-5))\n",
    "#Note we use math.isclose because they are numerically a bit different\n",
    "print(\"So that we've checked this get_nei_kernel_dis func is correct\")\n",
    "x_nei_kernel_dis=model.encoder.layers[0].net.get_nei_kernel_dis(kernels,x_nei)# (n, k, nei_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, we check whether nei_mask helps to filter out padding element\n",
      "\n",
      "torch.Size([251, 122])\n",
      "torch.Size([251, 6, 122])\n",
      "Since padding element are not actually connected to center node, we don't need them in outer aggregation,\n",
      "       so they are also not needed for inner aggregation, we use nei_mask to get rid of them.\n",
      "       Making use of element wise product of nei_mask.\n",
      "       I think this step is correct, let's check:\n",
      "Previous x_nei_kernel_dis(n,k,nei_num)[0][1]: \n",
      " tensor([12.2061, 12.2061, 12.2061, 12.2061, 12.2061,  0.6600,  0.6600,  0.6600,\n",
      "         0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,\n",
      "         0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,\n",
      "         0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,\n",
      "         0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,\n",
      "         0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,\n",
      "         0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,\n",
      "         0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,\n",
      "         0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,\n",
      "         0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,\n",
      "         0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,\n",
      "         0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,\n",
      "         0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,\n",
      "         0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,\n",
      "         0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,  0.6600,\n",
      "         0.6600,  0.6600], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward0>)\n",
      "After mask, x_nei_kernel_dis(n,k,nei_num)[0][1]: \n",
      " tensor([12.2061, 12.2061, 12.2061, 12.2061, 12.2061,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Now, we check whether nei_mask helps to filter out padding element\\n\")\n",
    "print(nei_mask.shape)\n",
    "nei_mask = nei_mask.repeat(1,1,model.encoder.layers[0].net.K).view(n,model.encoder.layers[0].net.K,nei_num)#(n, k, nei_num)\n",
    "print(nei_mask.shape)\n",
    "print(\"Since padding element are not actually connected to center node, we don't need them in outer aggregation,\\n\\\n",
    "       so they are also not needed for inner aggregation, we use nei_mask to get rid of them.\\n\\\n",
    "       Making use of element wise product of nei_mask.\\n\\\n",
    "       I think this step is correct, let's check:\")\n",
    "\n",
    "print(\"Previous x_nei_kernel_dis(n,k,nei_num)[0][1]: \\n\",x_nei_kernel_dis[0][1])\n",
    "x_nei_kernel_dis = x_nei_kernel_dis * nei_mask\n",
    "print(\"After mask, x_nei_kernel_dis(n,k,nei_num)[0][1]: \\n\",x_nei_kernel_dis[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, we check whether apply_kernel_transform is correct\n",
      "\n",
      "torch.Size([251, 122, 64])\n",
      "torch.Size([251, 6, 122, 32])\n",
      "Seems to be correct\n"
     ]
    }
   ],
   "source": [
    "print(\"Now, we check whether apply_kernel_transform is correct\\n\")\n",
    "print(x_nei.shape)\n",
    "x_nei_transform = model.encoder.layers[0].net.apply_kernel_transform(x_nei)\n",
    "print(x_nei_transform.shape)\n",
    "print(\"Seems to be correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check steps for aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperboloid Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Hyperboloid Centroid for Aggregation\n",
      "torch.Size([122, 32])\n",
      "torch.Size([122, 33])\n",
      "torch.Size([251, 6, 122, 32])\n",
      "torch.Size([251, 6, 122, 33])\n",
      "Dimensions seems to be correct, we need to further check whether things are on hyperboloid!\n",
      "Whether mapped points are on the hyperboloid:  True \n",
      "\n",
      "We don't use equal, because after testing, it seems they're not absolutely equal,\n",
      "so I think such projection is neccesary to constrain points on the manifold\n",
      "torch.Size([251, 6, 122, 33]) torch.Size([251, 6, 122])\n",
      "After inner aggregation,hyperboloid_x_nei_transform:  torch.Size([251, 122, 33])\n",
      "After outer aggregation,hyperboloid_x_final:  torch.Size([251, 33])\n"
     ]
    }
   ],
   "source": [
    "print(\"Using Hyperboloid Centroid for Aggregation\")\n",
    "#model.encoder.layers[0].net.\n",
    "print(x_nei_transform[0][0].shape)\n",
    "print(model.manifold.poincare_to_hyperboloid(x_nei_transform[0][0],c=model.c).shape)\n",
    "\n",
    "print(x_nei_transform.shape)\n",
    "print(model.manifold.poincare_to_hyperboloid(x_nei_transform,c=model.c).shape)\n",
    "print(\"Dimensions seems to be correct, we need to further check whether things are on hyperboloid!\")\n",
    "\n",
    "hyperboloid_x_nei_transform = model.manifold.poincare_to_hyperboloid(x_nei_transform,c=model.c)#(n,K,nei_num,d')\n",
    "a=torch.allclose(\n",
    "    model.manifold.hyperboloid_proj(hyperboloid_x_nei_transform, model.c),\n",
    "    hyperboloid_x_nei_transform,\n",
    "    rtol=1e-5,  # 相对容差\n",
    "    atol=1e-5   # 绝对容差\n",
    ")\n",
    "print(\"Whether mapped points are on the hyperboloid: \",a,\"\\n\")\n",
    "print(\"We don't use equal, because after testing, it seems they're not absolutely equal,\\n\\\n",
    "so I think such projection is neccesary to constrain points on the manifold\")\n",
    "\n",
    "hyperboloid_x_nei_transform = model.manifold.hyperboloid_proj(hyperboloid_x_nei_transform, model.c)\n",
    "print(hyperboloid_x_nei_transform.shape,x_nei_kernel_dis.shape)\n",
    "hyperboloid_x_nei_transform=model.encoder.layers[0].net.avg_kernel(hyperboloid_x_nei_transform,x_nei_kernel_dis,False)#inner_agg#(n,nei_num,d') in hyperboloid\n",
    "print(\"After inner aggregation,hyperboloid_x_nei_transform: \",hyperboloid_x_nei_transform.shape)\n",
    "hyperboloid_x_final=model.manifold.hyperboloid_centroid(hyperboloid_x_nei_transform,model.c)#outer_agg#(n,d')# on hyperboloid\n",
    "print(\"After outer aggregation,hyperboloid_x_final: \",hyperboloid_x_final.shape)\n",
    "x_final=model.manifold.hyperboloid_to_poincare(hyperboloid_x_final,c=model.c)#(n,d')# in Poincare\n",
    "x_final = model.manifold.proj(x_final,c=model.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([251, 122]) \n",
      " tensor([[12.2061, 12.2061, 12.2061,  ..., 12.2061, 12.2061, 12.2061],\n",
      "        [12.2061, 12.2061, 12.2061,  ..., 12.2061, 12.2061, 12.2061],\n",
      "        [12.2061, 12.2061, 12.2061,  ..., 12.2061, 12.2061, 12.2061],\n",
      "        ...,\n",
      "        [12.2061, 12.2061, 12.2061,  ..., 12.2061, 12.2061, 12.2061],\n",
      "        [12.2061, 12.2061, 12.2061,  ..., 12.2061, 12.2061, 12.2061],\n",
      "        [12.2061, 12.2061, 12.2061,  ..., 12.2061, 12.2061, 12.2061]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<SqrtBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0082, 0.0082, 0.0082,  ..., 0.0082, 0.0082, 0.0082],\n",
       "        [0.0082, 0.0082, 0.0082,  ..., 0.0082, 0.0082, 0.0082],\n",
       "        [0.0082, 0.0082, 0.0082,  ..., 0.0082, 0.0082, 0.0082],\n",
       "        ...,\n",
       "        [0.0082, 0.0082, 0.0082,  ..., 0.0082, 0.0082, 0.0082],\n",
       "        [0.0082, 0.0082, 0.0082,  ..., 0.0082, 0.0082, 0.0082],\n",
       "        [0.0082, 0.0082, 0.0082,  ..., 0.0082, 0.0082, 0.0082]],\n",
       "       device='cuda:0', dtype=torch.float64, grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "w=torch.sqrt(model.manifold.sqdist(torch.zeros_like(hyperboloid_x_nei_transform),hyperboloid_x_nei_transform,c=model.c))\n",
    "print(w.shape,'\\n',w)\n",
    "F.softmax(w, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4810, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(x_final,dim=-1).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klein Midpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Klein MidPoint for Aggregation\n",
      "torch.Size([251, 6, 122, 32])\n",
      "torch.Size([251, 6, 122, 32])\n",
      "Dimensions seems to be correct, we need to further check whether things are on klein disk!\n",
      "Whether mapped points are on the klein disk:  True \n",
      "\n",
      "We don't use equal, because after testing, it seems they're not absolutely equal,\n",
      "so I think such projection is neccesary to constrain points on the manifold\n",
      "torch.Size([251, 6, 122, 32]) torch.Size([251, 6, 122])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Double but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m klein_x_nei_transform \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmanifold\u001b[38;5;241m.\u001b[39mklein_proj(klein_x_nei_transform, model\u001b[38;5;241m.\u001b[39mc)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(klein_x_nei_transform\u001b[38;5;241m.\u001b[39mshape,x_nei_kernel_dis\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 31\u001b[0m klein_x_nei_transform\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mklein_x_nei_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_nei_kernel_dis\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/lige/HKN/layers/B_layers.py:514\u001b[0m, in \u001b[0;36mKernelPointAggregation.avg_kernel\u001b[0;34m(self, x_nei_transform, x_nei_kernel_dis, AggKlein)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanifold\u001b[38;5;241m.\u001b[39mklein_midpoint(x_nei_transform, x_nei_kernel_dis) \u001b[38;5;66;03m#(n, nei_num, d')\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanifold\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyperboloid_centroid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_nei_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_nei_kernel_dis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/lige/HKN/manifolds/poincare.py:255\u001b[0m, in \u001b[0;36mPoincareBall.hyperboloid_centroid\u001b[0;34m(self, x, c, w)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhyperboloid_centroid\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, c, w\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 255\u001b[0m         ave \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbnkd,bnk->bnd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m         ave \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/data/lige/miniconda3/envs/HKNet/lib/python3.10/site-packages/torch/functional.py:360\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# recurse incase operands contains value that has torch function\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# in the original implementation this line is omitted\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m--> 360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Double but found Float"
     ]
    }
   ],
   "source": [
    "print(\"Using Klein MidPoint for Aggregation\")\n",
    "#model.encoder.layers[0].net.\n",
    "print(x_nei_transform.shape)\n",
    "print(model.manifold.poincare_to_klein(x_nei_transform,c=model.c).shape)\n",
    "print(\"Dimensions seems to be correct, we need to further check whether things are on klein disk!\")\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "x_nei_transform = 0.08*(torch.randn(x_nei_transform.shape[0],x_nei_transform.shape[1],\n",
    "                                x_nei_transform.shape[2],x_nei_transform.shape[3])).to(model.c.device)\n",
    "x_nei_transform = model.manifold.proj(x_nei_transform, model.c)\n",
    "\n",
    "klein_x_nei_transform = model.manifold.poincare_to_klein(x_nei_transform,c=model.c)#(n,K,nei_num,d')\n",
    "#torch.norm(klein_x_nei_transform,dim=-1)#.shape\n",
    "a=torch.allclose(\n",
    "    model.manifold.klein_proj(klein_x_nei_transform, model.c),\n",
    "    klein_x_nei_transform,\n",
    "    rtol=1e-5,  # 相对容差\n",
    "    atol=1e-5   # 绝对容差\n",
    ")\n",
    "print(\"Whether mapped points are on the klein disk: \",a,\"\\n\")\n",
    "print(\"We don't use equal, because after testing, it seems they're not absolutely equal,\\n\\\n",
    "so I think such projection is neccesary to constrain points on the manifold\")\n",
    "\n",
    "klein_x_nei_transform = model.manifold.klein_proj(klein_x_nei_transform, model.c)\n",
    "print(klein_x_nei_transform.shape,x_nei_kernel_dis.shape)\n",
    "\n",
    "#klein_x_nei_transform=model.encoder.layers[0].net.avg_kernel(klein_x_nei_transform,x_nei_kernel_dis,False)#inner_agg#(n,nei_num,d') in hyperboloid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperboloid_x_nei_transform = model.manifold.hyperboloid_proj(hyperboloid_x_nei_transform, model.c)\n",
    "print(hyperboloid_x_nei_transform.shape,x_nei_kernel_dis.shape)\n",
    "hyperboloid_x_nei_transform=model.encoder.layers[0].net.avg_kernel(hyperboloid_x_nei_transform,x_nei_kernel_dis,False)#inner_agg#(n,nei_num,d') in hyperboloid\n",
    "print(\"After inner aggregation,hyperboloid_x_nei_transform: \",hyperboloid_x_nei_transform.shape)\n",
    "hyperboloid_x_final=model.manifold.hyperboloid_centroid(hyperboloid_x_nei_transform,model.c)#outer_agg#(n,d')# on hyperboloid\n",
    "print(\"After outer aggregation,hyperboloid_x_final: \",hyperboloid_x_final.shape)\n",
    "x_final=model.manifold.hyperboloid_to_poincare(hyperboloid_x_final,c=model.c)#(n,d')# in Poincare\n",
    "x_final = model.manifold.proj(x_final,c=model.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Hyperboloid Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n",
      "Using seed 8.\n",
      "Dataset: wisconsin\n",
      "Num classes: 5\n",
      "dict_keys(['adj_train', 'features', 'labels', 'idx_train', 'idx_val', 'idx_test', 'adj_train_norm'])\n",
      "(251, 251)\n",
      "torch.Size([251, 1703])\n",
      "False\n",
      "NCModel(\n",
      "  (encoder): BKNet(\n",
      "    (linear_before): BLinear(\n",
      "      in_features=1703, out_features=32, c=tensor([1.], device='cuda:0'), use_bias=1, act=<function relu at 0x7fdfb8ae3370>, dropout_rate=0.5\n",
      "      (E_linear): Linear(in_features=1703, out_features=32, bias=False)\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (layers): Sequential(\n",
      "      (0): KPGraphConvolution(\n",
      "        (net): KernelPointAggregation(\n",
      "          (linears): ModuleList(\n",
      "            (0): BLinear(\n",
      "              in_features=32, out_features=16, c=tensor([1.], device='cuda:0'), use_bias=1, act=<function relu at 0x7fdfb8ae3370>, dropout_rate=0.5\n",
      "              (E_linear): Linear(in_features=32, out_features=16, bias=False)\n",
      "              (dropout): Dropout(p=0.5, inplace=False)\n",
      "            )\n",
      "            (1): BLinear(\n",
      "              in_features=32, out_features=16, c=tensor([1.], device='cuda:0'), use_bias=1, act=<function relu at 0x7fdfb8ae3370>, dropout_rate=0.5\n",
      "              (E_linear): Linear(in_features=32, out_features=16, bias=False)\n",
      "              (dropout): Dropout(p=0.5, inplace=False)\n",
      "            )\n",
      "            (2): BLinear(\n",
      "              in_features=32, out_features=16, c=tensor([1.], device='cuda:0'), use_bias=1, act=<function relu at 0x7fdfb8ae3370>, dropout_rate=0.5\n",
      "              (E_linear): Linear(in_features=32, out_features=16, bias=False)\n",
      "              (dropout): Dropout(p=0.5, inplace=False)\n",
      "            )\n",
      "            (3): BLinear(\n",
      "              in_features=32, out_features=16, c=tensor([1.], device='cuda:0'), use_bias=1, act=<function relu at 0x7fdfb8ae3370>, dropout_rate=0.5\n",
      "              (E_linear): Linear(in_features=32, out_features=16, bias=False)\n",
      "              (dropout): Dropout(p=0.5, inplace=False)\n",
      "            )\n",
      "            (4): BLinear(\n",
      "              in_features=32, out_features=16, c=tensor([1.], device='cuda:0'), use_bias=1, act=<function relu at 0x7fdfb8ae3370>, dropout_rate=0.5\n",
      "              (E_linear): Linear(in_features=32, out_features=16, bias=False)\n",
      "              (dropout): Dropout(p=0.5, inplace=False)\n",
      "            )\n",
      "            (5): BLinear(\n",
      "              in_features=32, out_features=16, c=tensor([1.], device='cuda:0'), use_bias=1, act=<function relu at 0x7fdfb8ae3370>, dropout_rate=0.5\n",
      "              (E_linear): Linear(in_features=32, out_features=16, bias=False)\n",
      "              (dropout): Dropout(p=0.5, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): PoincareDecoder()\n",
      ")\n",
      "Total number of parameters: 57989\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from types import SimpleNamespace\n",
    "import sys\n",
    "sys.path.append('/data/lige/HKN')# Please change accordingly!\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from geoopt import ManifoldParameter as geoopt_ManifoldParameter\n",
    "from manifolds.base import ManifoldParameter as base_ManifoldParameter\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "from optim import RiemannianAdam, RiemannianSGD\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from config import parser\n",
    "from models.base_models import NCModel, LPModel, GCModel\n",
    "from utils.data_utils import load_data, get_nei, GCDataset, split_batch\n",
    "from utils.train_utils import get_dir_name, format_metrics\n",
    "from utils.eval_utils import acc_f1\n",
    "\n",
    "from geoopt import ManifoldParameter as geoopt_ManifoldParameter\n",
    "from manifolds.base import ManifoldParameter as base_ManifoldParameter\n",
    "\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#import torch.nn.functional as F\n",
    "\n",
    "\n",
    "config_args = {\n",
    "    'training_config': {\n",
    "        'lr': (1e-3, 'learning rate'),\n",
    "        'dropout': (0.5, 'dropout probability'),\n",
    "        'cuda': (0, 'which cuda device to use (-1 for cpu training)'),\n",
    "        'epochs': (5000, 'maximum number of epochs to train for'),\n",
    "        'weight_decay': (1e-3, 'l2 regularization strength'),\n",
    "        'optimizer': ('radam', 'which optimizer to use, can be any of [rsgd, radam]'),\n",
    "        'momentum': (0.999, 'momentum in optimizer'),\n",
    "        'patience': (250, 'patience for early stopping'),\n",
    "        'seed': (8, 'seed for training'),\n",
    "        'log_freq': (1, 'how often to compute print train/val metrics (in epochs)'),\n",
    "        'eval_freq': (1, 'how often to compute val metrics (in epochs)'),\n",
    "        'save': (0, '1 to save model and logs and 0 otherwise'),\n",
    "        'save_dir': (None, 'path to save training logs and model weights (defaults to logs/task/date/run/)'),\n",
    "        'sweep_c': (0, ''),\n",
    "        'lr_reduce_freq': (None, 'reduce lr every lr-reduce-freq or None to keep lr constant'),\n",
    "        'gamma': (0.5, 'gamma for lr scheduler'),\n",
    "        'print_epoch': (True, ''),\n",
    "        'grad_clip': (None, 'max norm for gradient clipping, or None for no gradient clipping'),\n",
    "        'min_epochs': (2500, 'do not early stop before min-epochs')\n",
    "    },\n",
    "    'model_config': {\n",
    "        'use_geoopt': (False, \"which manifold class to use, if false then use basd.manifold\"),\n",
    "        'AggKlein':(False, \"if false, then use hyperboloid centorid for aggregation\"),\n",
    "        'corr': (0,'0: d(x_i ominus x, x_k), 1: d(x_ik,x_k)'),\n",
    "        'task': ('nc', 'which tasks to train on, can be any of [lp, nc]'),\n",
    "        'model': ('BKNet', 'which encoder to use, can be any of [Shallow, MLP, HNN, GCN, GAT, HyperGCN, HyboNet,BKNet,BMLP]'),\n",
    "        #'dim': (64, 'embedding dimension'),\n",
    "        'dim': (16, 'embedding dimension'),\n",
    "        'manifold': ('PoincareBall', 'which manifold to use, can be any of [Euclidean, Hyperboloid, PoincareBall, Lorentz]'),\n",
    "        'c': (1.0, 'hyperbolic radius, set to None for trainable curvature'),\n",
    "        'r': (2., 'fermi-dirac decoder parameter for lp'),\n",
    "        't': (1., 'fermi-dirac decoder parameter for lp'),\n",
    "        'margin': (2., 'margin of MarginLoss'),\n",
    "        'pretrained_embeddings': (None, 'path to pretrained embeddings (.npy file) for Shallow node classification'),\n",
    "        'pos_weight': (0, 'whether to upweight positive class in node classification tasks'),\n",
    "        'num_layers': (2, 'number of hidden layers in encoder'),\n",
    "        'bias': (1, 'whether to use bias (1) or not (0)'),\n",
    "        'act': ('relu', 'which activation function to use (or None for no activation)'),\n",
    "        'n_heads': (4, 'number of attention heads for graph attention networks, must be a divisor dim'),\n",
    "        'alpha': (0.2, 'alpha for leakyrelu in graph attention networks'),\n",
    "        'double_precision': ('1', 'whether to use double precision'),\n",
    "        'use_att': (0, 'whether to use hyperbolic attention or not'),\n",
    "        'local_agg': (0, 'whether to local tangent space aggregation or not'),\n",
    "        'kernel_size': (6, 'number of kernels'),\n",
    "        'KP_extent': (0.75, 'influence radius of each kernel point'),\n",
    "        'radius': (1, 'radius used for kernel point init'),\n",
    "        'deformable': (False, 'deformable kernel'),\n",
    "        #'linear_before': (64, 'dim of linear before gcn')#64\n",
    "        'linear_before': (32, 'dim of linear before gcn')#64\n",
    "    },\n",
    "    'data_config': {\n",
    "        'dataset': ('wisconsin', 'which dataset to use(cornell,wisconsin,squirrel,cora)'),\n",
    "        'batch_size': (32, 'batch size for gc'),\n",
    "        'val_prop': (0.05, 'proportion of validation edges for link prediction'),\n",
    "        'test_prop': (0.1, 'proportion of test edges for link prediction'),\n",
    "        'use_feats': (1, 'whether to use node features or not'),\n",
    "        'normalize_feats': (1, 'whether to normalize input node features'),\n",
    "        'normalize_adj': (1, 'whether to row-normalize the adjacency matrix'),\n",
    "        'split_seed': (8, 'seed for data splits (train/test/val)'),\n",
    "        'split_graph': (False, 'whether to split the graph')\n",
    "    }\n",
    "}\n",
    "\n",
    "# 将所有参数转换为 SimpleNamespace\n",
    "args = SimpleNamespace(\n",
    "    **{k: v[0] for config in config_args.values() for k, v in config.items()}\n",
    ")\n",
    "\n",
    "#choose which manifold class to follow \n",
    "if args.use_geoopt == False:\n",
    "    ManifoldParameter = base_ManifoldParameter\n",
    "else:\n",
    "    ManifoldParameter = geoopt_ManifoldParameter\n",
    "np.random.seed(args.seed)#args.seed\n",
    "torch.manual_seed(args.seed)#args.seed\n",
    "if int(args.cuda):#args.double_precision\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "if int(args.cuda) >= 0:#args.cuda\n",
    "    torch.cuda.manual_seed(args.seed)#args.seed\n",
    "args.device = 'cuda:' + str(args.cuda) if int(args.cuda) >= 0 else 'cpu' #args.device actually,<-args.cuda\n",
    "args.patience = args.epochs if not args.patience else args.patience #args.patience<-args.epochs|args.patience\n",
    "\n",
    "print(f'Using: {args.device}')\n",
    "print(\"Using seed {}.\".format(args.seed))\n",
    "print(f\"Dataset: {args.dataset}\")\n",
    "\n",
    "# Load data\n",
    "data = load_data(args, os.path.join('data', args.dataset))\n",
    "if args.task == 'gc':\n",
    "    args.n_nodes, args.feat_dim = data['features'][0].shape\n",
    "else:\n",
    "    args.n_nodes, args.feat_dim = data['features'].shape\n",
    "if args.task == 'nc':\n",
    "    Model = NCModel\n",
    "    args.n_classes = int(data['labels'].max() + 1)\n",
    "    args.data = data\n",
    "    print(f'Num classes: {args.n_classes}')\n",
    "elif args.task == 'gc':\n",
    "    Model = GCModel\n",
    "    args.n_classes = int(data['labels'].max() + 1)\n",
    "    print(f'Num classes: {args.n_classes}')\n",
    "else:\n",
    "    args.nb_false_edges = len(data['train_edges_false'])\n",
    "    args.nb_edges = len(data['train_edges'])\n",
    "    if args.task == 'lp':\n",
    "        Model = LPModel\n",
    "        args.n_classes = 2\n",
    "\n",
    "if not args.lr_reduce_freq:\n",
    "    args.lr_reduce_freq = args.epochs\n",
    "\n",
    "\n",
    "###A simple check on data\n",
    "print(data.keys())\n",
    "print(data['adj_train'].todense().shape)\n",
    "print(data['features'].shape)\n",
    "###A simple check on data\n",
    "\n",
    "# Model and optimizer\n",
    "model = Model(args)\n",
    "print(str(model))\n",
    "no_decay = ['bias', 'scale']\n",
    "optimizer_grouped_parameters = [{\n",
    "    'params': [\n",
    "        p for n, p in model.named_parameters()\n",
    "        if p.requires_grad and not any(\n",
    "            nd in n\n",
    "            for nd in no_decay) and not isinstance(p, ManifoldParameter)\n",
    "    ],\n",
    "    'weight_decay':\n",
    "    args.weight_decay\n",
    "}, {\n",
    "    'params': [\n",
    "        p for n, p in model.named_parameters() if p.requires_grad and any(\n",
    "            nd in n\n",
    "            for nd in no_decay) or isinstance(p, ManifoldParameter)\n",
    "    ],\n",
    "    'weight_decay':\n",
    "    0.0\n",
    "}]\n",
    "if args.optimizer == 'radam':\n",
    "    optimizer = RiemannianAdam(params=optimizer_grouped_parameters,\n",
    "                                lr=args.lr,\n",
    "                                stabilize=10)\n",
    "elif args.optimizer == 'rsgd':\n",
    "    optimizer = RiemannianSGD(params=optimizer_grouped_parameters,\n",
    "                                lr=args.lr,\n",
    "                                stabilize=10)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=int(\n",
    "                                                args.lr_reduce_freq),\n",
    "                                                gamma=float(args.gamma))\n",
    "tot_params = sum([np.prod(p.size()) for p in model.parameters()])\n",
    "model = model.to(args.device)\n",
    "for x, val in data.items():\n",
    "    if torch.is_tensor(data[x]):\n",
    "        data[x] = data[x].to(args.device)\n",
    "print(f\"Total number of parameters: {tot_params}\")\n",
    "\n",
    "# Train model for nc:\n",
    "t_total = time.time()\n",
    "counter = 0\n",
    "best_val_metrics = model.init_metric_dict()\n",
    "best_test_metrics = None\n",
    "best_emb = None\n",
    "if args.n_classes > 2:\n",
    "    f1_average = 'micro'\n",
    "else:\n",
    "    f1_average = 'binary'\n",
    "\n",
    "if args.model == 'HKPNet':\n",
    "    nei, nei_mask = get_nei(data['adj_train'])\n",
    "    nei = nei.to(args.device)\n",
    "    nei_mask = nei_mask.to(args.device)\n",
    "elif args.model == 'BKNet':\n",
    "    nei, nei_mask = get_nei(data['adj_train'])\n",
    "    nei = nei.to(args.device)\n",
    "    nei_mask = nei_mask.to(args.device) #nei/nei_mask on cuda now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.linear_before.bias tensor([-0.0175, -0.0021,  0.0009, -0.0206,  0.0214,  0.0212, -0.0153, -0.0087,\n",
      "        -0.0109, -0.0013,  0.0230,  0.0185, -0.0172,  0.0237, -0.0191,  0.0045,\n",
      "         0.0109, -0.0075, -0.0007,  0.0086, -0.0053,  0.0080,  0.0082,  0.0031,\n",
      "        -0.0128,  0.0154, -0.0181,  0.0142,  0.0154, -0.0107,  0.0002, -0.0087],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "encoder.linear_before.E_linear.weight tensor([[ 0.0467, -0.0755, -0.0434,  ...,  0.0245,  0.0135, -0.0054],\n",
      "        [ 0.0182,  0.0564,  0.0509,  ...,  0.0562,  0.0532,  0.0816],\n",
      "        [-0.0103, -0.0821,  0.0264,  ...,  0.0627,  0.0687,  0.0562],\n",
      "        ...,\n",
      "        [ 0.0287,  0.0041, -0.0265,  ..., -0.0307, -0.0643, -0.0196],\n",
      "        [-0.0307, -0.0680, -0.0169,  ...,  0.0403, -0.0816, -0.0240],\n",
      "        [-0.0406,  0.0603, -0.0753,  ..., -0.0194,  0.0107,  0.0656]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "encoder.layers.0.net.linears.0.bias tensor([-0.1205,  0.1094, -0.0615, -0.1218,  0.0937, -0.1294, -0.0656, -0.1761,\n",
      "         0.0527, -0.0290, -0.0648,  0.0576, -0.1760, -0.0794, -0.1518,  0.1657],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "encoder.layers.0.net.linears.0.E_linear.weight tensor([[-1.1359e-01, -2.7631e-02,  8.9069e-02, -4.8811e-01,  4.7593e-02,\n",
      "         -3.0171e-01,  2.6707e-01,  1.0809e-02, -2.3038e-01, -1.3208e-01,\n",
      "          5.6166e-02, -4.1986e-01,  5.2644e-02, -4.6711e-01, -3.9974e-01,\n",
      "          1.6333e-01, -5.9809e-02, -1.2017e-01,  3.1240e-01, -2.3592e-01,\n",
      "         -9.1986e-02, -1.1410e-01, -2.2093e-01, -4.0425e-01,  2.6113e-02,\n",
      "          7.9043e-02,  1.5949e-01,  3.8452e-01,  1.6288e-01,  3.9019e-01,\n",
      "         -1.2252e-01, -4.3138e-01],\n",
      "        [-1.1120e-01,  1.0594e-01, -3.2963e-01,  4.9096e-01,  4.2831e-02,\n",
      "         -1.8028e-03, -2.2078e-01,  4.2077e-01, -4.6517e-01,  3.4818e-01,\n",
      "          2.3411e-04, -3.4572e-01,  4.5079e-01,  4.1697e-01,  2.7888e-01,\n",
      "          1.1642e-01,  4.0201e-01,  4.8788e-01, -3.7811e-01,  4.7041e-01,\n",
      "         -7.4513e-02, -1.6725e-03, -2.5091e-01, -3.2815e-01,  2.9706e-02,\n",
      "          2.9826e-01,  3.1859e-01,  3.2429e-02,  2.6029e-01, -2.8477e-02,\n",
      "          1.0848e-01,  3.2280e-01],\n",
      "        [-4.2459e-01,  2.9794e-01, -2.5931e-01, -4.4434e-03, -2.8033e-01,\n",
      "          1.8697e-01,  4.1446e-01,  4.3551e-01,  4.6272e-01, -4.4517e-01,\n",
      "         -2.0198e-02, -4.5125e-01,  4.7760e-01,  4.2571e-01, -3.4504e-01,\n",
      "          3.3887e-02,  2.3789e-01,  2.8878e-02,  5.4431e-02,  4.8370e-01,\n",
      "          2.3526e-02,  1.3220e-01, -3.5947e-01,  2.7897e-01, -1.8098e-02,\n",
      "          4.9091e-01,  3.5412e-01, -9.5770e-02, -2.4529e-01,  6.2377e-02,\n",
      "         -1.9745e-01, -2.7170e-01],\n",
      "        [ 4.3187e-01, -1.1517e-01, -3.0663e-01,  3.7301e-01, -4.5985e-02,\n",
      "         -3.5764e-01, -2.0779e-02, -1.2734e-01,  3.8543e-01,  3.6452e-01,\n",
      "          4.1606e-01, -2.0438e-01,  2.0256e-01,  7.5306e-02,  2.1689e-01,\n",
      "          9.3412e-02, -1.5338e-01,  1.5079e-01, -3.1321e-01, -4.1845e-01,\n",
      "          9.8344e-02, -4.8149e-01,  2.4281e-01, -9.1919e-02,  9.6570e-02,\n",
      "         -3.9209e-01, -3.8512e-01,  9.1866e-02, -4.9217e-01, -9.3497e-02,\n",
      "         -5.4137e-02, -1.3841e-01],\n",
      "        [-3.0601e-01,  2.9885e-01, -4.7868e-01, -1.9405e-01, -7.2047e-02,\n",
      "         -4.3578e-01, -5.1308e-02, -4.3807e-01, -3.7806e-01,  4.0444e-01,\n",
      "         -3.7326e-01,  3.9347e-01, -3.8760e-01, -3.2366e-01, -1.3855e-01,\n",
      "          1.5741e-01, -4.4979e-01, -4.5247e-01, -8.7500e-02, -2.5108e-02,\n",
      "          3.0502e-01, -4.2561e-01,  4.1319e-01, -3.7406e-01, -6.3360e-02,\n",
      "          2.3816e-02,  4.1580e-01, -3.5789e-01,  2.2579e-01,  3.1709e-01,\n",
      "          3.8280e-01, -1.2430e-01],\n",
      "        [ 1.9748e-01, -2.5678e-01, -1.3035e-01, -3.9072e-01,  2.6087e-01,\n",
      "          4.7676e-01,  2.2347e-01,  5.5657e-03,  1.8097e-01,  6.7422e-02,\n",
      "          7.0539e-02,  2.5273e-01, -2.5590e-01, -6.2087e-02, -1.5801e-01,\n",
      "         -2.8919e-01, -3.0474e-01, -7.3634e-02, -3.9067e-01,  2.6081e-02,\n",
      "         -4.5733e-01,  4.9508e-01, -8.4393e-03, -1.2221e-01,  1.4897e-01,\n",
      "         -7.3079e-02,  1.2457e-01, -4.7516e-01,  4.6696e-01,  4.3348e-01,\n",
      "          1.2217e-01, -5.8601e-03],\n",
      "        [ 9.8748e-02,  2.4094e-01, -4.3779e-01,  1.3715e-01,  7.6679e-02,\n",
      "          1.6676e-01,  4.2583e-01,  2.0863e-01, -3.0084e-01, -1.6178e-01,\n",
      "          4.1161e-01, -2.6246e-01,  1.2549e-01, -2.5851e-01, -4.5980e-01,\n",
      "          2.5765e-01,  1.3436e-01,  2.2343e-01,  3.2428e-01, -1.1631e-01,\n",
      "          4.4175e-01, -1.8396e-01, -3.4855e-01, -2.5259e-01, -1.4669e-01,\n",
      "         -4.3578e-02,  3.4212e-01,  8.5985e-02,  3.2767e-01, -3.7579e-01,\n",
      "         -7.4741e-02, -3.2116e-01],\n",
      "        [-2.3821e-02,  9.8113e-02, -1.2196e-01,  1.0738e-01,  6.5058e-02,\n",
      "          1.8274e-01,  1.4636e-02, -1.3601e-01,  3.9363e-01,  4.2648e-01,\n",
      "         -4.6709e-01,  1.3505e-01,  3.2044e-01,  3.2496e-01,  3.8025e-01,\n",
      "          3.8255e-01,  4.8074e-01, -2.3346e-01,  2.7350e-01, -1.7143e-01,\n",
      "         -1.2610e-01, -2.2282e-01, -3.5809e-01, -4.5946e-01,  2.5013e-01,\n",
      "         -1.0212e-01,  1.9187e-01, -1.0860e-01, -4.1126e-01,  3.2062e-01,\n",
      "          3.9279e-01, -3.3359e-01],\n",
      "        [-2.2828e-01,  8.7271e-02,  3.4827e-01,  3.2552e-01, -2.2424e-01,\n",
      "         -3.4125e-01, -2.7478e-01,  1.2392e-01, -4.3987e-01,  1.1531e-01,\n",
      "         -4.0791e-01, -4.2333e-01,  2.2469e-02,  4.2194e-01,  1.6664e-01,\n",
      "          4.6353e-02,  1.6435e-01, -2.7368e-01, -2.8962e-01,  3.1279e-01,\n",
      "          2.3489e-01, -1.2379e-01,  3.5855e-01, -4.9201e-01,  1.1610e-01,\n",
      "         -1.4946e-01,  2.4599e-01,  4.0599e-01, -1.7172e-01,  9.1012e-02,\n",
      "         -3.1419e-01, -2.4718e-01],\n",
      "        [-4.1367e-01, -1.2620e-02, -1.6996e-01, -2.8064e-01,  2.2473e-01,\n",
      "         -3.3929e-01, -3.1552e-01,  4.8300e-01, -2.0451e-02, -3.8350e-02,\n",
      "          2.0430e-02,  7.3413e-02,  1.6602e-01, -6.1441e-02,  2.5833e-01,\n",
      "         -1.0219e-01,  1.1812e-01,  1.3828e-01, -2.3167e-01,  7.0411e-02,\n",
      "         -2.0827e-02, -4.4972e-02,  1.1709e-01,  1.1358e-01,  2.2257e-01,\n",
      "          1.5217e-01, -3.1246e-01, -2.5218e-01,  9.6705e-02,  2.0155e-01,\n",
      "         -4.2073e-01,  1.9811e-01],\n",
      "        [ 2.1204e-01, -1.1630e-01,  2.9589e-01,  1.8580e-01,  3.4918e-01,\n",
      "         -2.0580e-01,  3.1457e-01,  1.9273e-01,  2.7593e-01,  3.9231e-01,\n",
      "         -3.4799e-01,  4.8640e-01, -1.1249e-01,  4.0141e-01,  2.3153e-02,\n",
      "          6.9896e-02, -4.1934e-01,  4.2153e-01,  2.5972e-01,  4.6148e-01,\n",
      "          3.4137e-01, -1.4611e-02,  2.0063e-01, -2.6501e-01,  2.1309e-01,\n",
      "         -1.5605e-01,  3.3933e-01,  4.5763e-01,  4.2997e-01,  7.3695e-02,\n",
      "          4.7598e-01,  3.2901e-01],\n",
      "        [ 8.8708e-03, -1.9510e-01,  3.0699e-01,  4.2719e-01,  4.3423e-01,\n",
      "          4.9481e-01, -4.2210e-01,  2.0302e-01,  1.9951e-01, -2.7643e-01,\n",
      "         -4.4953e-01, -4.2269e-01, -2.8700e-01, -1.7650e-01,  4.8864e-01,\n",
      "          1.8886e-01,  2.8171e-01,  4.3466e-01, -4.0515e-01, -8.8346e-03,\n",
      "          1.1574e-01, -6.4423e-02, -2.0611e-03, -3.0234e-01, -2.1330e-01,\n",
      "          4.0063e-01,  2.0691e-01, -2.0782e-01,  1.5656e-01, -2.8908e-01,\n",
      "         -4.3075e-01, -3.3656e-01],\n",
      "        [-3.8409e-02, -4.2512e-01,  4.1234e-02,  1.6214e-01,  1.7175e-01,\n",
      "          1.7785e-02,  2.8443e-01,  4.6558e-01,  3.4906e-01,  8.8954e-02,\n",
      "          1.6685e-01, -4.7053e-02,  1.4718e-01,  3.2572e-02,  4.9087e-01,\n",
      "         -1.0474e-01, -3.2139e-01, -1.3951e-01,  4.4927e-01,  2.3938e-01,\n",
      "         -1.0707e-01,  2.9885e-01, -3.1428e-01,  3.0267e-01, -3.3577e-01,\n",
      "         -4.9319e-01, -4.7879e-01, -3.3210e-01,  3.0536e-01, -6.4889e-02,\n",
      "         -1.4667e-01,  7.6349e-03],\n",
      "        [-3.7465e-01,  1.4067e-01, -4.1199e-01, -4.9576e-01, -2.6730e-02,\n",
      "         -3.6860e-01, -4.5664e-01,  3.7001e-01, -2.7668e-01, -9.0975e-02,\n",
      "         -4.6651e-01, -2.7967e-01, -3.7682e-01,  1.8714e-01,  4.4892e-01,\n",
      "         -3.3456e-01,  9.5853e-02,  5.0339e-02,  4.4416e-01,  4.9416e-01,\n",
      "         -8.2628e-02, -3.1436e-01,  2.4545e-01,  1.3345e-02,  3.0457e-01,\n",
      "         -2.5382e-01,  9.7998e-02,  1.3129e-01,  1.9315e-01, -9.3887e-02,\n",
      "          3.4530e-01, -1.5596e-01],\n",
      "        [-2.9592e-01,  3.7170e-01,  3.6208e-01,  3.3631e-01, -2.2233e-01,\n",
      "          2.2089e-02, -5.3896e-02,  4.5399e-01, -3.2895e-01, -4.2676e-01,\n",
      "          1.3271e-01,  4.5635e-01, -1.8112e-01, -3.1586e-01,  1.0471e-01,\n",
      "         -3.1336e-01, -4.6585e-01,  4.7543e-01, -1.7840e-01, -1.4372e-01,\n",
      "         -2.4585e-01, -3.4455e-01,  1.1580e-01,  2.0923e-01,  1.7181e-01,\n",
      "          2.3307e-01,  1.4658e-01,  4.7917e-01,  4.6547e-01,  4.6010e-01,\n",
      "         -4.3002e-01,  4.6224e-01],\n",
      "        [ 6.2266e-02, -3.1265e-01, -3.5592e-01,  1.9771e-01, -1.8806e-01,\n",
      "         -3.2843e-01,  3.7373e-01,  2.4943e-01, -2.7379e-03,  4.4466e-01,\n",
      "          3.6409e-03,  3.8834e-01, -4.2165e-01, -3.4687e-02,  1.1873e-01,\n",
      "          4.1004e-01, -3.7634e-01, -4.2352e-01, -4.2519e-01, -4.1940e-01,\n",
      "          4.3065e-01, -3.0867e-01, -2.8877e-01,  4.6075e-01,  3.2843e-01,\n",
      "          7.3964e-02, -4.6128e-01, -4.9335e-02, -1.2119e-01,  1.7806e-01,\n",
      "         -4.9067e-01, -1.1587e-01]], device='cuda:0', dtype=torch.float64)\n",
      "encoder.layers.0.net.linears.1.bias tensor([-1.2452e-04,  4.4509e-02, -1.1041e-01,  1.1418e-01, -1.0526e-01,\n",
      "         1.1088e-01,  1.7341e-01,  4.0004e-02, -1.6451e-01,  7.4568e-02,\n",
      "         1.6612e-03, -1.3389e-01, -1.7024e-01,  8.5500e-02,  8.6617e-02,\n",
      "        -2.0626e-02], device='cuda:0', dtype=torch.float64)\n",
      "encoder.layers.0.net.linears.1.E_linear.weight tensor([[-1.7343e-01,  1.1563e-01, -3.2122e-01, -1.4444e-01,  1.0724e-02,\n",
      "         -4.5954e-01, -4.2057e-03, -3.1216e-01, -3.5519e-01, -1.1119e-01,\n",
      "         -2.4786e-01, -3.0263e-01, -3.1499e-01, -3.7801e-01,  5.3344e-02,\n",
      "         -1.4738e-01,  4.7005e-01,  1.5158e-01,  9.0371e-02, -2.3339e-02,\n",
      "          2.6796e-01,  3.6815e-01, -2.7052e-01, -4.7344e-01, -4.9175e-02,\n",
      "          3.8249e-01,  4.4440e-01, -1.9084e-01, -3.1790e-02,  2.7527e-01,\n",
      "         -2.0943e-01, -3.2185e-01],\n",
      "        [ 2.2620e-01,  4.7070e-01,  8.6237e-02,  3.3097e-01,  3.4469e-01,\n",
      "          4.5379e-01,  8.6842e-02,  1.0015e-01,  2.4880e-01,  1.0178e-01,\n",
      "          1.0299e-01,  3.8227e-02, -2.7989e-01, -9.5175e-02,  1.1692e-01,\n",
      "          4.7138e-01,  3.2617e-02, -4.0012e-01,  3.4689e-02,  2.5173e-01,\n",
      "          1.6838e-01, -4.6612e-01, -7.9598e-02,  2.8244e-01,  2.8160e-01,\n",
      "         -3.5513e-01, -4.7114e-01, -1.5469e-01, -3.9964e-01,  1.2642e-01,\n",
      "         -1.8833e-01, -3.9108e-01],\n",
      "        [ 2.1294e-01,  3.9248e-01, -4.5570e-01,  2.8743e-01,  4.8977e-01,\n",
      "          1.9327e-02, -4.0898e-01, -3.2916e-01,  4.2207e-01, -1.3132e-02,\n",
      "         -1.7091e-01, -4.7886e-01, -4.8176e-01,  4.8299e-01, -2.8811e-01,\n",
      "         -1.4277e-01, -3.6932e-01, -2.0355e-01,  1.9969e-01,  7.0314e-02,\n",
      "          1.9363e-01, -4.6169e-01,  4.6227e-01, -3.2534e-01,  1.3875e-02,\n",
      "         -2.9342e-01,  1.8911e-01,  3.1610e-01, -1.8765e-01,  3.6713e-01,\n",
      "          4.9174e-01,  4.2543e-01],\n",
      "        [ 3.0096e-01,  3.4170e-01, -4.4526e-02,  3.8099e-01, -4.8113e-01,\n",
      "          4.0363e-01,  1.3453e-02,  4.8192e-01,  3.5591e-01, -3.9074e-01,\n",
      "          3.6477e-01,  4.3663e-01,  1.5565e-01,  4.5152e-02, -1.2852e-01,\n",
      "         -1.7748e-01,  4.7278e-01,  3.5238e-01,  2.4479e-01, -1.1717e-01,\n",
      "         -8.7451e-02, -3.4443e-01,  1.7154e-01, -3.5057e-02, -3.1457e-01,\n",
      "          4.6001e-01,  2.5775e-01, -4.4780e-01, -3.4819e-02,  2.7688e-01,\n",
      "         -3.3173e-01, -5.5218e-02],\n",
      "        [ 1.0699e-01, -3.7432e-01, -2.2899e-01,  4.7509e-01,  3.0080e-01,\n",
      "         -4.6873e-01, -1.8234e-01,  2.1721e-01, -2.8946e-01, -3.3867e-01,\n",
      "         -1.3418e-01, -1.6929e-01, -1.6583e-01,  5.8569e-02, -1.2872e-01,\n",
      "          4.3464e-01, -4.5140e-01,  2.0796e-01,  4.8402e-01,  3.8753e-01,\n",
      "         -4.9206e-02, -2.5014e-01,  2.1464e-01, -6.7293e-02,  1.0808e-01,\n",
      "          8.9402e-03, -1.3994e-01,  9.9145e-02,  1.5551e-01, -1.8559e-01,\n",
      "          6.1013e-03,  1.3504e-01],\n",
      "        [ 3.4135e-01, -3.9796e-01, -1.2811e-01,  8.8447e-02, -8.0790e-02,\n",
      "         -3.8706e-01, -2.2924e-01, -2.5981e-01, -2.3031e-01, -1.9779e-02,\n",
      "         -1.7844e-01,  7.0316e-02,  2.5378e-01, -1.7354e-01, -7.0689e-02,\n",
      "         -1.8209e-01, -2.2434e-01,  8.8047e-02, -3.1839e-01, -2.6944e-01,\n",
      "         -1.4090e-01,  4.1320e-01, -4.8823e-01, -9.8278e-02, -3.0570e-01,\n",
      "         -3.4217e-01,  1.8002e-02, -3.5925e-01, -3.5563e-02,  1.2772e-01,\n",
      "         -3.1575e-01,  4.3389e-01],\n",
      "        [-3.3908e-01, -3.9360e-01,  2.4471e-01, -2.6454e-01,  1.9752e-01,\n",
      "         -3.3816e-01,  1.3790e-01, -3.8911e-01, -3.5620e-01, -4.1857e-01,\n",
      "         -4.6461e-01,  1.4919e-01, -1.0134e-01,  5.0840e-02,  4.6084e-01,\n",
      "          1.5392e-01, -1.6301e-01, -2.8273e-01,  3.1718e-01,  3.6369e-01,\n",
      "         -3.2979e-03, -4.3963e-01,  2.9276e-01,  6.6475e-02, -2.2838e-03,\n",
      "         -2.8267e-01,  1.1346e-01, -3.9567e-01,  4.2357e-01, -4.1988e-01,\n",
      "          1.2838e-02,  2.0236e-01],\n",
      "        [-2.6847e-01,  4.1154e-02, -2.5855e-01,  1.6046e-01, -2.3259e-01,\n",
      "         -1.9677e-01, -4.4199e-01,  1.9797e-01,  3.6402e-01,  3.2581e-01,\n",
      "         -3.7681e-01,  3.0325e-02, -1.7168e-01,  2.2112e-01, -1.5260e-01,\n",
      "          1.5227e-01, -2.3548e-01,  7.4983e-02,  2.6717e-01,  2.9364e-01,\n",
      "          2.8423e-01, -2.6461e-01,  4.6142e-01,  1.5720e-01, -3.1138e-01,\n",
      "         -9.4513e-02, -4.0453e-02, -7.6520e-02, -4.1808e-01, -2.1931e-01,\n",
      "          7.9005e-02, -3.4570e-01],\n",
      "        [-1.9455e-01,  1.7271e-01,  9.7438e-03, -1.6150e-01, -2.4885e-01,\n",
      "         -1.7069e-02,  4.6005e-01, -2.1000e-01, -2.4430e-01,  4.8597e-01,\n",
      "          2.6887e-01, -1.5708e-01,  2.8276e-01, -2.3780e-01,  1.5567e-01,\n",
      "         -8.3560e-02, -3.1403e-01,  4.4001e-01,  4.3404e-01,  4.3188e-01,\n",
      "          4.6233e-01,  1.5489e-01,  2.9781e-01,  2.0760e-01,  4.2966e-01,\n",
      "          2.8991e-01,  1.7203e-01,  4.7408e-01,  2.5838e-01, -9.5090e-02,\n",
      "          3.2482e-01,  3.9407e-01],\n",
      "        [-3.3041e-01, -2.9399e-01, -1.0786e-01, -2.0488e-01,  1.5140e-01,\n",
      "          3.3843e-01,  3.9508e-01,  4.2020e-01, -3.5415e-01, -2.9173e-01,\n",
      "         -6.5263e-02,  4.4496e-01,  4.8872e-01, -2.2885e-01,  3.6751e-01,\n",
      "         -1.4837e-01, -1.9841e-01,  1.1488e-01,  2.2685e-01,  3.3262e-01,\n",
      "         -3.9822e-01, -3.7805e-01,  3.5340e-02,  2.5813e-01, -3.8466e-01,\n",
      "          4.7928e-01, -5.1439e-02,  2.4273e-01, -3.7626e-01, -4.0137e-01,\n",
      "         -4.1273e-01,  2.9311e-01],\n",
      "        [ 2.7941e-01, -2.7799e-01,  2.9204e-01, -1.5116e-01,  4.1965e-01,\n",
      "          4.5914e-01,  2.3888e-01, -6.5057e-03,  3.4436e-01,  3.5705e-01,\n",
      "         -2.0121e-01, -4.2071e-01,  4.1344e-01,  3.7208e-01,  4.3398e-01,\n",
      "         -1.6955e-01,  2.5618e-01,  2.6365e-01, -1.0025e-01,  4.6771e-02,\n",
      "         -2.0489e-01, -1.4922e-02,  3.3192e-01, -2.4970e-01,  9.1442e-02,\n",
      "          1.8297e-01, -3.2959e-01, -3.6131e-01, -2.5241e-01, -4.5258e-01,\n",
      "         -4.4752e-01,  4.3183e-01],\n",
      "        [-4.9239e-01,  4.0911e-02,  2.3493e-01,  1.7374e-01,  3.8511e-01,\n",
      "         -3.3509e-01,  3.2483e-01, -2.4865e-01, -2.1780e-02, -2.3470e-03,\n",
      "         -2.9898e-01, -3.4312e-01, -1.4965e-01, -1.0569e-01, -9.0111e-02,\n",
      "          3.0730e-01, -7.1004e-05,  3.5332e-01, -2.0599e-01, -2.3835e-02,\n",
      "         -4.7235e-01,  2.4358e-01, -4.2718e-01,  3.0557e-02,  4.9300e-01,\n",
      "          3.2481e-01, -1.8298e-01,  3.2977e-02, -1.8362e-01,  2.2198e-01,\n",
      "         -3.8424e-01, -3.6883e-02],\n",
      "        [ 4.6852e-01,  3.6211e-01,  4.1562e-01, -2.9701e-01, -2.7369e-01,\n",
      "          2.3880e-01,  3.8369e-01, -1.5909e-02, -2.2461e-01, -4.2088e-01,\n",
      "          2.2557e-01, -4.1628e-01,  1.7758e-01, -3.4000e-01, -3.0789e-01,\n",
      "         -1.4980e-01,  4.9020e-01,  1.8770e-01,  1.2309e-01, -4.3623e-01,\n",
      "         -2.1539e-01, -1.5428e-01,  2.4264e-02,  4.0710e-01,  1.7387e-01,\n",
      "          3.6072e-01,  4.0583e-01,  2.0334e-01,  7.7504e-02,  1.6278e-01,\n",
      "         -3.9073e-01,  1.7770e-01],\n",
      "        [-2.7238e-01, -1.5528e-01, -3.2696e-01, -3.5053e-01,  1.7318e-01,\n",
      "         -1.4317e-01, -1.7927e-01,  2.9313e-02, -3.3649e-01, -2.2049e-01,\n",
      "          3.0382e-01,  4.6060e-01,  3.4512e-01,  3.1371e-01, -3.6985e-01,\n",
      "         -6.1531e-02, -7.4205e-02, -1.8177e-01, -2.3976e-01, -1.7595e-01,\n",
      "         -4.2169e-01,  5.2908e-02, -3.7634e-01, -3.2681e-01,  9.9790e-02,\n",
      "          4.7544e-01,  4.2037e-01, -2.7085e-01, -1.0754e-01,  3.0716e-01,\n",
      "         -5.3750e-02, -3.9191e-01],\n",
      "        [ 1.1528e-01,  9.2747e-03,  4.5166e-01,  4.4656e-01, -2.7655e-02,\n",
      "         -1.5139e-01, -1.3980e-01, -4.2962e-01, -3.8818e-01, -2.9328e-01,\n",
      "          4.5808e-01,  1.8899e-01, -4.8210e-01,  1.8461e-01, -4.4975e-01,\n",
      "          7.9525e-02, -3.6460e-01, -1.1926e-01, -3.2769e-01, -1.9947e-01,\n",
      "          7.6226e-02, -3.9353e-01, -2.6236e-01, -1.3506e-01, -1.9173e-02,\n",
      "          3.5995e-01,  7.1468e-02, -2.2823e-01, -3.2696e-01,  4.2182e-01,\n",
      "          1.5834e-01, -9.8861e-02],\n",
      "        [-3.1462e-01,  1.4074e-01, -4.0806e-01, -1.9797e-01, -4.1167e-01,\n",
      "         -1.2809e-01, -4.0394e-01,  4.8547e-01,  4.9690e-01, -3.2822e-01,\n",
      "          2.6167e-01,  1.4346e-02,  3.3006e-01, -8.4942e-02,  6.5773e-02,\n",
      "         -1.2017e-01, -1.7036e-01, -7.7045e-02, -4.8103e-02,  1.2748e-01,\n",
      "          1.1939e-01,  2.2148e-02,  1.0981e-01, -2.9344e-01,  3.8854e-01,\n",
      "          2.7424e-01,  4.2409e-01, -1.2745e-01, -1.2879e-01,  3.0676e-01,\n",
      "          3.3973e-01, -1.4499e-01]], device='cuda:0', dtype=torch.float64)\n",
      "encoder.layers.0.net.linears.2.bias tensor([ 8.1064e-04,  4.5755e-41,  8.1064e-04,  4.5755e-41,  8.4397e-06,\n",
      "         0.0000e+00,  8.4397e-06,  0.0000e+00, -1.0356e-01, -4.0885e-02,\n",
      "        -5.9725e-02, -1.2095e-01,  1.0046e-01, -1.7276e-01, -7.7129e-02,\n",
      "        -1.4185e-01], device='cuda:0', dtype=torch.float64)\n",
      "encoder.layers.0.net.linears.2.E_linear.weight tensor([[-4.7006e-01, -5.1811e-04, -1.4242e-01, -2.2686e-01, -1.9834e-01,\n",
      "          6.5917e-02,  2.4893e-01, -7.6782e-02,  1.2843e-01,  7.7331e-02,\n",
      "          1.6488e-01,  4.0755e-02,  1.8898e-01, -3.6728e-01,  2.8263e-01,\n",
      "          8.4106e-02,  4.0685e-01, -3.7000e-03,  3.4963e-01,  4.7979e-01,\n",
      "          2.4436e-02,  4.8455e-01,  1.7729e-01,  8.2233e-02, -1.8433e-04,\n",
      "         -2.2611e-02,  1.4429e-01,  2.4591e-01,  2.9524e-01, -3.9883e-02,\n",
      "         -6.6878e-02, -1.8955e-01],\n",
      "        [-8.9306e-02, -3.6372e-01,  2.4109e-01,  4.1076e-01,  5.6838e-02,\n",
      "          2.5719e-01, -3.3043e-01,  4.1304e-01, -4.9594e-01, -6.7161e-02,\n",
      "         -2.7439e-01,  4.1645e-01, -1.5231e-01, -5.7453e-03,  3.2218e-01,\n",
      "          2.6752e-01, -3.1173e-01, -1.9173e-01, -1.8715e-01, -2.4313e-01,\n",
      "         -5.1088e-03, -9.7401e-02, -9.1531e-03,  1.6352e-01,  1.3018e-01,\n",
      "         -3.9409e-01, -3.6084e-02,  9.4361e-02, -4.3091e-01, -1.4533e-01,\n",
      "         -4.5570e-01, -3.5243e-01],\n",
      "        [-3.9363e-02, -4.7198e-01,  2.1233e-01, -8.0625e-02,  3.4092e-01,\n",
      "          3.0305e-01, -8.1333e-02,  1.7929e-01, -1.2002e-02,  4.9935e-01,\n",
      "          4.2558e-01,  2.8923e-01, -4.5570e-01,  2.9179e-01, -1.6541e-01,\n",
      "          4.7096e-01,  4.0549e-01,  3.4809e-01,  3.6252e-02,  2.9056e-01,\n",
      "          3.4463e-01,  4.7197e-01, -5.2636e-03,  1.2070e-01,  5.2841e-02,\n",
      "          3.6979e-02,  2.3579e-01,  1.3787e-01,  4.9744e-01, -2.8680e-01,\n",
      "          2.0753e-01, -3.2206e-01],\n",
      "        [-8.3496e-02, -3.4654e-01,  1.3256e-01, -8.2741e-02, -1.7881e-02,\n",
      "          4.7779e-01,  2.2600e-01,  3.3124e-01,  1.2032e-02, -2.3870e-01,\n",
      "          3.3010e-01, -2.0699e-01, -2.7875e-01, -4.4205e-01,  2.0287e-01,\n",
      "          2.1231e-01,  2.9483e-01,  1.0876e-01,  3.1485e-01,  4.9849e-01,\n",
      "          3.3627e-01, -3.6863e-01,  2.5705e-02,  4.7815e-01,  4.0675e-01,\n",
      "          7.0030e-02, -4.1652e-01, -3.4281e-01,  2.7586e-01,  4.8648e-01,\n",
      "         -1.0979e-01,  8.4257e-02],\n",
      "        [ 4.6017e-02,  2.7600e-01,  7.9528e-02,  4.4785e-01,  1.2003e-01,\n",
      "          4.5051e-01, -3.7216e-01,  1.6074e-01, -1.8899e-01,  3.7396e-01,\n",
      "          3.8470e-01, -4.5553e-01,  4.2751e-01, -2.5482e-01, -8.9108e-02,\n",
      "          4.3195e-01,  2.2021e-01,  2.2324e-01, -3.3139e-01, -1.7435e-01,\n",
      "         -3.7103e-01, -4.8995e-01, -3.9822e-02,  2.8983e-01, -4.1333e-01,\n",
      "          4.1531e-01, -4.7686e-01,  4.4492e-01,  2.9057e-01,  4.4359e-01,\n",
      "         -6.6290e-02, -5.3613e-02],\n",
      "        [ 7.6314e-02,  1.8981e-01,  4.6746e-02, -3.1948e-01,  4.6469e-01,\n",
      "          3.1996e-02, -3.6835e-01,  3.2286e-01, -2.6312e-01,  3.2854e-01,\n",
      "         -3.8130e-01, -7.9800e-02, -3.1681e-01, -1.8628e-01, -3.3653e-01,\n",
      "          4.4941e-01, -2.8126e-01,  8.1195e-02,  1.6560e-01,  4.7317e-01,\n",
      "         -2.8053e-01, -2.4899e-01, -6.6363e-02,  2.9766e-01, -3.9595e-01,\n",
      "          4.8147e-02, -8.4392e-02,  1.7634e-02, -2.1829e-01,  4.6653e-01,\n",
      "          2.4551e-01, -8.3648e-02],\n",
      "        [-3.1012e-01,  6.8450e-02, -2.0956e-01,  9.9198e-02,  1.7802e-01,\n",
      "          1.0565e-01,  4.1011e-01,  1.6395e-02,  1.6025e-01,  2.0552e-01,\n",
      "          1.9858e-01, -4.6209e-01,  4.3763e-01, -1.2790e-01,  3.8872e-01,\n",
      "          2.3136e-01, -4.1179e-01, -1.7012e-01,  3.4305e-01,  1.5150e-01,\n",
      "         -3.2595e-01, -1.7339e-01, -1.5235e-01, -3.8879e-01,  1.8649e-02,\n",
      "         -7.3517e-02, -3.1372e-01,  4.8814e-01, -4.5800e-01, -4.9512e-01,\n",
      "         -3.6627e-01, -2.3632e-01],\n",
      "        [-4.1781e-01,  3.6381e-01,  4.1565e-01, -3.5134e-01, -3.5912e-01,\n",
      "          2.9814e-01,  2.4500e-01, -2.8823e-02,  3.1176e-01,  1.7863e-01,\n",
      "         -2.9458e-02, -4.4921e-01, -4.9876e-01, -2.8644e-01,  2.4506e-02,\n",
      "          3.6331e-01, -3.9310e-01, -2.1642e-01, -3.6800e-01, -3.7851e-01,\n",
      "          4.7312e-01, -4.0122e-01, -2.8530e-01, -1.9621e-01,  1.0933e-01,\n",
      "         -1.0813e-01, -6.5499e-02, -2.0403e-01, -4.9642e-01,  2.8733e-01,\n",
      "         -3.7465e-01, -2.5725e-01],\n",
      "        [ 4.4073e-01, -4.5687e-01,  3.0627e-01, -7.7410e-02, -3.7315e-01,\n",
      "          2.4665e-01,  3.8908e-01,  2.8379e-01,  2.2693e-01, -1.8054e-02,\n",
      "         -2.1175e-01, -7.0979e-02, -4.0448e-01,  2.4513e-01, -3.5572e-02,\n",
      "         -4.8263e-01,  3.6572e-01,  2.2757e-01,  3.7858e-01, -3.1981e-02,\n",
      "         -1.5301e-01, -3.4426e-01,  1.4939e-01,  4.4150e-02,  2.1421e-01,\n",
      "          2.5017e-01,  2.0896e-01, -7.4138e-02, -7.6880e-02,  3.3312e-01,\n",
      "          2.1186e-01,  1.2097e-01],\n",
      "        [ 2.5245e-01,  2.3557e-01,  4.2975e-01,  3.5964e-01,  3.6819e-01,\n",
      "          4.6485e-01, -2.5751e-01, -1.2090e-01, -1.2843e-01, -4.4471e-01,\n",
      "          4.3242e-01, -1.8482e-01, -4.5409e-01,  3.8490e-01, -7.3364e-02,\n",
      "          2.2232e-01, -4.5939e-01, -1.0704e-01,  4.1251e-01, -2.0513e-01,\n",
      "         -4.0209e-01,  5.1523e-03,  3.1641e-01,  3.3789e-01, -2.5863e-01,\n",
      "         -2.2330e-01,  8.8313e-02, -2.6864e-01, -1.1296e-01,  1.0893e-01,\n",
      "          9.7604e-03,  3.6475e-01],\n",
      "        [ 4.4435e-01,  1.5821e-01, -9.7728e-03,  4.8912e-01, -1.1553e-01,\n",
      "          9.2791e-02,  3.7022e-01, -2.7037e-01,  4.0061e-01, -4.6638e-01,\n",
      "          7.6300e-02,  9.5039e-02,  3.0205e-01,  1.2284e-01,  3.6737e-01,\n",
      "          2.8804e-01,  4.8840e-01,  6.3821e-02, -1.1938e-01,  3.0135e-01,\n",
      "         -3.7105e-01,  3.4916e-01,  4.8429e-01, -2.3950e-01, -3.0421e-01,\n",
      "         -3.8166e-01, -2.2180e-01, -2.9752e-02,  4.1849e-01,  8.6950e-02,\n",
      "         -3.4933e-01, -3.2599e-01],\n",
      "        [ 4.3471e-01,  9.2771e-02,  4.4865e-01,  2.3190e-01, -3.0394e-01,\n",
      "          2.9069e-01,  4.7452e-01,  4.2019e-01, -4.7342e-01, -3.9404e-01,\n",
      "          1.8171e-01,  6.3584e-02,  4.2432e-01, -3.1914e-01,  3.4532e-02,\n",
      "          2.2856e-01, -2.9731e-01, -2.4878e-01,  1.8341e-01,  3.9591e-01,\n",
      "          3.9661e-01, -2.3934e-01, -2.1683e-02, -1.4086e-01, -9.5381e-02,\n",
      "         -2.4311e-01, -6.0418e-02, -1.1691e-01, -2.6173e-02, -3.3394e-01,\n",
      "         -1.3504e-01,  3.2704e-01],\n",
      "        [ 4.4313e-01,  9.8241e-03, -2.8448e-02,  1.9721e-01,  2.3393e-01,\n",
      "         -2.0066e-01,  2.7930e-01, -3.7293e-01,  3.6808e-01, -6.2239e-03,\n",
      "         -1.4978e-01,  1.5319e-01, -1.2654e-01, -2.5546e-01, -1.1024e-01,\n",
      "         -1.5167e-02,  3.3952e-01, -3.4526e-01, -4.6465e-01,  4.8128e-01,\n",
      "         -2.0480e-01, -1.3895e-02, -2.8867e-01, -1.9828e-02, -1.7720e-01,\n",
      "          8.4724e-02, -4.5303e-01, -2.0935e-01, -3.3246e-01,  3.2553e-02,\n",
      "         -2.3097e-01,  3.5793e-01],\n",
      "        [-8.3338e-02, -2.8742e-01,  1.1714e-01, -7.9033e-02,  8.5003e-03,\n",
      "         -2.0545e-01,  1.3209e-02,  4.6015e-01, -3.1453e-01, -4.6975e-01,\n",
      "         -1.7886e-01, -2.8110e-02, -4.2412e-01,  1.6902e-02, -3.9705e-01,\n",
      "          3.1293e-01,  3.5502e-01, -2.2498e-01, -2.0394e-01, -4.3013e-01,\n",
      "         -1.4068e-01,  4.9927e-01,  1.9752e-01,  3.0401e-01, -1.3927e-01,\n",
      "          1.6386e-01,  2.8020e-01, -2.1847e-01, -1.4703e-01, -1.2181e-01,\n",
      "         -1.5441e-01,  1.2761e-01],\n",
      "        [-2.3805e-01, -3.0051e-01, -1.1979e-01,  4.9477e-02,  1.8250e-01,\n",
      "          9.4153e-03, -4.2602e-01, -4.8375e-01, -6.2506e-02, -1.7171e-01,\n",
      "          3.2989e-01, -2.4310e-01,  2.5111e-01, -3.8445e-01,  2.7757e-02,\n",
      "          1.4761e-01, -3.4496e-01,  4.5653e-01, -1.0799e-01,  2.7888e-01,\n",
      "         -2.1176e-01, -2.6422e-01,  4.3858e-02, -2.3879e-01, -3.6945e-01,\n",
      "         -2.8138e-01, -1.6004e-01, -2.3910e-01, -1.7921e-01, -7.2772e-02,\n",
      "          2.9676e-01, -1.0557e-02],\n",
      "        [-4.0451e-01, -2.0328e-01, -3.2507e-01,  8.1030e-02, -2.3106e-01,\n",
      "         -2.1570e-01, -4.2489e-01,  1.5087e-01, -3.2993e-01,  4.6300e-01,\n",
      "         -2.2952e-01,  2.6425e-01,  3.3514e-01, -4.5048e-03,  1.9684e-02,\n",
      "          4.4510e-01,  3.5918e-01,  1.8319e-01, -4.6779e-02,  4.1747e-01,\n",
      "         -4.2477e-01,  4.4889e-01,  6.7286e-02,  2.5501e-01, -1.3005e-01,\n",
      "          1.7440e-01, -2.5555e-01,  2.2542e-01,  3.5203e-01,  2.7619e-01,\n",
      "          3.8174e-01, -1.6554e-01]], device='cuda:0', dtype=torch.float64)\n",
      "encoder.layers.0.net.linears.3.bias tensor([ 8.1064e-04,  4.5755e-41,  8.1064e-04,  4.5755e-41,  8.4454e-06,\n",
      "         0.0000e+00,  8.4454e-06,  0.0000e+00, -8.2616e-02,  3.0071e-02,\n",
      "         1.0180e-01,  1.4723e-01, -3.7875e-02, -1.7093e-01, -1.3623e-01,\n",
      "        -6.7970e-02], device='cuda:0', dtype=torch.float64)\n",
      "encoder.layers.0.net.linears.3.E_linear.weight tensor([[-0.2606, -0.2135, -0.4614,  0.1412, -0.1172, -0.0873, -0.1862, -0.4399,\n",
      "         -0.1678,  0.3635,  0.4121, -0.4774,  0.3762,  0.2640, -0.0310,  0.4794,\n",
      "         -0.3269, -0.0546,  0.2827, -0.2466, -0.0183,  0.4143,  0.3752, -0.4195,\n",
      "         -0.3623,  0.2433,  0.4383,  0.0403, -0.0962,  0.1447, -0.0793,  0.1136],\n",
      "        [ 0.0823,  0.3089, -0.3669,  0.3365,  0.0246,  0.3033,  0.0666, -0.3780,\n",
      "          0.3056, -0.3507,  0.2706, -0.2605,  0.3748, -0.4045,  0.0158,  0.4195,\n",
      "         -0.0768,  0.3949, -0.3162,  0.2444,  0.1429, -0.1033,  0.2893, -0.0647,\n",
      "         -0.2201,  0.4839,  0.1447, -0.3323, -0.2515,  0.0930,  0.0317, -0.3095],\n",
      "        [ 0.1430,  0.2482, -0.2911,  0.0931,  0.0390, -0.2539,  0.1331, -0.3959,\n",
      "          0.3299, -0.4839, -0.4493, -0.0572, -0.0127,  0.3807, -0.4020, -0.4882,\n",
      "          0.4999,  0.0131,  0.0151, -0.0732,  0.0415, -0.4548,  0.0223,  0.4268,\n",
      "          0.4503, -0.1818,  0.4209, -0.1229, -0.3477, -0.1824,  0.1081,  0.1292],\n",
      "        [-0.0464, -0.1150,  0.4105, -0.4344,  0.4502,  0.2658,  0.1166,  0.4691,\n",
      "          0.3872,  0.2898,  0.0018,  0.4129,  0.4672,  0.0526,  0.2990, -0.0941,\n",
      "          0.2973,  0.4056,  0.4012,  0.0282,  0.4892,  0.2803, -0.4238, -0.2894,\n",
      "          0.3915,  0.3314,  0.4121,  0.3146, -0.3210,  0.0223,  0.0467, -0.2424],\n",
      "        [-0.3087, -0.0783, -0.0211, -0.3118, -0.3854, -0.1721, -0.1768,  0.1390,\n",
      "          0.2326, -0.2621,  0.1409,  0.0073, -0.2127, -0.0701,  0.3969, -0.0575,\n",
      "          0.1537, -0.1078,  0.2282,  0.3320, -0.2032, -0.4442, -0.0436, -0.1785,\n",
      "          0.3048, -0.0686, -0.3556,  0.4009, -0.3453, -0.4110, -0.1611,  0.1527],\n",
      "        [ 0.1483,  0.3538, -0.3284,  0.0843, -0.2212, -0.0778, -0.3786,  0.4766,\n",
      "         -0.3970,  0.4183,  0.0090, -0.1446, -0.2847, -0.2233, -0.0989, -0.4411,\n",
      "          0.0374,  0.4043,  0.2865, -0.0644, -0.4847, -0.3184,  0.1463, -0.4703,\n",
      "          0.1251,  0.1389,  0.3888,  0.2781, -0.1820,  0.4341, -0.1832, -0.3276],\n",
      "        [ 0.1248, -0.2878, -0.0252,  0.3979, -0.2824, -0.1781,  0.3615, -0.0342,\n",
      "          0.2848, -0.1375,  0.1673, -0.3569, -0.2930, -0.1230,  0.4321,  0.4723,\n",
      "          0.2180,  0.3156, -0.4287,  0.4960,  0.2015,  0.4220,  0.4467,  0.0591,\n",
      "          0.0304,  0.4791, -0.2254,  0.3035, -0.4388, -0.2703,  0.3208,  0.0969],\n",
      "        [-0.2361,  0.0328, -0.4967, -0.4478,  0.3870,  0.0983, -0.3486, -0.0516,\n",
      "          0.0568,  0.1612,  0.4199,  0.2094,  0.0331, -0.0846,  0.1240, -0.2141,\n",
      "          0.3870,  0.3738,  0.0719, -0.2663,  0.4465, -0.4666, -0.4503, -0.4141,\n",
      "         -0.4545, -0.2208, -0.3862, -0.0181,  0.2970, -0.3178, -0.1551, -0.3274],\n",
      "        [-0.3720, -0.2032, -0.3158,  0.1446, -0.4993, -0.4785,  0.3489, -0.4893,\n",
      "          0.0371,  0.4652, -0.4617,  0.2583, -0.2866, -0.4064, -0.3261, -0.0598,\n",
      "          0.4068,  0.2033,  0.4834, -0.1564, -0.2290, -0.0351, -0.2989, -0.2917,\n",
      "         -0.2515, -0.0375, -0.2852, -0.3613,  0.1390, -0.3991,  0.1716,  0.0904],\n",
      "        [-0.3940,  0.0558,  0.0343,  0.0607, -0.2373,  0.0279,  0.1979, -0.3416,\n",
      "         -0.4054, -0.1272,  0.2969, -0.3639,  0.3567, -0.2781,  0.3423,  0.2530,\n",
      "         -0.1701,  0.3584, -0.4685,  0.2464,  0.2695,  0.2717, -0.2915, -0.3394,\n",
      "         -0.1800,  0.3301,  0.3663,  0.2255,  0.2621, -0.2992,  0.3105, -0.2183],\n",
      "        [ 0.2414,  0.0112,  0.2379,  0.1594,  0.0665,  0.0061, -0.0686, -0.0311,\n",
      "          0.1236,  0.3427, -0.0971,  0.2226,  0.2333,  0.4887,  0.1431,  0.2717,\n",
      "          0.4242, -0.3875,  0.2058,  0.4426, -0.3370, -0.3953, -0.1972, -0.4352,\n",
      "         -0.3286, -0.4809,  0.1660, -0.2477, -0.0858,  0.4520, -0.0935, -0.0354],\n",
      "        [-0.0393, -0.0883,  0.0881, -0.2569, -0.1244, -0.4899, -0.2426,  0.3253,\n",
      "         -0.3343,  0.1204,  0.2584, -0.0842,  0.3971,  0.4660, -0.0604,  0.4957,\n",
      "          0.4943,  0.4902,  0.4700, -0.1744, -0.3093,  0.0343, -0.2106, -0.1988,\n",
      "          0.1147, -0.0541,  0.1166,  0.0623, -0.3384,  0.4223, -0.4132, -0.4108],\n",
      "        [ 0.0704,  0.4980,  0.3221, -0.3642,  0.0902,  0.4880,  0.0429,  0.0699,\n",
      "         -0.4779, -0.1117,  0.3287, -0.2353,  0.4439,  0.1878, -0.1269, -0.0196,\n",
      "          0.4463, -0.0181, -0.1683, -0.0775, -0.4649,  0.4270,  0.2741, -0.1635,\n",
      "         -0.0762, -0.1308,  0.3107, -0.0530,  0.2976, -0.1972,  0.1806,  0.2555],\n",
      "        [ 0.3831, -0.4503, -0.3975, -0.1117,  0.0555, -0.0579, -0.4602, -0.4722,\n",
      "         -0.2043, -0.4044, -0.2360, -0.1024,  0.1627,  0.0424,  0.4318, -0.2200,\n",
      "         -0.4776,  0.0990, -0.1954,  0.3449,  0.2738,  0.2552,  0.4910, -0.4015,\n",
      "          0.4058, -0.1696, -0.4791,  0.1072,  0.3526, -0.4799,  0.1048, -0.0711],\n",
      "        [ 0.4959, -0.2699,  0.2448,  0.1713,  0.2506,  0.0664,  0.3980, -0.0784,\n",
      "         -0.0567,  0.2652,  0.3155, -0.2823, -0.3440, -0.1247,  0.2306, -0.3364,\n",
      "          0.2190,  0.3259,  0.1843,  0.0313, -0.3225,  0.4267,  0.2488,  0.0442,\n",
      "          0.4605, -0.1174, -0.2806,  0.3775, -0.4081,  0.4852,  0.0583,  0.1413],\n",
      "        [ 0.2686, -0.2671,  0.0179,  0.1072,  0.4239,  0.3887,  0.4525, -0.1907,\n",
      "          0.3143,  0.2516,  0.2763,  0.0072,  0.4346, -0.1240, -0.4298,  0.2958,\n",
      "         -0.0552,  0.4925,  0.2152, -0.0229, -0.0659,  0.1898, -0.1493, -0.1271,\n",
      "         -0.2065,  0.4336,  0.0078,  0.2067, -0.4468, -0.1331,  0.2004, -0.0164]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "encoder.layers.0.net.linears.4.bias tensor([ 8.1057e-04,  4.5755e-41,  8.1057e-04,  4.5755e-41,  8.4454e-06,\n",
      "         0.0000e+00,  8.4454e-06,  0.0000e+00, -8.2616e-02,  3.0071e-02,\n",
      "         1.0180e-01,  1.4723e-01, -3.7875e-02, -1.7093e-01, -1.3623e-01,\n",
      "        -6.7970e-02], device='cuda:0', dtype=torch.float64)\n",
      "encoder.layers.0.net.linears.4.E_linear.weight tensor([[-0.3209,  0.0204,  0.0694, -0.2852,  0.1668,  0.4977, -0.3025, -0.4470,\n",
      "         -0.0448,  0.2506,  0.4210,  0.0253,  0.1105,  0.3318, -0.0936,  0.0109,\n",
      "          0.3906, -0.1350,  0.1774,  0.0620,  0.3630, -0.1969, -0.0901,  0.4326,\n",
      "          0.3694,  0.3407,  0.2100,  0.4833, -0.1830,  0.0672, -0.3738,  0.2539],\n",
      "        [-0.1026, -0.4502, -0.0152, -0.2168, -0.4533,  0.2276, -0.1940,  0.4669,\n",
      "         -0.1993, -0.4208, -0.3779,  0.2926,  0.0193,  0.0907,  0.0304,  0.4596,\n",
      "         -0.2013, -0.0899, -0.4804, -0.2890, -0.3367,  0.2563, -0.2898,  0.4042,\n",
      "          0.2005,  0.0196, -0.1074, -0.2695, -0.4250,  0.4045,  0.2730, -0.3957],\n",
      "        [-0.3681,  0.3029, -0.2039, -0.3193,  0.3972, -0.0996, -0.2260, -0.0479,\n",
      "          0.1241, -0.4935, -0.0670, -0.0278,  0.1231, -0.3591,  0.4615,  0.1721,\n",
      "          0.0163, -0.4798, -0.4520, -0.3743, -0.2769,  0.2028,  0.4710,  0.2264,\n",
      "          0.1662,  0.1465,  0.1701,  0.2129, -0.1164, -0.3313,  0.3672, -0.4418],\n",
      "        [-0.4658, -0.0031,  0.4567,  0.1506, -0.4898, -0.1724, -0.0646, -0.3248,\n",
      "          0.4776,  0.0704, -0.1121, -0.4164, -0.3398, -0.0641, -0.1418,  0.0116,\n",
      "         -0.2047, -0.2328, -0.2089,  0.0846,  0.4535, -0.0159, -0.1748,  0.1950,\n",
      "          0.4343, -0.0885, -0.1744,  0.4465,  0.2052, -0.4848,  0.1790, -0.3035],\n",
      "        [ 0.2748,  0.3247, -0.4469, -0.4272, -0.3375,  0.3978, -0.2846, -0.2075,\n",
      "         -0.0576, -0.4750, -0.3381, -0.3769,  0.0559,  0.3367, -0.4145,  0.0314,\n",
      "         -0.2663,  0.0440,  0.1359,  0.2603, -0.3221,  0.2001, -0.1060,  0.0696,\n",
      "          0.4957, -0.4429,  0.1613, -0.0077, -0.3430, -0.3410,  0.1851, -0.3911],\n",
      "        [-0.4495,  0.4162,  0.0996,  0.4210, -0.3578,  0.3715, -0.2293, -0.2062,\n",
      "          0.4456,  0.4962, -0.4647, -0.2855,  0.0464, -0.3241, -0.4134, -0.2104,\n",
      "          0.0848,  0.4810,  0.3175,  0.1589, -0.3744, -0.1605, -0.2832,  0.3289,\n",
      "         -0.3007, -0.2395, -0.1814, -0.0789, -0.4428, -0.4559,  0.0403,  0.2801],\n",
      "        [-0.0890, -0.1711, -0.2521,  0.0868,  0.3635, -0.4970, -0.1716,  0.4086,\n",
      "         -0.1902,  0.4965, -0.2512,  0.3065, -0.2482, -0.2060,  0.4550,  0.2317,\n",
      "         -0.3932, -0.1024, -0.2907,  0.0512,  0.4833,  0.3744,  0.4486,  0.4014,\n",
      "          0.2456,  0.2220, -0.1117,  0.4153, -0.3883,  0.4176,  0.2599, -0.4012],\n",
      "        [ 0.2072,  0.4480, -0.3740,  0.2199, -0.3126, -0.1313, -0.1327,  0.2508,\n",
      "          0.0765,  0.0536,  0.4591,  0.2814,  0.3831, -0.1194, -0.2550,  0.1671,\n",
      "          0.3052, -0.0945,  0.3494, -0.0789, -0.2748, -0.1207, -0.3582,  0.2143,\n",
      "         -0.3590, -0.3158,  0.3146, -0.3228, -0.1944, -0.1706,  0.3372, -0.3693],\n",
      "        [-0.3745,  0.0651, -0.4471,  0.0157,  0.2873, -0.3452,  0.1333, -0.3572,\n",
      "          0.0467, -0.2650,  0.0298, -0.3046, -0.3221,  0.3391, -0.0628,  0.2336,\n",
      "         -0.1043,  0.4554, -0.2132, -0.1032,  0.1653, -0.4011,  0.1320, -0.3332,\n",
      "         -0.4912,  0.4896,  0.3003,  0.2302, -0.0278, -0.4088, -0.2865, -0.3262],\n",
      "        [ 0.3031, -0.3601,  0.3061, -0.0170,  0.1573, -0.3230, -0.0157,  0.1466,\n",
      "          0.2361, -0.4809,  0.0989,  0.4733, -0.1448, -0.0118, -0.1175,  0.1277,\n",
      "         -0.4203,  0.4451,  0.4947,  0.4643, -0.1891, -0.1984,  0.1717, -0.3916,\n",
      "          0.4281,  0.4754,  0.3858, -0.0367, -0.3558,  0.2561,  0.0470, -0.2427],\n",
      "        [-0.1739, -0.1374,  0.2200, -0.3214,  0.0712, -0.1364,  0.0859,  0.2062,\n",
      "          0.1605, -0.1334, -0.2052, -0.3599, -0.3097, -0.4470, -0.1129,  0.4013,\n",
      "          0.3530,  0.0299, -0.3073, -0.2126,  0.3322,  0.3288, -0.2764, -0.3568,\n",
      "          0.4548,  0.3365,  0.3283,  0.3095, -0.1989, -0.2133,  0.0263,  0.1415],\n",
      "        [-0.0175, -0.0530,  0.1160, -0.1335, -0.2348,  0.0550,  0.4880, -0.3801,\n",
      "         -0.1637,  0.2767, -0.2633, -0.2280,  0.4090, -0.3587, -0.0400,  0.2239,\n",
      "          0.3879, -0.0134,  0.0472,  0.4146, -0.2706, -0.1231, -0.0481,  0.4700,\n",
      "         -0.0997,  0.2657,  0.4411,  0.0203, -0.4012, -0.1736,  0.2934, -0.0433],\n",
      "        [-0.2071,  0.2652, -0.2467,  0.3109,  0.4847,  0.4326,  0.1082, -0.3386,\n",
      "         -0.3241,  0.2340,  0.4639, -0.2333, -0.1170, -0.0257,  0.1830,  0.1108,\n",
      "          0.1952, -0.3504,  0.1815,  0.4338,  0.3053,  0.1252, -0.1921,  0.0770,\n",
      "         -0.4936,  0.2589, -0.4510,  0.4619,  0.3455, -0.1593, -0.3218,  0.0139],\n",
      "        [-0.3604,  0.4507,  0.0978, -0.1033, -0.4276, -0.3777,  0.4436,  0.3749,\n",
      "         -0.4994, -0.3467,  0.3797,  0.2103,  0.2300, -0.1436, -0.4082, -0.3206,\n",
      "         -0.2392,  0.3599,  0.2880, -0.3621,  0.1382, -0.4830, -0.2452, -0.0340,\n",
      "         -0.2822, -0.3244,  0.1744,  0.4289,  0.2940, -0.0873,  0.2848, -0.0022],\n",
      "        [ 0.3339,  0.0517, -0.1410,  0.4080, -0.0881, -0.3829,  0.1415, -0.3243,\n",
      "          0.0938, -0.1423,  0.3598,  0.4146,  0.3407,  0.1765,  0.4690,  0.4473,\n",
      "          0.4826, -0.1162, -0.1478, -0.4388,  0.2343, -0.2719,  0.1224, -0.2539,\n",
      "          0.0242, -0.3240,  0.2444,  0.2311, -0.4649,  0.0728, -0.4718, -0.4705],\n",
      "        [-0.0909,  0.3817, -0.0232,  0.0335, -0.2913,  0.0580,  0.1845, -0.0427,\n",
      "          0.1935, -0.2533, -0.1217,  0.1895, -0.2467,  0.3765, -0.3648, -0.2357,\n",
      "         -0.4420,  0.0292,  0.4548, -0.0545,  0.3352,  0.4294, -0.4921,  0.4013,\n",
      "          0.1297,  0.2498,  0.4547,  0.4017, -0.1954, -0.2432, -0.0785,  0.0197]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "encoder.layers.0.net.linears.5.bias tensor([ 8.1057e-04,  4.5755e-41,  8.1057e-04,  4.5755e-41,  8.4454e-06,\n",
      "         0.0000e+00,  8.4454e-06,  0.0000e+00, -8.2616e-02,  3.0071e-02,\n",
      "         1.0180e-01,  1.4723e-01, -3.7875e-02, -1.7093e-01, -1.3623e-01,\n",
      "        -6.7970e-02], device='cuda:0', dtype=torch.float64)\n",
      "encoder.layers.0.net.linears.5.E_linear.weight tensor([[ 0.4620, -0.0996,  0.0292, -0.3161,  0.0866, -0.3059,  0.2747, -0.4077,\n",
      "         -0.2991,  0.2255, -0.4755, -0.3416,  0.0278, -0.2876, -0.0659, -0.3963,\n",
      "         -0.3117, -0.2474,  0.3666,  0.0417, -0.2954,  0.0593,  0.0792,  0.3734,\n",
      "         -0.4497, -0.4525,  0.4783, -0.2351,  0.1062,  0.3974,  0.3436,  0.1378],\n",
      "        [ 0.1306,  0.2898,  0.2988,  0.0837, -0.4906,  0.3061,  0.3421,  0.3320,\n",
      "         -0.3372, -0.3616, -0.1224,  0.4734,  0.2416, -0.4541,  0.3103, -0.3937,\n",
      "          0.3429,  0.4770, -0.3994, -0.0150,  0.1618,  0.3663,  0.2155, -0.3482,\n",
      "         -0.0774,  0.0537,  0.4204, -0.3626, -0.2629,  0.1570, -0.2618,  0.2469],\n",
      "        [ 0.1001,  0.0170, -0.3077,  0.1450,  0.4233,  0.4636, -0.0389,  0.0405,\n",
      "          0.4327,  0.1959,  0.0221, -0.4249, -0.0726,  0.4595,  0.1355, -0.3346,\n",
      "          0.1926, -0.4541, -0.2692,  0.2887,  0.2315,  0.4165, -0.2549,  0.4255,\n",
      "          0.0779, -0.3955,  0.0916,  0.3122,  0.1995,  0.3202,  0.4521,  0.1894],\n",
      "        [-0.4963, -0.1847, -0.0139,  0.0436, -0.4351, -0.2069, -0.3774, -0.2782,\n",
      "         -0.2174, -0.4722,  0.1561, -0.0969, -0.2185,  0.0694, -0.1904,  0.0396,\n",
      "         -0.0069, -0.1921, -0.3218, -0.2441, -0.4217,  0.3718,  0.2009,  0.0493,\n",
      "          0.4658, -0.3753, -0.1495, -0.0866,  0.3596, -0.1204,  0.1300,  0.3216],\n",
      "        [-0.3456, -0.3066, -0.0561, -0.0951, -0.0081, -0.0787,  0.4747, -0.0848,\n",
      "          0.3446,  0.2484,  0.0048, -0.0832,  0.0445, -0.1163, -0.4759, -0.1258,\n",
      "         -0.3657,  0.4990,  0.2353,  0.4310,  0.3585,  0.4613,  0.3612,  0.4436,\n",
      "         -0.3166,  0.1748,  0.4655, -0.4944, -0.3731, -0.2275,  0.0170,  0.2080],\n",
      "        [-0.4867, -0.4051, -0.4577,  0.3211, -0.4044, -0.0430, -0.3237, -0.2535,\n",
      "         -0.4182,  0.4238, -0.0189, -0.0938, -0.0701, -0.4644,  0.0834, -0.3627,\n",
      "          0.1530,  0.3763,  0.3122,  0.0621, -0.0314,  0.1614, -0.1746,  0.2101,\n",
      "          0.2019, -0.2387,  0.3363,  0.3552,  0.0827, -0.0981,  0.0872,  0.1492],\n",
      "        [ 0.2365,  0.3149, -0.4971, -0.3040, -0.2020, -0.4954,  0.0975, -0.4498,\n",
      "         -0.4135,  0.3106,  0.3289,  0.2955,  0.0012, -0.4703,  0.0173, -0.1469,\n",
      "          0.3838,  0.1318, -0.3351, -0.2671,  0.3939,  0.3723,  0.0275,  0.4316,\n",
      "         -0.4154,  0.0759,  0.1063, -0.2764, -0.2206,  0.1955, -0.4801,  0.3258],\n",
      "        [ 0.2051, -0.4668,  0.2481,  0.3016,  0.1267, -0.2538,  0.0057, -0.0810,\n",
      "         -0.2501,  0.4435, -0.0603, -0.3747, -0.4065, -0.2148,  0.0989,  0.2626,\n",
      "          0.1175,  0.2606, -0.0671, -0.2422, -0.1257,  0.3221, -0.0789, -0.0968,\n",
      "          0.1780, -0.0658, -0.3093, -0.3155,  0.1374,  0.4236,  0.4758, -0.1973],\n",
      "        [-0.4537, -0.0406,  0.3005, -0.4541, -0.2423, -0.1149, -0.2234,  0.4358,\n",
      "          0.2747,  0.0893,  0.4067, -0.0275, -0.0647,  0.1926,  0.0535,  0.0482,\n",
      "         -0.0817,  0.0056, -0.2718, -0.1596, -0.4836, -0.1209, -0.0479,  0.1076,\n",
      "          0.1112,  0.0091,  0.3104,  0.2701,  0.1521,  0.3282, -0.1043,  0.1255],\n",
      "        [-0.1681,  0.4777,  0.3925, -0.4390,  0.0465,  0.0166, -0.0098, -0.2423,\n",
      "          0.0901, -0.4714, -0.2650, -0.3231, -0.3848, -0.2730,  0.2893,  0.0051,\n",
      "          0.1599, -0.4981, -0.2025,  0.0280, -0.2354,  0.3692, -0.1710, -0.4442,\n",
      "          0.3921,  0.3147,  0.0859,  0.4186,  0.4997, -0.0431, -0.3890, -0.3734],\n",
      "        [ 0.2092, -0.2190, -0.4413,  0.3603, -0.4583, -0.2677,  0.2360,  0.3560,\n",
      "         -0.4548,  0.3405, -0.0662,  0.4793,  0.0479,  0.0621,  0.1349,  0.3019,\n",
      "         -0.0969,  0.3071,  0.0252, -0.0497, -0.3402, -0.2962,  0.3280, -0.3887,\n",
      "          0.4860,  0.4835, -0.0354,  0.2970,  0.3788, -0.3536, -0.2998, -0.0709],\n",
      "        [ 0.2466,  0.2773,  0.1745, -0.4831, -0.2412,  0.0750,  0.4168, -0.3322,\n",
      "         -0.3743,  0.0977,  0.3652,  0.0128,  0.4282,  0.2855,  0.4079, -0.1989,\n",
      "          0.4337,  0.0696, -0.2716, -0.2647,  0.4440, -0.1230,  0.3155, -0.0997,\n",
      "          0.2038, -0.0783,  0.3373, -0.2946, -0.0214,  0.1208, -0.2312, -0.4096],\n",
      "        [ 0.4860, -0.3589, -0.1419,  0.1881, -0.1467,  0.1972,  0.3527, -0.4257,\n",
      "          0.4766,  0.4035,  0.1150,  0.3075,  0.2905,  0.0596, -0.4667,  0.4233,\n",
      "          0.0170,  0.2288, -0.4106,  0.4263, -0.2841,  0.3187,  0.1196,  0.3851,\n",
      "          0.3745, -0.4717,  0.1230, -0.3838,  0.0752,  0.4716,  0.0844,  0.3572],\n",
      "        [-0.4133, -0.1809,  0.3984,  0.1286,  0.2864, -0.3949, -0.2873,  0.0247,\n",
      "          0.0984, -0.0656, -0.0066, -0.4610, -0.1599, -0.2101, -0.1273, -0.1535,\n",
      "          0.2297, -0.0241,  0.2930, -0.1906,  0.3440,  0.1687, -0.2537, -0.1832,\n",
      "          0.1392,  0.3660, -0.2440,  0.2108, -0.1950,  0.1650, -0.4862,  0.2983],\n",
      "        [ 0.1176, -0.3081, -0.1593, -0.0031, -0.2490,  0.3521, -0.3930,  0.0191,\n",
      "          0.4906,  0.0835,  0.1447, -0.0402,  0.0196,  0.2591, -0.0167, -0.2013,\n",
      "          0.0894,  0.0105,  0.4747,  0.1591,  0.2796, -0.2272, -0.2514,  0.1189,\n",
      "          0.2943, -0.3126,  0.2513,  0.4608, -0.0240, -0.0035,  0.2941, -0.2387],\n",
      "        [ 0.1087,  0.2974,  0.3341,  0.1444,  0.3931, -0.2678,  0.0023, -0.3464,\n",
      "         -0.4226,  0.0492, -0.4277,  0.4348,  0.2439,  0.2917,  0.1250, -0.0557,\n",
      "         -0.4658,  0.4299, -0.4180,  0.0236,  0.4783,  0.4025,  0.0815, -0.0173,\n",
      "          0.4823,  0.1275,  0.4741, -0.4987, -0.2647, -0.1473,  0.1236,  0.0726]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "decoder.cls tensor([[ 0.1907, -0.0855,  0.2884,  0.0059,  0.0151, -0.1211, -0.0240,  0.0505,\n",
      "         -0.0680, -0.1656,  0.4143,  0.2014,  0.2621,  0.1465,  0.0651,  0.3104],\n",
      "        [-0.0564, -0.0214,  0.4032, -0.1098, -0.0946, -0.1517,  0.0408,  0.4662,\n",
      "          0.1239,  0.1935, -0.1554, -0.1079, -0.0227, -0.0097,  0.1681, -0.1826],\n",
      "        [-0.0996,  0.2353, -0.1533,  0.1698, -0.2153, -0.0522, -0.0355, -0.1356,\n",
      "          0.2591,  0.0297,  0.1535,  0.0189,  0.2870, -0.1519,  0.4332, -0.0928],\n",
      "        [ 0.2755, -0.0304,  0.0118, -0.2897,  0.3529,  0.1428,  0.0286, -0.3676,\n",
      "         -0.1368,  0.2292, -0.0721, -0.0598,  0.0905,  0.1802,  0.0581, -0.1193],\n",
      "        [-0.1338,  0.0976,  0.2820, -0.1065,  0.2079, -0.3327, -0.0939, -0.1354,\n",
      "          0.0402, -0.1933, -0.0809,  0.2801,  0.0853, -0.3842,  0.0270,  0.0329]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "decoder.bias tensor([0., 0., 0., 0., 0.], device='cuda:0')\n",
      "tensor([[0.0117, 0.0159, 0.0069,  ..., 0.0286, 0.0262, 0.0031],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0076, 0.0196, 0.0044,  ..., 0.0410, 0.0338, 0.0050],\n",
      "        ...,\n",
      "        [0.0082, 0.0148, 0.0015,  ..., 0.0249, 0.0245, 0.0015],\n",
      "        [0.0062, 0.0185, 0.0058,  ..., 0.0264, 0.0271, 0.0028],\n",
      "        [0.0058, 0.0181, 0.0024,  ..., 0.0239, 0.0231, 0.0023]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<WhereBackward0>)\n",
      "encoder.linear_before.bias True\n",
      "encoder.linear_before.E_linear.weight True\n",
      "encoder.layers.0.net.kernel_tangents False\n",
      "encoder.layers.0.net.linears.0.bias True\n",
      "encoder.layers.0.net.linears.0.E_linear.weight True\n",
      "encoder.layers.0.net.linears.1.bias True\n",
      "encoder.layers.0.net.linears.1.E_linear.weight True\n",
      "encoder.layers.0.net.linears.2.bias True\n",
      "encoder.layers.0.net.linears.2.E_linear.weight True\n",
      "encoder.layers.0.net.linears.3.bias True\n",
      "encoder.layers.0.net.linears.3.E_linear.weight True\n",
      "encoder.layers.0.net.linears.4.bias True\n",
      "encoder.layers.0.net.linears.4.E_linear.weight True\n",
      "encoder.layers.0.net.linears.5.bias True\n",
      "encoder.layers.0.net.linears.5.E_linear.weight True\n",
      "decoder.origin False\n",
      "decoder.cls True\n",
      "decoder.bias True\n"
     ]
    }
   ],
   "source": [
    "if config_args['model_config']['model'][0] == 'BMLP':\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(name, param.data)\n",
    "    print(model.encode(data['features'], data['adj_train_norm']))#[2]\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param.requires_grad)  # 确保所有参数的 requires_grad 都是 True\n",
    "elif config_args['model_config']['model'][0] == 'BKNet':\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(name, param.data)\n",
    "    print(model.encode(data['features'], (nei, nei_mask)))#[2]\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param.requires_grad)  # 确保所有参数的 requires_grad 都是 True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Klein Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport func_outof_class as foc\\n\\n\\nprint(data[\\'features\\'].shape)\\nprint(torch.norm(data[\\'features\\'], p=2, dim=1).max())\\nprint(torch.norm(data[\\'features\\'], p=2, dim=1).min())\\n\\nx = data[\\'features\\'] #(n,d)\\nx_nei = foc.gather(x, nei) #(n,nei_num,d)\\n\\nx=x.to(torch.float64)\\nx_tan = model.manifold.proj_tan0(x, model.c)\\nx_hyp = model.manifold.expmap0(x_tan, c=model.c)\\nx = model.manifold.proj(x_hyp, c=model.c)\\n\\nx=model.encoder.linear_before(x)\\n\\nx_nei = foc.gather(x, nei) #(n,nei_num,d\\')\\nprint(x.shape)#,x)\\nprint(x_nei.shape)#,x_nei) #Many repeated samples so that n might be different from x_nei\\n\\nprint(\"If we PT neiborhood back to the origin, we shouldn remain kernel points near at the origin\")\\nx, x_nei = foc.transport_x(model.manifold,x, x_nei,model.c)\\n#print(\"x: \",x)\\n#print(\"x_nei: \",x_nei[0].shape,x_nei[0])\\nkernel_tangents = foc.init_KP(model.manifold,KP_extent=0.66,K=6,in_channels=64,c=model.c)\\nkernels = foc.get_kernel_pos(model.manifold,kernel_tangents,x,model.c,KP_extent=0.66,transp=False)#(n, k, d)\\nprint(torch.equal(kernels[0],kernels[-1]))\\nprint(\"E_dis between kernel points and origin: \",torch.norm(kernels,dim=-1))\\n\\n#Note we are currently using Poincare distance\\nx_nei_kernel_dis=foc.get_nei_kernel_dis(model.manifold,kernels, x_nei, model.c) #(n,k,nei_num)\\nnei_mask = nei_mask.repeat(1, 1, kernels.shape[1]).view(x_nei_kernel_dis.shape[0],kernels.shape[1],x_nei_kernel_dis.shape[2])\\nx_nei_kernel_dis = x_nei_kernel_dis * nei_mask  # (n, k, nei_num)\\nprint(x_nei_kernel_dis.shape,\\'\\n\\',x_nei_kernel_dis)\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import func_outof_class as foc\n",
    "\n",
    "\n",
    "print(data['features'].shape)\n",
    "print(torch.norm(data['features'], p=2, dim=1).max())\n",
    "print(torch.norm(data['features'], p=2, dim=1).min())\n",
    "\n",
    "x = data['features'] #(n,d)\n",
    "x_nei = foc.gather(x, nei) #(n,nei_num,d)\n",
    "\n",
    "x=x.to(torch.float64)\n",
    "x_tan = model.manifold.proj_tan0(x, model.c)\n",
    "x_hyp = model.manifold.expmap0(x_tan, c=model.c)\n",
    "x = model.manifold.proj(x_hyp, c=model.c)\n",
    "\n",
    "x=model.encoder.linear_before(x)\n",
    "\n",
    "x_nei = foc.gather(x, nei) #(n,nei_num,d')\n",
    "print(x.shape)#,x)\n",
    "print(x_nei.shape)#,x_nei) #Many repeated samples so that n might be different from x_nei\n",
    "\n",
    "print(\"If we PT neiborhood back to the origin, we shouldn remain kernel points near at the origin\")\n",
    "x, x_nei = foc.transport_x(model.manifold,x, x_nei,model.c)\n",
    "#print(\"x: \",x)\n",
    "#print(\"x_nei: \",x_nei[0].shape,x_nei[0])\n",
    "kernel_tangents = foc.init_KP(model.manifold,KP_extent=0.66,K=6,in_channels=64,c=model.c)\n",
    "kernels = foc.get_kernel_pos(model.manifold,kernel_tangents,x,model.c,KP_extent=0.66,transp=False)#(n, k, d)\n",
    "print(torch.equal(kernels[0],kernels[-1]))\n",
    "print(\"E_dis between kernel points and origin: \",torch.norm(kernels,dim=-1))\n",
    "\n",
    "#Note we are currently using Poincare distance\n",
    "x_nei_kernel_dis=foc.get_nei_kernel_dis(model.manifold,kernels, x_nei, model.c) #(n,k,nei_num)\n",
    "nei_mask = nei_mask.repeat(1, 1, kernels.shape[1]).view(x_nei_kernel_dis.shape[0],kernels.shape[1],x_nei_kernel_dis.shape[2])\n",
    "x_nei_kernel_dis = x_nei_kernel_dis * nei_mask  # (n, k, nei_num)\n",
    "print(x_nei_kernel_dis.shape,'\\n',x_nei_kernel_dis)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HKNet",
   "language": "python",
   "name": "hknet"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
