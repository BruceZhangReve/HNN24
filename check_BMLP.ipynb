{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typical BMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/lige/miniconda3/envs/HKNet/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n",
      "Using seed 1234.\n",
      "Dataset: wisconsin\n",
      "Num classes: 5\n",
      "dict_keys(['adj_train', 'features', 'labels', 'idx_train', 'idx_val', 'idx_test', 'adj_train_norm'])\n",
      "(251, 251)\n",
      "torch.Size([251, 1703])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/lige/HKN/utils/data_utils.py:347: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G, sorted(G.nodes()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCModel(\n",
      "  (encoder): BMLP(\n",
      "    (layers): Sequential(\n",
      "      (0): BMLP(\n",
      "        (linear1): BLinear(in_features=1703, out_features=64, c=tensor([1.], device='cuda:0'), use_bias=1, act=None)\n",
      "        (act): BAct(c=tensor([1.], device='cuda:0'), act=<function relu at 0x7fd81043b370>)\n",
      "        (linear2): BLinear(in_features=64, out_features=64, c=tensor([1.], device='cuda:0'), use_bias=1, act=None)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): PoincareDecoder()\n",
      ")\n",
      "Total number of parameters: 113605\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from types import SimpleNamespace\n",
    "import sys\n",
    "sys.path.append('/data/lige/HKN')# Please change accordingly!\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from geoopt import ManifoldParameter as geoopt_ManifoldParameter\n",
    "from manifolds.base import ManifoldParameter as base_ManifoldParameter\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "from optim import RiemannianAdam, RiemannianSGD\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from config import parser\n",
    "from models.base_models import NCModel, LPModel, GCModel\n",
    "from utils.data_utils import load_data, get_nei, GCDataset, split_batch\n",
    "from utils.train_utils import get_dir_name, format_metrics\n",
    "from utils.eval_utils import acc_f1\n",
    "\n",
    "from geoopt import ManifoldParameter as geoopt_ManifoldParameter\n",
    "from manifolds.base import ManifoldParameter as base_ManifoldParameter\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "config_args = {\n",
    "    'training_config': {\n",
    "        'lr': (1e-3, 'learning rate'),\n",
    "        'dropout': (0.25, 'dropout probability'),\n",
    "        'cuda': (0, 'which cuda device to use (-1 for cpu training)'),\n",
    "        'epochs': (1000, 'maximum number of epochs to train for'),\n",
    "        'weight_decay': (1e-3, 'l2 regularization strength'),\n",
    "        'optimizer': ('radam', 'which optimizer to use, can be any of [rsgd, radam]'),\n",
    "        'momentum': (0.999, 'momentum in optimizer'),\n",
    "        'patience': (15, 'patience for early stopping'),\n",
    "        'seed': (1234, 'seed for training'),\n",
    "        'log_freq': (5, 'how often to compute print train/val metrics (in epochs)'),\n",
    "        'eval_freq': (1, 'how often to compute val metrics (in epochs)'),\n",
    "        'save': (0, '1 to save model and logs and 0 otherwise'),\n",
    "        'save_dir': (None, 'path to save training logs and model weights (defaults to logs/task/date/run/)'),\n",
    "        'sweep_c': (0, ''),\n",
    "        'lr_reduce_freq': (None, 'reduce lr every lr-reduce-freq or None to keep lr constant'),\n",
    "        'gamma': (0.5, 'gamma for lr scheduler'),\n",
    "        'print_epoch': (True, ''),\n",
    "        'grad_clip': (None, 'max norm for gradient clipping, or None for no gradient clipping'),\n",
    "        'min_epochs': (300, 'do not early stop before min-epochs')\n",
    "    },\n",
    "    'model_config': {\n",
    "        'use_geoopt': (False, \"which manifold class to use, if false then use basd.manifold\"),\n",
    "        'AggKlein':(False, \"if false, then use hyperboloid centorid for aggregation\"),\n",
    "        'corr': (0,'0: d(x_i ominus x, x_k), 1: d(x_ik,x_k)'),\n",
    "        'task': ('nc', 'which tasks to train on, can be any of [lp, nc]'),\n",
    "        'model': ('BMLP', 'which encoder to use, can be any of [Shallow, MLP, HNN, GCN, GAT, HyperGCN, HyboNet,BKNet,BMLP]'),\n",
    "        'dim': (64, 'embedding dimension'),\n",
    "        'manifold': ('PoincareBall', 'which manifold to use, can be any of [Euclidean, Hyperboloid, PoincareBall, Lorentz]'),\n",
    "        'c': (1.0, 'hyperbolic radius, set to None for trainable curvature'),\n",
    "        'r': (2., 'fermi-dirac decoder parameter for lp'),\n",
    "        't': (1., 'fermi-dirac decoder parameter for lp'),\n",
    "        'margin': (2., 'margin of MarginLoss'),\n",
    "        'pretrained_embeddings': (None, 'path to pretrained embeddings (.npy file) for Shallow node classification'),\n",
    "        'pos_weight': (0, 'whether to upweight positive class in node classification tasks'),\n",
    "        'num_layers': (2, 'number of hidden layers in encoder'),\n",
    "        'bias': (1, 'whether to use bias (1) or not (0)'),\n",
    "        'act': ('relu', 'which activation function to use (or None for no activation)'),\n",
    "        'n_heads': (4, 'number of attention heads for graph attention networks, must be a divisor dim'),\n",
    "        'alpha': (0.2, 'alpha for leakyrelu in graph attention networks'),\n",
    "        'double_precision': ('1', 'whether to use double precision'),\n",
    "        'use_att': (0, 'whether to use hyperbolic attention or not'),\n",
    "        'local_agg': (0, 'whether to local tangent space aggregation or not'),\n",
    "        'kernel_size': (4, 'number of kernels'),\n",
    "        'KP_extent': (0.66, 'influence radius of each kernel point'),\n",
    "        'radius': (1, 'radius used for kernel point init'),\n",
    "        'deformable': (False, 'deformable kernel'),\n",
    "        'linear_before': (64, 'dim of linear before gcn')#64\n",
    "    },\n",
    "    'data_config': {\n",
    "        'dataset': ('wisconsin', 'which dataset to use(cornell,wisconsin,squirrel,cora)'),\n",
    "        'batch_size': (32, 'batch size for gc'),\n",
    "        'val_prop': (0.05, 'proportion of validation edges for link prediction'),\n",
    "        'test_prop': (0.1, 'proportion of test edges for link prediction'),\n",
    "        'use_feats': (1, 'whether to use node features or not'),\n",
    "        'normalize_feats': (1, 'whether to normalize input node features'),\n",
    "        'normalize_adj': (1, 'whether to row-normalize the adjacency matrix'),\n",
    "        'split_seed': (1234, 'seed for data splits (train/test/val)'),\n",
    "        'split_graph': (False, 'whether to split the graph')\n",
    "    }\n",
    "}\n",
    "\n",
    "# 将所有参数转换为 SimpleNamespace\n",
    "args = SimpleNamespace(\n",
    "    **{k: v[0] for config in config_args.values() for k, v in config.items()}\n",
    ")\n",
    "\n",
    "#choose which manifold class to follow \n",
    "if args.use_geoopt == False:\n",
    "    ManifoldParameter = base_ManifoldParameter\n",
    "else:\n",
    "    ManifoldParameter = geoopt_ManifoldParameter\n",
    "np.random.seed(args.seed)#args.seed\n",
    "torch.manual_seed(args.seed)#args.seed\n",
    "if int(args.cuda):#args.double_precision\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "if int(args.cuda) >= 0:#args.cuda\n",
    "    torch.cuda.manual_seed(args.seed)#args.seed\n",
    "args.device = 'cuda:' + str(args.cuda) if int(args.cuda) >= 0 else 'cpu' #args.device actually,<-args.cuda\n",
    "args.patience = args.epochs if not args.patience else args.patience #args.patience<-args.epochs|args.patience\n",
    "\n",
    "print(f'Using: {args.device}')\n",
    "print(\"Using seed {}.\".format(args.seed))\n",
    "print(f\"Dataset: {args.dataset}\")\n",
    "\n",
    "# Load data\n",
    "data = load_data(args, os.path.join('data', args.dataset))\n",
    "if args.task == 'gc':\n",
    "    args.n_nodes, args.feat_dim = data['features'][0].shape\n",
    "else:\n",
    "    args.n_nodes, args.feat_dim = data['features'].shape\n",
    "if args.task == 'nc':\n",
    "    Model = NCModel\n",
    "    args.n_classes = int(data['labels'].max() + 1)\n",
    "    args.data = data\n",
    "    print(f'Num classes: {args.n_classes}')\n",
    "elif args.task == 'gc':\n",
    "    Model = GCModel\n",
    "    args.n_classes = int(data['labels'].max() + 1)\n",
    "    print(f'Num classes: {args.n_classes}')\n",
    "else:\n",
    "    args.nb_false_edges = len(data['train_edges_false'])\n",
    "    args.nb_edges = len(data['train_edges'])\n",
    "    if args.task == 'lp':\n",
    "        Model = LPModel\n",
    "        args.n_classes = 2\n",
    "\n",
    "if not args.lr_reduce_freq:\n",
    "    args.lr_reduce_freq = args.epochs\n",
    "\n",
    "\n",
    "###A simple check on data\n",
    "print(data.keys())\n",
    "print(data['adj_train'].todense().shape)\n",
    "print(data['features'].shape)\n",
    "###A simple check on data\n",
    "\n",
    "# Model and optimizer\n",
    "model = Model(args)\n",
    "print(str(model))\n",
    "no_decay = ['bias', 'scale']\n",
    "optimizer_grouped_parameters = [{\n",
    "    'params': [\n",
    "        p for n, p in model.named_parameters()\n",
    "        if p.requires_grad and not any(\n",
    "            nd in n\n",
    "            for nd in no_decay) and not isinstance(p, ManifoldParameter)\n",
    "    ],\n",
    "    'weight_decay':\n",
    "    args.weight_decay\n",
    "}, {\n",
    "    'params': [\n",
    "        p for n, p in model.named_parameters() if p.requires_grad and any(\n",
    "            nd in n\n",
    "            for nd in no_decay) or isinstance(p, ManifoldParameter)\n",
    "    ],\n",
    "    'weight_decay':\n",
    "    0.0\n",
    "}]\n",
    "if args.optimizer == 'radam':\n",
    "    optimizer = RiemannianAdam(params=optimizer_grouped_parameters,\n",
    "                                lr=args.lr,\n",
    "                                stabilize=10)\n",
    "elif args.optimizer == 'rsgd':\n",
    "    optimizer = RiemannianSGD(params=optimizer_grouped_parameters,\n",
    "                                lr=args.lr,\n",
    "                                stabilize=10)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=int(\n",
    "                                                args.lr_reduce_freq),\n",
    "                                                gamma=float(args.gamma))\n",
    "tot_params = sum([np.prod(p.size()) for p in model.parameters()])\n",
    "model = model.to(args.device)\n",
    "for x, val in data.items():\n",
    "    if torch.is_tensor(data[x]):\n",
    "        data[x] = data[x].to(args.device)\n",
    "print(f\"Total number of parameters: {tot_params}\")\n",
    "\n",
    "# Train model for nc:\n",
    "t_total = time.time()\n",
    "counter = 0\n",
    "best_val_metrics = model.init_metric_dict()\n",
    "best_test_metrics = None\n",
    "best_emb = None\n",
    "if args.n_classes > 2:\n",
    "    f1_average = 'micro'\n",
    "else:\n",
    "    f1_average = 'binary'\n",
    "\n",
    "if args.model == 'HKPNet':\n",
    "    nei, nei_mask = get_nei(data['adj_train'])\n",
    "    nei = nei.to(args.device)\n",
    "    nei_mask = nei_mask.to(args.device)\n",
    "elif args.model == 'BKNet':\n",
    "    nei, nei_mask = get_nei(data['adj_train'])\n",
    "    nei = nei.to(args.device)\n",
    "    nei_mask = nei_mask.to(args.device) #nei/nei_mask on cuda now\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.layers.0.linear1.weight tensor([[-0.0442, -0.0694, -0.0055,  ...,  0.0167,  0.0654,  0.0218],\n",
      "        [ 0.0190,  0.0472,  0.0733,  ..., -0.0487,  0.0387,  0.0355],\n",
      "        [-0.0693,  0.0475, -0.0709,  ..., -0.0135, -0.0324,  0.0420],\n",
      "        ...,\n",
      "        [-0.0812, -0.0316,  0.0350,  ..., -0.0817, -0.0659, -0.0149],\n",
      "        [ 0.0214,  0.0800,  0.0784,  ...,  0.0245,  0.0677,  0.0106],\n",
      "        [ 0.0500,  0.0287,  0.0559,  ..., -0.0123,  0.0511, -0.0228]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "encoder.layers.0.linear1.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "encoder.layers.0.linear2.weight tensor([[-0.2587,  0.2692, -0.2231,  ..., -0.2295, -0.2376,  0.2128],\n",
      "        [ 0.0599, -0.0904,  0.1964,  ...,  0.0389,  0.1595, -0.1563],\n",
      "        [-0.3035, -0.1732,  0.1453,  ...,  0.2392, -0.1525,  0.0017],\n",
      "        ...,\n",
      "        [-0.2077,  0.0995,  0.1589,  ..., -0.1210, -0.2564, -0.2325],\n",
      "        [-0.2190, -0.1978,  0.2991,  ...,  0.1714,  0.1404, -0.2979],\n",
      "        [ 0.2271,  0.1262,  0.0886,  ..., -0.0172,  0.0560,  0.2758]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "encoder.layers.0.linear2.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "decoder.cls tensor([[ 0.0089, -0.1082, -0.0190, -0.0567, -0.0751, -0.1393, -0.0189, -0.0408,\n",
      "         -0.0605,  0.0317, -0.0730,  0.0653,  0.2058,  0.0609,  0.0704,  0.1405,\n",
      "         -0.0349,  0.1365,  0.0160,  0.1879,  0.0379,  0.0117,  0.1314, -0.0448,\n",
      "         -0.1047,  0.1926,  0.1988, -0.0449, -0.0553,  0.0954,  0.0918, -0.1125,\n",
      "          0.0366, -0.0040,  0.0206,  0.0265, -0.1692,  0.0291,  0.0447,  0.1976,\n",
      "          0.1279, -0.0475, -0.0768, -0.0468,  0.1033,  0.0979,  0.0235, -0.1060,\n",
      "          0.0268, -0.0326,  0.0530,  0.0829, -0.0122, -0.0898,  0.0332, -0.0699,\n",
      "         -0.1434,  0.1357, -0.0399, -0.0978, -0.0651, -0.1201,  0.1425, -0.0835],\n",
      "        [-0.0161,  0.1070, -0.1078, -0.0689, -0.1846, -0.0663, -0.0326, -0.1326,\n",
      "          0.0929,  0.0233, -0.1004,  0.0678, -0.1567, -0.0166,  0.1319,  0.0078,\n",
      "          0.0381,  0.1796, -0.0798, -0.0334, -0.1300, -0.0647,  0.0466, -0.0063,\n",
      "          0.1083, -0.0329,  0.1209,  0.1303,  0.0346, -0.0162, -0.0063, -0.1091,\n",
      "         -0.2180, -0.0260, -0.0414,  0.0675, -0.0746,  0.0642, -0.1143, -0.1107,\n",
      "         -0.0305, -0.0834,  0.1443,  0.2448, -0.1402,  0.1060, -0.0144,  0.0106,\n",
      "         -0.0254,  0.1085,  0.0055,  0.0023,  0.0917,  0.0591, -0.0475, -0.0873,\n",
      "         -0.0524,  0.1616, -0.0142, -0.0580,  0.0127, -0.0917, -0.0198, -0.1465],\n",
      "        [ 0.0056,  0.0596, -0.0699, -0.0781, -0.0073, -0.1253, -0.0092,  0.0503,\n",
      "          0.0342,  0.0025, -0.0817,  0.2286,  0.0615,  0.0461,  0.1156, -0.0136,\n",
      "          0.0543,  0.0443, -0.2450,  0.0558,  0.0471,  0.0585,  0.0152,  0.0716,\n",
      "          0.0976, -0.0437,  0.1120,  0.0693,  0.2130, -0.0478,  0.0049, -0.0642,\n",
      "          0.1251, -0.0755, -0.0938, -0.0826, -0.0008,  0.0303,  0.0756, -0.0355,\n",
      "         -0.1492, -0.0006, -0.0119, -0.0531,  0.0125, -0.0549, -0.1614, -0.0205,\n",
      "         -0.0753,  0.1018, -0.0856, -0.1503, -0.0190,  0.0180, -0.2444,  0.1082,\n",
      "         -0.0699, -0.0460, -0.0896,  0.1269, -0.1078,  0.1722,  0.0143, -0.1325],\n",
      "        [ 0.0918, -0.1098, -0.1305, -0.0969, -0.1545, -0.0557,  0.1714,  0.2042,\n",
      "         -0.0724, -0.0394,  0.1016, -0.0519,  0.0601,  0.0666, -0.0438,  0.0705,\n",
      "          0.0380,  0.0555,  0.2511, -0.0980, -0.0038,  0.0484,  0.2337, -0.0321,\n",
      "         -0.1299,  0.0096, -0.0093, -0.0548, -0.0096, -0.0954,  0.0131, -0.0101,\n",
      "          0.0884,  0.0249, -0.0294,  0.1757, -0.0646,  0.0873, -0.0557,  0.0588,\n",
      "          0.0876,  0.0173,  0.1088, -0.0446,  0.0096,  0.0307, -0.0100, -0.0139,\n",
      "         -0.0984, -0.1654, -0.0543,  0.0703,  0.0284, -0.0417, -0.0547,  0.1101,\n",
      "         -0.1531, -0.0856,  0.0276, -0.0761,  0.0684, -0.0502, -0.1418,  0.1635],\n",
      "        [ 0.0741, -0.0062,  0.0900, -0.1284,  0.0050,  0.1007, -0.0085, -0.1435,\n",
      "          0.0019,  0.0742, -0.0264,  0.0371,  0.0817,  0.0104, -0.1189,  0.0484,\n",
      "         -0.0379,  0.0079, -0.0703,  0.0386,  0.0640,  0.0349, -0.1012,  0.1957,\n",
      "         -0.0009, -0.0192, -0.0128, -0.0702,  0.0773, -0.1718, -0.0902,  0.0351,\n",
      "         -0.0194,  0.0883, -0.0720, -0.1267, -0.0795, -0.1642,  0.0196,  0.1569,\n",
      "          0.0168,  0.1784, -0.0379, -0.0558, -0.2721, -0.1526, -0.0777,  0.0181,\n",
      "          0.1438, -0.0478,  0.0320,  0.0608,  0.0048, -0.0113, -0.0466,  0.1199,\n",
      "         -0.0486, -0.0238, -0.0968, -0.0879, -0.1654, -0.1374, -0.1789, -0.0738]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "decoder.bias tensor([0., 0., 0., 0., 0.], device='cuda:0')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'norm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(name, param\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madj_train_norm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;66;03m#[2]\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(name, param\u001b[38;5;241m.\u001b[39mrequires_grad)  \u001b[38;5;66;03m# 确保所有参数的 requires_grad 都是 True\u001b[39;00m\n",
      "File \u001b[0;32m/data/lige/HKN/models/base_models.py:52\u001b[0m, in \u001b[0;36mBaseModel.encode\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanifold\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPoincareBall\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     51\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanifold\u001b[38;5;241m.\u001b[39mexpmap0(x,c\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc)\u001b[38;5;66;03m#Using manifold.base\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#Note: h is the updated feature points matrix of shape (n,d')\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h\n",
      "File \u001b[0;32m/data/lige/HKN/models/encoders.py:362\u001b[0m, in \u001b[0;36mBMLP.encode\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m    360\u001b[0m x_hyp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanifold\u001b[38;5;241m.\u001b[39mexpmap0(x_tan, c\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc)\n\u001b[1;32m    361\u001b[0m x_hyp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanifold\u001b[38;5;241m.\u001b[39mproj(x_hyp, c\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc)\n\u001b[0;32m--> 362\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBMLP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_hyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/lige/HKN/models/encoders.py:40\u001b[0m, in \u001b[0;36mEncoder.encode\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_graph:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#Not sure what this means\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m (x, adj)\n\u001b[0;32m---> 40\u001b[0m     output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mforward(x)\n",
      "File \u001b[0;32m/data/lige/miniconda3/envs/HKNet/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/data/lige/miniconda3/envs/HKNet/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/lige/HKN/layers/B_layers.py:209\u001b[0m, in \u001b[0;36mBMLP.forward\u001b[0;34m(self, x_nei_transform)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_nei_transform):\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m#x_nei_transform: (n, nei_num, d')\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_nei_transform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(h)\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2\u001b[38;5;241m.\u001b[39mforward(h)\n",
      "File \u001b[0;32m/data/lige/HKN/layers/B_layers.py:153\u001b[0m, in \u001b[0;36mBLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    150\u001b[0m drop_weight \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Perform Mobius matrix-vector multiplication\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m#mv = self.manifold.mobius_matvec(drop_weight, x, self.c)\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m mv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanifold\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmobius_matvec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# Project the result\u001b[39;00m\n\u001b[1;32m    155\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanifold\u001b[38;5;241m.\u001b[39mproj(mv, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc)\n",
      "File \u001b[0;32m/data/lige/HKN/manifolds/poincare.py:108\u001b[0m, in \u001b[0;36mPoincareBall.mobius_matvec\u001b[0;34m(self, m, x, c)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmobius_matvec\u001b[39m(\u001b[38;5;28mself\u001b[39m, m, x, c):\n\u001b[1;32m    107\u001b[0m     sqrt_c \u001b[38;5;241m=\u001b[39m c \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m--> 108\u001b[0m     x_norm \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp_min(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_norm)\n\u001b[1;32m    109\u001b[0m     mx \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m@\u001b[39m m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    110\u001b[0m     mx_norm \u001b[38;5;241m=\u001b[39m mx\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp_min(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_norm)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'norm'"
     ]
    }
   ],
   "source": [
    "if config_args['model_config']['model'][0] == 'BMLP':\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(name, param.data)\n",
    "    print(model.encode(data['features'], data['adj_train_norm']))#[2]\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param.requires_grad)  # 确保所有参数的 requires_grad 都是 True\n",
    "elif config_args['model_config']['model'][0] == 'BKNet':\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(name, param.data)\n",
    "    print(model.encode(data['features'], (nei, nei_mask)))#[2]\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param.requires_grad)  # 确保所有参数的 requires_grad 都是 True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_values=[]\n",
    "embedded_values=[]\n",
    "model_linear_weight=[]\n",
    "\n",
    "# 检查权重更新\n",
    "def check_weights(model, epoch):\n",
    "    print(f\"Epoch {epoch + 1} - Model Linear Weight:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name}: {param.data}\")\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    if args.model == 'HKPNet':\n",
    "        embeddings = model.encode(data['features'], (nei, nei_mask))\n",
    "    elif args.model == 'BKNet':\n",
    "        embeddings = model.encode(data['features'], (nei, nei_mask))\n",
    "    else:\n",
    "        embeddings = model.encode(data['features'], data['adj_train_norm'])\n",
    "\n",
    "    idx=data[f'idx_train']\n",
    "    output=model.decode(embeddings, data['adj_train_norm'], idx)\n",
    "    train_metrics = model.compute_metrics(embeddings, data, 'train')\n",
    "    \n",
    "    # 检查 decoded_values 和 embeddings\n",
    "    embedded_values.append(embeddings)\n",
    "    #idx = data[f'idx_train']\n",
    "    #output = model.decode(embeddings, data['adj_train_norm'], idx)\n",
    "    decoded_values.append(output)\n",
    "    model_linear_weight.append(model.encoder.layers[0].linear.weight.clone())\n",
    "    \n",
    "    # 检查梯度是否被正确计算\n",
    "    #train_metrics['loss'].backward()\n",
    "    loss = F.cross_entropy(output, data['labels'][idx])\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            print(f\"Grad of {name}: {param.grad.abs().mean()}\")\n",
    "        else:\n",
    "            print(f\"Grad of {name}: None\")  # 检查梯度是否为 None\n",
    "    \n",
    "    if args.grad_clip is not None:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # 打印和检查权重是否被更新\n",
    "    check_weights(model, epoch)\n",
    "    \n",
    "    if (epoch + 1) % args.log_freq == 0:\n",
    "        print(\" \".join([\n",
    "            'Epoch: {:04d}'.format(epoch + 1),\n",
    "            'lr: {}'.format(lr_scheduler.get_last_lr()),\n",
    "            format_metrics(train_metrics, 'train'),\n",
    "            'time: {:.4f}s'.format(time.time() - t)\n",
    "        ]))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if (epoch + 1) % args.eval_freq == 0:\n",
    "            model.eval()\n",
    "            if args.model == 'HKPNet':\n",
    "                embeddings = model.encode(data['features'], (nei, nei_mask))\n",
    "            elif args.model == 'BKNet':\n",
    "                embeddings = model.encode(data['features'], (nei, nei_mask))\n",
    "            else:\n",
    "                embeddings = model.encode(data['features'],\n",
    "                                        data['adj_train_norm'])\n",
    "            val_metrics = model.compute_metrics(embeddings, data, 'val')\n",
    "            if (epoch + 1) % args.log_freq == 0:\n",
    "                print(\" \".join([\n",
    "                    'Epoch: {:04d}'.format(epoch + 1),\n",
    "                    format_metrics(val_metrics, 'val')\n",
    "                ]))\n",
    "            if model.has_improved(best_val_metrics, val_metrics):\n",
    "                best_test_metrics = model.compute_metrics(\n",
    "                    embeddings, data, 'test')\n",
    "                best_emb = embeddings.cpu()\n",
    "                if args.save:\n",
    "                    np.save(os.path.join(save_dir, 'embeddings.npy'),\n",
    "                            best_emb.detach().numpy())\n",
    "                best_val_metrics = val_metrics\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter == args.patience and epoch > args.min_epochs:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "if not best_test_metrics:\n",
    "    model.eval()\n",
    "    best_emb = model.encode(data['features'], data['adj_train_norm'])\n",
    "    best_test_metrics = model.compute_metrics(best_emb, data, 'test')\n",
    "print(\" \".join(\n",
    "    [\"Val set results:\",\n",
    "    format_metrics(best_val_metrics, 'val')]))\n",
    "print(\" \".join(\n",
    "    [\"Test set results:\",\n",
    "    format_metrics(best_test_metrics, 'test')]))\n",
    "if args.save:\n",
    "    np.save(os.path.join(save_dir, 'embeddings.npy'),\n",
    "            best_emb.cpu().detach().numpy())\n",
    "    if hasattr(model.encoder, 'att_adj'):\n",
    "        filename = os.path.join(save_dir, args.dataset + '_att_adj.p')\n",
    "        pickle.dump(model.encoder.att_adj.cpu().to_dense(),\n",
    "                    open(filename, 'wb'))\n",
    "        print('Dumped attention adj: ' + filename)\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, 'model.pth'))\n",
    "    json.dump(vars(args), open(os.path.join(save_dir, 'config.json'), 'w'))\n",
    "    logging.info(f\"Saved model in {save_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HKNet",
   "language": "python",
   "name": "hknet"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
